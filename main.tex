\documentclass[10pt, oneside]{article} 

\usepackage{amsmath, amsthm, amssymb, calrsfs, wasysym, verbatim, bbm, color, graphics, geometry, graphicx, mdframed, xcolor, multirow, tcolorbox, hyperref, framed}
\usepackage[czech]{babel}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=blue,
    citecolor=blue
}

% Definice prostředí pro zkušební otázku
\newmdenv[
  backgroundcolor=gray!10,
  linewidth=0pt,
  innertopmargin=10pt,
  innerbottommargin=10pt,
  leftmargin=0pt,
  rightmargin=0pt
]{examquestion}

\newtcolorbox{examquestion}{
  colback=blue!5!white,
  colframe=blue!75!black,
  title=Zkušební otázka,
  fonttitle=\bfseries
}

\definecolor{shadecolor}{rgb}{0.95,0.95,0.95}

\newcommand{\examq}[2]{%
  \begin{mdframed}[
    backgroundcolor=yellow!10,
    linecolor=orange!80,
    linewidth=2pt,
    innertopmargin=8pt,
    innerbottommargin=8pt
  ]
  \noindent\textbf{Otázka #1:} #2
  \end{mdframed}
}

\geometry{tmargin=.75in, bmargin=.75in, lmargin=.75in, rmargin = .75in}  

\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Cdot}{\boldsymbol{\cdot}}

\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{conv}{Convention}
\newtheorem{rem}{Remark}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}

\title{Strojové učení 1 (Machine Learning 1)}

\author{Matěj Štaif \\[0.15cm]
CTU--FIT \\[0.15cm]
\texttt{staifmat@fit.cvut.cz}}

\date{January 1, 2026}

\begin{document}

\maketitle

\begin{abstract}
Tento dokument představuje soubor poznámek pro přípravu na zkoušku z předmětu Strojové učení 1 (BI-ML1) na ČVUT FIT (ZS 2025/26). Text je syntézou informací prezentovaných na přednáškách, autorsky jsem jej však zpracoval samostatně. Prohlašuji, že samotný text nebyl generován pomocí LLM. V případě nalezení chyb prosím využijte issues nebo kontakt v záhlaví.
\end{abstract}

\tableofcontents

\vspace{.25in}

\section{Obecné pojmy}

\subsection{Klasifikace a regrese (Classification and Regression)}

\begin{itemize}

\item {\bf Klasifikace (angl. Classification):} Problém, kdy vysvětlovaná proměnná $Y$ (angl. target variable) může nabývat jen několik málo hodnot.

\begin{itemize}
\item Příklady: určení, jestli pacient má/nemá nemoc či jaké písmeno je vyobrazeno na obrázku.
\end{itemize}

\item {\bf Regrese (angl. Regression):} Problém, kdy vysvětlovaná proměnná $Y$ (angl. target variable) může nabývat tolika hodnot, že je rozumnější ji považovat za spojitou.

\begin{itemize}
\item Příklady: Předpovídání ceny, teploty či věku.
\end{itemize}

\item Oba typy patří do supervizovaného učení, kde se snažíme zjistit funkční vztah:

\[ Y = f(X_1, X_2, \dots, X_p), \quad p \in \N. \]

\end{itemize}

\subsection{Supervizované a nesupervizované učení}

\begin{itemize}

\item {\bf Supervizované učení (angl. Supervised Learning):} typ učení , kde máme známé hodnoty vysvětlované proměnné $Y$ (angl. target variable), což je veličina, kterou se snažíme pomocí modelu predikovat resp. pochopit, na čem závisí.
Tím "učitelem" jsou zde známé hodnoty $Y$ v trénovacích datech.

\begin{itemize}
\item Označováno taktéž jako \textbf{učení s učitelem}.
\item Příklady: Rozhodovací stromy (angl. Decision Trees), k-Nejbližších sousedů (angl. k-Nearest Neighbors), lineární regrese (angl. Linear Regression), hřebenová regrese (angl. Ridge Regression), logistická regrese (angl. Logistic Regression), náhodné lesy (angl. Random Forests), AdaBoost, XGBoost, Support Vector Machines (SVM).
\end{itemize}

\item {\bf Nesupervizované učení (angl. Unsupervised Learning):} nastává v situaci, kdy data nemáme nikterak označena. Tj. nemáme žádnou veličinu, kterou bychom u trénovacích dat znali a snažili se ji naučit predikovat. Cílem nesupervizovaného učení je porozumět struktuře dat pouze na základě nich samotných. To znamená bez nějakého vnějšího vodítka.
Proto se nesupervizovanému učení také říká učení bez učitele.

\begin{itemize}
\item Označováno taktéž jako \textbf{učení bez učitele}.
\item Příklady: 
Hierarchické shlukování (angl. Hierarchical Clustering), k-Means, DBSCAN (Density-Based Spatial Clustering), Asociační pravidla (Association Rules), PCA (Principal Component Analysis), Hidden Markov Models. 
\end{itemize}

\end{itemize}

\subsection{Nominální a ordinální příznaky}

\begin{itemize}

\item {\bf Nominální příznaky (angl. Nominal features):} Jsou kategorické příznaky, které nemají žádné přirozené uspořádání mezi kategoriemi. Kategorie jsou pouze odlišné "jmény" \ nebo "štítky" \ bez vztahu nadřazenosti či podřazenosti.

\item Příklady:
\begin{itemize}
\item Barva: \{červená, modrá, zelená\}
\item Jména: \{Pavel, Ivan, $...$\}
\item Město: \{Praha, Brno, Ostrava\}
\end{itemize}

\item {\bf Ordinální příznaky (angl. Ordinal features):} Jsou kategorické příznaky, které mají přirozené uspořádání mezi kategoriemi. Lze říct, že jedna kategorie je "větší", "lepší" nebo "vyšší" než druhá, ale rozdíly mezi kategoriemi nemusí být stejné.

\item Příklady:
\begin{itemize}
    \item Vzdělání: \{základní $<$ střední $<$ vysokoškolské\}
    \item Velikost trička: \{XS $<$ S $<$ M $<$ L $<$ XL\}
    \item Hodnocení: \{špatné $<$ průměrné $<$ dobré $<$ výborné\}
\end{itemize}
\end{itemize}

\subsection{Singulární a regulární matice}

\begin{itemize}

\item {\bf Regulární matice (angl. Invertible/Non-singular matrix):}
\begin{itemize}
\item Čtvercová matice $\mathbf{A} \in \mathbb{R}^{n \times n}$, pro kterou existuje inverzní matice $\mathbf{A}^{-1}$ taková, že $\mathbf{A}\mathbf{A}^{-1} = \mathbf{A}^{-1}\mathbf{A} = I$.
\item Ekvivalentní podmínky pro regularitu:
\begin{itemize}
\item $\det(\mathbf{A}) \neq 0$
\item Sloupce (resp. řádky) $\mathbf{A}$ jsou lineárně nezávislé
\item Hodnost $\text{rank}(\mathbf{A}) = n$
\item Soustava $\mathbf{A}\mathbf{x} = \mathbf{b}$ má pro každé $\mathbf{b}$ právě jedno řešení
\end{itemize}
\end{itemize}

\item {\bf Singulární matice (angl. Singular matrix):}
\begin{itemize}
\item Čtvercová matice $\mathbf{A} \in \mathbb{R}^{n \times n}$, pro kterou neexistuje inverzní matice.
\item Ekvivalentní podmínky pro singularitu:
\begin{itemize}
\item $\det(\mathbf{A}) = 0$
\item Sloupce (resp. řádky) $\mathbf{A}$ jsou lineárně závislé
\item Hodnost $\text{rank}(\mathbf{A}) < n$
\item Existuje nenulový vektor $\mathbf{v} \neq \mathbf{0}$ takový, že $\mathbf{A}\mathbf{v} = \mathbf{0}$
\end{itemize}
\end{itemize}

\item {\bf Souvislost s lineární regresí} (viz sekce \ref{sec:linear_regression}}):

\begin{itemize}
\item Normální rovnice $\mathbf{X}^T\mathbf{X}\mathbf{w} = \mathbf{X}^T\mathbf{Y}$ má jednoznačné řešení, pokud je matice $\mathbf{X}^T\mathbf{X}$ regulární.
\item Pokud je $\mathbf{X}^T\mathbf{X}$ singulární (lineárně závislé sloupce, kolinearita příznaků), nelze přímo vypočítat $(\mathbf{X}^T\mathbf{X})^{-1}$.
\item {\bf Řešení:} SVD rozklad poskytuje stabilní výpočet pomocí Moorovy-Penroseovy pseudoinverze i pro singulární případy: $\hat{\mathbf{w}} = \mathbf{V}\boldsymbol{\Sigma}^+\mathbf{U}^T\mathbf{Y}$.
\end{itemize}

\end{itemize}

\subsection{Parametry vs. hyperparametry}

\begin{itemize}

\item {\bf Parametry (angl. Parameters):} Jsou hodnoty, které se model učí přímo z trénovacích dat během trénování. Parametry definují naučený model a používají se při predikci.

\begin{itemize}
\item Příklady: Váhy $\mathbf{w}$ v lineární regresi, pravděpodobnosti přechodů v HMM, váhy neuronové sítě.
\item Parametry se {\bf optimalizují automaticky} během trénování (např. minimalizací ztrátové funkce).
\end{itemize}

\item {\bf Hyperparametry (angl. Hyperparameters):} Jsou hodnoty, které musí být nastaveny {\bf před} zahájením trénování a řídí samotný proces učení nebo strukturu modelu. Model se je neučí z dat.

\begin{itemize}
\item Příklady: \texttt{max\_depth} u rozhodovacích stromů, počet sousedů $k$ v kNN, learning rate u gradientního sestupu, regularizační parametr $\lambda$ v hřebenové regresi.
\item Hyperparametry se {\bf ladí ručně} nebo pomocí technik jako grid search nebo random search na validační množině.
\end{itemize}

\item {\bf Shrnutí:}
\begin{itemize}
\item {\bf Parametry}: Učí se z trénovacích dat (např. $\mathbf{w}$ v $Y = \mathbf{w}^T\mathbf{x}$).
\item {\bf Hyperparametry}: Nastavují se před trénováním (např. $k$ v kNN).
\end{itemize}

\end{itemize}

\subsection{Parametrické vs. neparametrické modely}

\begin{itemize}

\item {\bf Parametrické modely (angl. Parametric models):} Předpokládají {\bf pevnou funkční formu}. Počet parametrů je fixní (nezávisí na $N$, kde $N$ je počet řádků dat).

\begin{itemize}
\item Model si pamatuje jen pár čísel (váhy), trénovací data můžeme zahodit.
\item Příklad: Lineární regrese — předpokládá $Y = \mathbf{w}^T\mathbf{x}$. Pro $p$ příznaků má $p+1$ vah. Natrénuješ na 1 000 000 bodech → pamatuješ jen $p+1$ čísel. Predikce = dosazení do vzorce.
\item Další příklady: Logistická regrese, naivní Bayesův klasifikátor, neuronové sítě (fixní architektura = fixní počet vah).
\item Výhody: Rychlá predikce, méně paměti, interpretovatelné.
\end{itemize}

\item {\bf Neparametrické modely (angl. Non-parametric models):} {\bf Žádný předpoklad} o funkční formě. Složitost modelu roste s $N$, kde $N$ je počet řádků dat.

\begin{itemize}
\item Model si pamatuje celá trénovací data (nebo strukturu odvozenou z dat).
\item Příklad: kNN — žádný předpoklad, jestli to bude přímka, parabola, nebo cokoliv jiného. Musí mít uložené všechny trénovací body. Natrénuješ na 1 000 000 bodech → musíš pamatovat všech 1 000 000 bodů. Predikce = projet všechny body a najít $k$ nejbližších.
\item Další příklady: Rozhodovací stromy (složitost stromu roste s daty), kernel regrese.
\item Nevýhody: Pomalé predikce (musíš projet data), více paměti.
\end{itemize}

\item {\bf Poznámka:} Název "neparametrický" je zavádějící — tyto modely mají parametry, ale jejich počet není fixní a roste s daty.

\end{itemize}

\subsection{Přeučení vs. podučení (Overfitting vs. Underfitting)}

\begin{itemize}

\item {\bf Přeučení (angl. Overfitting):} Nastává, když se model {\bf příliš přizpůsobí} trénovacím datům včetně náhodného šumu $\varepsilon$, takže dobře funguje na trénovací množině, ale špatně generalizuje na nová (testovací) data.

\begin{itemize}
\item {\bf Příznaky:} Nízká chyba na trénovací množině, vysoká chyba na testovací množině.
\item {\bf Příčiny:} Příliš složitý model, málo trénovacích dat, žádná regularizace.
\item {\bf Příklady:} Rozhodovací strom s neomezenou hloubkou, kNN s $k=1$, polynomiální regrese vysokého stupně.
\item {\bf Řešení:} Regularizace (ridge, lasso), omezení složitosti modelu (\texttt{max\_depth}), více dat, cross-validace, early stopping.
\end{itemize}

\item {\bf Podučení (angl. Underfitting):} Nastává, když je model {\bf příliš jednoduchý} a nedokáže zachytit skutečný vztah mezi příznaky a vysvětlovanou proměnnou. Špatně funguje jak na trénovací, tak na testovací množině.

\begin{itemize}
\item {\bf Příznaky:} Vysoká chyba na trénovací i testovací množině.
\item {\bf Příčiny:} Příliš jednoduchý model, nedostatek příznaků, příliš silná regularizace.
\item {\bf Příklady:} Lineární regrese pro silně nelineární data, rozhodovací strom s \texttt{max\_depth=1}, kNN s velmi velkým $k$.
\item {\bf Řešení:} Složitější model, více příznaků, feature engineering, snížení regularizace.
\end{itemize}

\item {\bf Bias-Variance Trade-off:}
\begin{itemize}
\item {\bf Podučení:} Vysoký bias (model je systematicky nepřesný), nízká variance.
\item {\bf Přeučení:} Nízký bias, vysoká variance (model je přecitlivělý na konkrétní trénovací data).
\item {\bf Cíl:} Najít rovnováhu mezi biasem a variancí pomocí ladění hyperparametrů na validační množině.
\end{itemize}

\end{itemize}



\section{Rozhodovací stromy (Decision Trees)}

\begin{shaded}
\noindent\textbf{Otázka ke zkoušce 1 (Rozhodovací stromy):} 
Algoritmus ID3, kritéria pro větvení (entropie, gini, MSE), použití pro klasifikaci a regresi, hyperparametry rozhodovacích stromů.
\end{shaded}

\subsection{Rozhodovací stromy: výhody a nevýhody}

\begin{itemize}

\item Neparametrický model -- počet parametrů (uzlů stromu) není 
fixní a závisí na datech. Strom si pamatuje pouze pravidla větvení 
a predikce v~listech, ne samotná trénovací data.

\item Výhody:
\begin{itemize}
\item Nenáročnost na  přípravu dat.
\item Jednoduché.
\item Srozumitelné.
\item Relativně rychlé učení.
\item Dobře interpretovatelné.
\end{itemize}

\item Nevýhody:
\begin{itemize}
\item Nerobustní, tj. drobná změna v trénovacích datech může znamenat zásadní změnu struktury výsledného stromu.
\item Většina implementací podporuje pouze binární stromy.
\item Snadno se přeučí (tj. overfitting).
\item Najít optimální strom hrubou silou je neproveditelné kvůli obrovskému počtu možných stromů.
\end{itemize}

\end{itemize}

\subsection{Rozhodovací stromy: konstrukce}

\begin{itemize}

\item Stromů existuje obrovské množství, tj. nemůžeme vyzkoušet všechny možnosti a vybrat tu nejlepší.

\item Pro klasifikaci (angl. c4lassification) používám \textbf{Entropii} (angl. Entropy) či \textbf{Gini index} (angl. Gini impurity), pro regresi (angl. regression) \textbf{MSE} (Mean Squared Error). Moderní implementace (CART) umí obojí.

\item Protože nalezení optimálního stromu je NP-úplný problém, používají se v praxi \textbf{hladové algoritmy} (angl. Greedy Algorithms) (např. ID3, C4.5, C5 či CART), které strom budují rekurzivně výběrem příznaku, jenž v daném kroku nejlépe rozděluje data (maximalizuje informační zisk).
\begin{itemize}
\item {\bf ID3 (Iterative Dichotomiser 3):} Je určen primárně pro klasifikaci (pracuje s Entropií (angl. Entropy) či alternativně s Gini Index (angl. Gini impurity) pro diskrétní třídy). Vylepšené verze jsou C4.5 a C5. 
\item {\bf CART (Classification and Regression Trees):} Moderní algoritmus (používaný v \texttt{scikit-learn}). Pro klasifikaci typicky využívá \textbf{Gini index (angl. Gini impurity)} (lze i Entropii (angl. Entropy)), pro regresi (angl. regression) \textbf{MSE (Mean Squared Error)}. Vytváří binární stromy. 
\end{itemize}

\end{itemize}

\subsection{ID3 (Iterative Dichotomiser 3)}
\begin{itemize}

\item Vytvořil vědec a výzkumník Ross Quinlan.

\item {\bf Pseudokód:}
\begin{itemize}
\item {\bf Vstup}: Máme $N$ řádkovou tabulku s hodnotami pro binární proměnnou a $p$ binárních příznaků (angl. features) $X_1, X_2, \dots, X_p$.

\item {\bf 1. krok:} Spočítáme informační zisk pro každý příznak a vybereme (resp. sloupec), kde je informační zisk největší.

\item {\bf 2. krok:} Vytvoříme uzel stromu. Jelikož jsou naše příznaky binární, strom se v tomto uzlu rozdělí na dvě větve (splní/nesplní podmínku).

\item {\bf Rekurze:} Data se rozdělí na dvě hromádky a celý postup opakujeme pro tyto menší hromádky, ale již bez příznaků, které jsme vybrali s největším informačním ziskem. 

\item {\bf Konec:} Zastavovací kritérium rekurze je maximální hloubka, vyčerpání příznaků či když data jsou dokonale čistá ($Entropie = 0$), tj. všechny instance v uzlu patří do stejné třídy.
\end{itemize}

\item {\bf Entropie (angl. Entropy):}
\label{sec:entropie}
\begin{itemize}
\item Entropie je míra neuspořádanosti množiny dat.
\item Nechť je dána množina $\mathcal{D}$ nul a jedniček (nebo i více hodnot) a chceme nějak změřit,
jak moc je uspořádaná.
\item Nechť $p_0$ a $p_1$ označují poměry počtu $0$ resp. $1$ v
množině $\mathcal{D}$ (tj. $p_0 + p_1 = 1$).
\item Taková míra by měla splnovat:
\begin{enumerate}
\item Míra by měla být nezáporná.
\item Pokud jsou v množině $\mathcal{D}$ např. samé nuly (tj. $p_0 = 1$), měla by být neuspořádanost nulová.
\item Měla by být maximální, pokud jsou počty nul a jedniček stejné, tj. když
$p_0 = p_1 = 1/2$.
\item Měla by to být rostoucí funkce $p_0$ na intervalu $[0,1/2]$ a klesající na intervalu
$[1/2,1]$.
\end{enumerate}
\item Taková funkce měřící neuspořádanost existuje a říká se jí entropie:
$$H(\mathcal{D}) = -p_0 \log p_0 - p_1 \log p_1 = -p_0 \log p_0 - (1 - p_0) \log(1 - p_0)$$
Ve vzorci se nejčastěji používá dvojkový logaritmus. V takovém
případě se jednotce entropie říká {\bf bit}. (např. $H(\mathcal{D})$ se bude rovnat $0.81$ a jednotce se říká $0.81$ bitu, tj. $H(\mathcal{D}) = 0.81 \ bitu$).
\end{itemize}

\item {\bf Informační zisk (angl. Information Gain):}
\label{sec:information_gain}
\begin{itemize}
\item Informační zisk je číslo (metrika), které určuje, o kolik se snížila neuspořádanost (entropie) dat poté, co 
jsme je rozdělili podle určitého příznaku.

\item Formálně:
$$ IG(\mathcal{D}, X_i) = H(\mathcal{D}) - t_0H(\mathcal{D}_0) - t_1H(\mathcal{D}_1), \ i \in \{1,\ldots,p\} $$

kde pro $v \in \{0, 1\}$ je $\mathcal{D}_v = \{ \mathbf{x} \in \mathcal{D} \mid x_i = v \}$ a $t_v$ je podíl počtu prvků v $\mathcal{D}_v$ a $\mathcal{D}$, neboli $t_v = \frac{\left|\mathcal{D}_v\right|}{\left|\mathcal{D}\right|}$. V tomto vzorci $X_i$ označuje $i$-tý sloupec (resp. příznak) matice $\mathbf{X}$, což je matice dat.
\end{itemize}

%%%%%%%%%%%%%%%%

\item {\bf Příklad výpočtu entropie a informačního zisku:}

Mějme následující trénovací data s vysvětlovanou proměnnou $Y$ (vstal/nevstal) a dvěma binárními příznaky:

\begin{table}[h]
\centering
\begin{tabular}{c|cc|c}
\textbf{id} & \textbf{teplota} $> 39^\circ$\textbf{C} ($X_1$) & \textbf{bolest hlavy} ($X_2$) & \textbf{vstal(a)?} ($Y$) \\
\hline
1 & ano & ano & ne \\
2 & ne & ne & ano \\
3 & ne & ne & ano \\
4 & ano & ano & ne \\
5 & ano & ano & ne \\
6 & ne & ano & ano \\
\end{tabular}
\caption{Trénovací data pro výpočet entropie a informačního zisku.}
\label{tab:entropy_example}
\end{table}

{\bf Poznámka k použití logaritmu:} Ve vzorci pro entropii používáme logaritmus z praktických důvodů. Při násobení pravděpodobností (čísel z intervalu $[0,1]$) bychom rychle dostávali extrémně malá čísla. To je problém, protože počítače mají omezenou přesnost reprezentace čísel s plovoucí řádovou čárkou — při příliš malých hodnotách dochází k tzv. {\bf numerickému podtečení (angl. underflow)}, kdy je číslo zaokrouhleno na nulu a ztrácíme veškerou informaci. Logaritmus má tu vlastnost, že převádí násobení na sčítání: $\log(a \cdot b) = \log a + \log b$, čímž se tomuto problému vyhneme — místo součinu velmi malých čísel sčítáme záporná čísla běžných velikostí.

{\bf Krok 1: Výpočet entropie celého datasetu $H(\mathcal{D})$}

V datasetu máme 6 záznamů: 3 s $Y = 1$ (vstal) a 3 s $Y = 0$ (nevstal). Tedy:
$$p_1 = \frac{3}{6} = \frac{1}{2}, \quad p_0 = \frac{3}{6} = \frac{1}{2}$$

Dosadíme do vzorce pro entropii:
\begin{align*}
H(\mathcal{D}) &= -p_0 \log_2 p_0 - p_1 \log_2 p_1 \\
&= -\frac{1}{2} \log_2 \frac{1}{2} - \frac{1}{2} \log_2 \frac{1}{2} \\
&= -\frac{1}{2} \cdot (-1) - \frac{1}{2} \cdot (-1) \\
&= \frac{1}{2} + \frac{1}{2} = 1 \ \text{bit}
\end{align*}

Entropie 1 bit odpovídá maximální neuspořádanosti pro binární proměnnou (obě třídy jsou stejně zastoupeny).

{\bf Krok 2: Výpočet informačního zisku pro příznak $X_1$ (teplota $> 39^\circ$C)}

Rozdělíme data podle příznaku $X_1$:
\begin{itemize}
\item $\mathcal{D}_1 = \{$záznamy kde $X_1 = 1$ (ano)$\}$ = \{1, 4, 5\} $\Rightarrow$ 3 záznamy, z toho 0 vstal, 3 nevstal
\item $\mathcal{D}_0 = \{$záznamy kde $X_1 = 0$ (ne)$\}$ = \{2, 3, 6\} $\Rightarrow$ 3 záznamy, z toho 3 vstal, 0 nevstal
\end{itemize}

Váhy podmnožin: $t_1 = \frac{3}{6} = \frac{1}{2}$, $t_0 = \frac{3}{6} = \frac{1}{2}$

Entropie podmnožin:
\begin{itemize}
\item Pro $\mathcal{D}_1$: $p_1 = 0$, $p_0 = 1$ $\Rightarrow$ $H(\mathcal{D}_1) = -0 \cdot \log_2 0 - 1 \cdot \log_2 1 = 0$ bit (konvence: $0 \cdot \log_2 0 = 0$)
\item Pro $\mathcal{D}_0$: $p_1 = 1$, $p_0 = 0$ $\Rightarrow$ $H(\mathcal{D}_0) = -1 \cdot \log_2 1 - 0 \cdot \log_2 0 = 0$ bit
\end{itemize}

Informační zisk pro $X_1$:
\begin{align*}
IG(\mathcal{D}, X_1) &= H(\mathcal{D}) - t_1 H(\mathcal{D}_1) - t_0 H(\mathcal{D}_0) \\
&= 1 - \frac{1}{2} \cdot 0 - \frac{1}{2} \cdot 0 \\
&= 1 \ \text{bit}
\end{align*}

{\bf Krok 3: Výpočet informačního zisku pro příznak $X_2$ (bolest hlavy)}

Rozdělíme data podle příznaku $X_2$:
\begin{itemize}
\item $\mathcal{D}_1 = \{$záznamy kde $X_2 = 1\}$ = \{1, 4, 5, 6\} $\Rightarrow$ 4 záznamy, z toho 1 vstal, 3 nevstal
\item $\mathcal{D}_0 = \{$záznamy kde $X_2 = 0\}$ = \{2, 3\} $\Rightarrow$ 2 záznamy, z toho 2 vstal, 0 nevstal
\end{itemize}

Váhy: $t_1 = \frac{4}{6} = \frac{2}{3}$, $t_0 = \frac{2}{6} = \frac{1}{3}$

Entropie podmnožin:
\begin{itemize}
\item Pro $\mathcal{D}_1$: $p_1 = \frac{1}{4}$, $p_0 = \frac{3}{4}$ $\Rightarrow$ $H(\mathcal{D}_1) = -\frac{1}{4} \log_2 \frac{1}{4} - \frac{3}{4} \log_2 \frac{3}{4} \approx 0.81$ bit
\item Pro $\mathcal{D}_0$: $p_1 = 1$, $p_0 = 0$ $\Rightarrow$ $H(\mathcal{D}_0) = 0$ bit
\end{itemize}

Informační zisk pro $X_2$:
\begin{align*}
IG(\mathcal{D}, X_2) &= H(\mathcal{D}) - t_1 H(\mathcal{D}_1) - t_0 H(\mathcal{D}_0) \\
&= 1 - \frac{2}{3} \cdot 0.81 - \frac{1}{3} \cdot 0 \\
&\approx 1 - 0.54 = 0.46 \ \text{bit}
\end{align*}

{\bf Závěr:} Příznak $X_1$ (teplota $> 39^\circ$C) má informační zisk 1 bit, zatímco $X_2$ (bolest hlavy) má pouze 0.46 bit. Algoritmus ID3 by tedy v prvním kroku vybral příznak $X_1$ pro větvení, protože má největší informační zisk — po rozdělení podle tohoto příznaku jsou obě podmnožiny zcela homogenní (entropie 0).
%%%%%%%%%%%%%%%%

\item {\bf Gini Index (angl. Gini impurity):}
\label{sec:gini_index}
\begin{itemize}
\item Gini Index je alternativní metrika k entropii. Má podobné vlastnosti jako Entropie. 
\item Je to jakási míra toho, že nově přidaný prvek bude špatně klasifikován.
\item Jinak ale vše funguje stejně, jen se nahradí $H(\mathcal{D})$ výrazem $GI(\mathcal{D})$.
\item Pro množinu $\mathcal{D}$ s $k$ různými hodnotami Gini Index formálně definujeme jako:
$$GI(\mathcal{D}) = 1 - \sum_{i=0}^{k-1} p_i^2 = \sum_{i=0}^{k-1} p_i(1 - p_i)$$

kde $p_i$ je relativní četnost tj. $$p_i = \frac{\text{počet vzorků třídy } i}{\text{celkový počet vzorků}}$$
\end{itemize}

\end{itemize}

\subsection{Regrese \& CART (Classification and Regression Trees)}
\label{sec:cart}

\begin{itemize}

\item Pro práci se spojitou vysvětlovanou proměnnou.

\item {\bf Predikce hodnoty vysvětlované proměnné:}
\begin{itemize}
    \item Obvykle bereme průměr (resp. lze i medián či modus) hodnot z listu, někdy může být zavádějící, stačí když se tam dostane jedna extrémní hodnota (angl. outlier).
\end{itemize}

\item {\bf CART (Classification and Regression Trees) pro Regresi:}
\begin{itemize}
\item Místo entropie (popř. Gini Index) použijeme MSE (Mean Squared Error), což  je skoro stejná veličina jako výběrový rozptyl (angl. Sample Variance).
\item MSE (Mean Squared Error) definujeme předpisem:
$$MSE(Y) = \frac{1}{N} \sum_{i=0}^{N-1} (Y_i - \bar{Y})^2$$

\item MSE lze nahradit i MAE (Mean Absolute Error), i když je to méně obvyklé. MAE definujeme takto:
$$MAE(Y) = \frac{1}{N} \sum_{i=0}^{N-1} |Y_i - \bar{Y}|$$
MAE není derivovatelné v bodě $Y_i = \bar{Y}$, což komplikuje gradient-based optimalizaci. Proto se v praxi častěji používá MSE.

\item Funguje stejně jako ID3 — rozdělí $\mathcal{D}$ na $\mathcal{D}_L$ a $\mathcal{D}_R$, ale místo entropie:
$$IG(\mathcal{D}, X_i) = H(\mathcal{D}) - t_L H(\mathcal{D}_L) - t_R H(\mathcal{D}_R), \ i \in \{1,...,p\}$$
používá:
$$IG(\mathcal{D}, X_i) = MSE(\mathcal{D}) - t_L MSE(\mathcal{D}_L) - t_R MSE(\mathcal{D}_R), \ i \in \{1,...,p\}$$
kde $t_L = \frac{|\mathcal{D}_L|}{|\mathcal{D}|}$ a $t_R = \frac{|\mathcal{D}_R|}{|\mathcal{D}|}$. V tomto vzorci $X_i$ označuje $i$-tý sloupec (resp. příznak) matice $\mathbf{X}$, což je matice dat.
\end{itemize}

\end{itemize}

\subsection{Hyperparametry rozhodovacích stromů}

\begin{itemize}

\item {\bf Ladění rozhodovacích stromů:}
\begin{itemize}
\item \texttt{max\_depth}: Hloubka stromu.
\item \texttt{criterion}: Funkce kvality splitu (gini, entropie, …).
\item \texttt{min\_samples\_split}: Minimální počet prvků potřebných pro rozdělení vnitřního uzlu.
\item \texttt{min\_samples\_leaf}: Minimální počet vzorků, které musí obsahovat každý list (koncový uzel).
\end{itemize}

\item {\bf Nebinární příznaky (one-hot-encoding):}

\begin{itemize}
\item Jeden kategorický příznak s $n$ různými hodnotami se nahradí $n-1$ binárními dummy proměnnými.
\item Mohli bychom vytvořit i sloupec pro "spadl", ale byl by redundantní, protože informace je již obsažena v předchozích dvou sloupcích.
\item Příklad: příznak "stav" s hodnotami \{vstal, nevstal, spadl\} se zakóduje pomocí dvou binárních příznaků následovně:
\end{itemize}

\begin{table}[h]
\centering
\begin{tabular}{c|cc}
\textbf{Původní příznak} ($X$) & $X_{\text{vstal}}$ & $X_{\text{nevstal}}$ \\
\hline
vstal & 1 & 0  \\
nevstal & 0 & 1 \\
spadl & 0 & 0 \\
\end{tabular}
\caption{Dummy encoding: 3 kategorie zakódovány pomocí 2 binárních příznaků ($n-1$).}
\label{tab:dummy_encoding}
\end{table}

\item Nevýhody:
\begin{itemize}
    \item Zvyšuje počet příznaků (dimenzi datasetu).
    \item Není vhodná pro ordinální příznaky (uspořádané jako základní $<$ střední $<$ univerzitní), je vhodnější pro nominální (mezi kategoriemi není žádný vztah).
\end{itemize}

\item {\bf Spojité příznaky:}
\begin{itemize}
\item Moderní algoritmy (CART, C4.5, C5) zpracovávají spojité příznaky automaticky pomocí porovnání s prahem (např. $X < 5.2$).
\item {\bf Hledání optimálního prahu:} Algoritmus seřadí hodnoty příznaku a pro každý možný prah spočítá informační zisk. Vybere práh s největším ziskem.

\item Stejný příznak lze použít opakovaně s různými prahy v různých částech stromu (např. "věk $< 30$", později "věk $< 15$").
\item {\bf Poznámka:} Původní ID3 vyžaduje diskretizaci spojitých příznaků. C4.5, C5 a CART je zpracovávají automaticky.
\end{itemize}
\end{itemize}

\subsection{Zajímavá videa na YouTube}

\begin{itemize}
\item \href{https://youtu.be/YtebGVx-Fxw?si=Cj4XiEApsAgYF5YE}{Entropy (for data science) Clearly Explained!!! (StatQuest with Josh Starmer)} \hfill \textbf{[16~min]}

\item \href{https://youtu.be/9r7FIXEAGvs?si=2px9DQ6yIkzj6x-u}{Shannon Entropy and Information Gain
(Serrano.Academy)} \hfill \textbf{[21~min]}

\end{itemize}

\section{kNN (k-Nearest Neighbor)}

\begin{shaded}
\noindent\textbf{Otázka ke zkoušce 2 (Metoda nejbližších sousedů: kNN):} 
Popis metody, hyperparametrů a jejího použití pro klasifikaci a regresi. Pojem metrika a normalizace dat.
\end{shaded}

\subsection{kNN: základní popis}

\begin{itemize}

\item {\bf kNN (k-Nearest Neighbors):} Metoda nejbližších sousedů je algoritmus pro supervizované učení, který funguje jak pro klasifikaci, tak pro regresi.

\item {\bf Princip metody:}
\begin{itemize}
\item Má se predikovat hodnota vysvětlované proměnné pro datový bod $\mathbf{x} \in \mathbb{R}^p, p \in \N$.
\item V trénovacích datech najdeme $k$ (zadaný hyperparametr) nejbližších bodů k $\mathbf{x}$.
\item Vzdálenost měříme pomocí \textbf{metriky} — funkce, pro kterou platí:
\begin{enumerate}
\item pozitivní definitnost
\item symetrie
\item trojúhelníková nerovnost
\end{enumerate}
\item Příklad metriky je Euklidovská vzdálenost.
\item Predikci založíme na známých hodnotách vysvětlované proměnné pro těchto $k$ bodů:
\begin{itemize}
\item {\bf Pro regresi:} Bereme průměr (popřípadě medián) hodnot.
\item {\bf Pro klasifikaci:} Bereme nejčastější hodnotu (modus).
\end{itemize}
\end{itemize}

\item {\bf kNN učení vs. predikování:}
\begin{itemize}
\item Pro většinu metod pro supervizované učení platí, že řádově více je náročné učení modelu (trénování, vytváření stromu) než výpočet predikcí.
\item U kNN je tomu obráceně — \textbf{učení neprobíhá}: trénovací data jsou sama o sobě naučeným modelem, náročná je predikce — hledání nejbližších sousedů (lze zrychlit indexací).
\end{itemize}

\end{itemize}

\subsection{kNN: hyperparametry}
\label{sec:knn_hyperparametry}

\begin{itemize}

\item {\bf Číslo $k$ určující počet nejbližších sousedů:}
\begin{itemize}
\item Čím větší počet sousedů ($k$), tím menší šance na přeučení (pokud bereme všechny — děláme průměr ze všech, pokud bereme jeden — blížíme se konkrétním trénovacím datům).
\end{itemize}

\item {\bf Použitá vzdálenost (metrika):}
\begin{itemize}
\item Nejoblíbenější volbou jsou tzv. $\mathbf{k}$\textbf{-normy} (také $L_k$ případně Minkovského $k$-metriky):
$$\|\mathbf{x} - \mathbf{y}\|_k = d_k(\mathbf{x}, \mathbf{y}) = \sqrt[k]{\sum_{i=1}^{p} |x_i - y_i|^k}$$
kde $k \in \N$.

\item Pro $k = 2$ dostáváme \textbf{Euklidovskou vzdálenost}:
$$\|\mathbf{x} - \mathbf{y}\|_2 = d_2(\mathbf{x}, \mathbf{y}) = \sqrt{\sum_{i=1}^{p} (x_i - y_i)^2}$$

\item Pro $k = 1$ dostáváme \textbf{Manhattanskou vzdálenost}:
$$\|\mathbf{x} - \mathbf{y}\|_1 = d_1(\mathbf{x}, \mathbf{y}) = \sum_{i=1}^{p} |x_i - y_i|$$
\end{itemize}

\item {\bf Váhy nejbližších sousedů:}
\begin{itemize}
\item Co, když jeden parametr bude v mm a druhý v m?
\item Co, když jeden je v počtu pokojů a druhý parametr v úhlopříčce televize?
\item Je potřeba \textbf{normalizace} do intervalu $[0, 1]$ nebo \textbf{standardizace}.
\end{itemize}

\end{itemize}

\subsection{Normalizace dat}
\label{sec:minmax_normalizace}

\begin{itemize}
\item {\bf Min-max normalizace (do intervalu $[0, 1]$):}
\label{sec:minmax_norm}
\begin{itemize}
\item Pro daný příznak najdeme jeho minimální a maximální hodnotu v trénovacích datech: $\min_x$, $\max_x$.
\item Pak hodnotu $x_i$ tohoto příznaku pro $i$-tý datový bod nahradíme:
$$x_i \leftarrow \frac{x_i - \min_x}{\max_x - \min_x}$$
\item Tím docílíme toho, že všechny hodnoty budou z intervalu $[0, 1]$.
\item {\bf Nevýhody:}
\begin{itemize}
\item Této metodě škodí outliers (odlehlé hodnoty).
\item Zahazujeme informaci o vztahu mezi příznaky (např. jeden je 2x co druhý).
\item Obecně složité téma, neexistuje univerzální správný přístup.
\end{itemize}
\item V \texttt{scikit-learn} je pod názvem \texttt{MinMaxScaler}.
\end{itemize}

\item {\bf Standardizace (z-score normalizace):}
\begin{itemize}
\item Alternativa normalizace.
\item Pro každý příznak nalezneme jeho výběrový průměr $\bar{x}$ a výběrový rozptyl $s_x^2$ a každý $i$-tý bod nahradíme:
$$x_i \leftarrow \frac{x_i - \bar{x}}{\sqrt{s_x^2}}$$
kde $\bar{x}$ je výběrový průměr a $s_x^2$ je výběrový rozptyl.
\item Také citlivé na outliers, ale méně nežli \hyperref[sec:minmax_norm]{Min-max normalizace}.
\item Převádí na škálu s $\bar{x}=0$ (výběrový průměr) a $\sqrt{s_x^2} = 1$ (výběrová směrodatná odchylka).
\item V \texttt{scikit-learn} je pod názvem \texttt{StandardScaler}.
\end{itemize}

\item {\bf Další metody škálování v \texttt{scikit-learn}:}
\begin{itemize}
\item \texttt{RobustScaler} - používá medián a mezikvartilové rozpětí (IQR), odolnější vůči outliers než standardizace.
\item \texttt{MaxAbsScaler} - škáluje podle maximální absolutní hodnoty do intervalu $[-1, 1]$.
\item A další (\texttt{Normalizer}, \texttt{QuantileTransformer}, \texttt{PowerTransformer}, atd.)
\end{itemize}
\end{itemize}

\subsection{kNN a nominální příznaky}

\begin{itemize}

\item kNN se bez použití speciálních metrik neumí dobře vypořádat s nominálními příznaky (neuspořádanými).

\item Můžeme použít \textbf{one-hot encoding} a \textbf{dummy příznaky} (viz. sekce Rozhodovací stromy).

\end{itemize}

\subsection{Prokletí dimenzionality (The Curse of Dimensionality)}

\begin{itemize}

\item {\bf Prokletí dimenzionality (angl. The curse of dimensionality):} Je pojem, který odkazuje na některé problémy objevující se v případě vysokého počtu příznaků, kdy jsou datové body prvky mnohadimenzionálního prostoru.

\item Proto je potřeba někdy provést \textbf{redukci dimenzionality}, např. výběr příznaků (\textbf{Feature Selection}).

\item {\bf S kNN jsou spojeny zejména 2 efekty způsobené vysokou dimenzí:}
\begin{itemize}
\item Data se zvyšováním dimenzí řídnou a navzájem se vzdalují. Pro zachování stejné hustoty pro vyšší dimenzi by bylo nutné řádově navýšit počet datových bodů, což většinou není možné.
\item S rostoucí dimenzí se pro klasické metriky zmenšují rozdíly mezi vzdálenými a blízkými body.
\end{itemize}

\item Ve velké dimenzi jsou všichni sousedi daleko, takže jejich zprůměrováním nedostaneme dobrý odhad.

\item Příklad 1:

\begin{center}
\includegraphics[width=0.8\textwidth]{assets/cod.png}
\label{fig:curse_dim}
\end{center}
\begin{center}
\text{Obrázek 1: Prokletí dimenzionality (řídnutí bodů)}
\end{center}

Obrázek 1 ilustruje \textbf{řídnutí dat} (angl. Data Sparsity) s rostoucí dimenzí. Stejný počet bodů je v 1D hustě vedle sebe, v 2D se rozloží do plochy a jsou řidší, v 3D se rozloží do prostoru a vzdálenosti mezi nimi dramaticky rostou. Data se zvyšováním dimenze řídnou a navzájem se vzdalují — hustota trénovacích bodů s rostoucí dimenzí klesá.

\item Příklad 2: Mějme $d$-dimenzionální jednotkovou hyperkrychli: $$[0,1]^d, d \in \N$$ s $1000$ náhodně rozmístěnými body. Chceme najít oblast, která obsahuje v průměru $10$ bodů, tj. pokrývá 1\% celkového objemu. {\bf Otázka:} Jak velkou hyperkrychli o straně $a$ potřebujeme, aby obsahovala 1\% objemu?

V dimenzi $d$ musí platit: $a^d = 0.01$, tedy $a = \sqrt[d]{0.01}$.

{\bf Konkrétní výpočet:}
\begin{itemize}
    \item {\bf 1D:} $a = 0.01$ — potřebujeme 1\% délky.
    \item {\bf 2D:} $a = \sqrt{0.01} = 0.1$ — potřebujeme 10\% na každou stranu.
    \item {\bf 3D:} $a = \sqrt[3]{0.01} \approx 0.215$ — potřebujeme 21.5\% na každou stranu.
    \item {\bf 5D:} $a = \sqrt[5]{0.01} \approx 0.398$ — potřebujeme skoro 40\% na každou stranu!
    \item {\bf 10D:} $a = \sqrt[10]{0.01} \approx 0.63$ — potřebujeme 63\% na každou stranu.
    \item {\bf 50D:} $a = \sqrt[50]{0.01} \approx 0.91$ — potřebujeme 91\% na každou stranu!
    \item {\bf 999D:} $a = \sqrt[999]{0.01} \approx 0.9954$ — potřebujeme 99.54\% na každou stranu — téměř celý prostor!
\end{itemize}

{\bf Co to znamená?} Ačkoliv ve všech dimenzích pokrýváme stejné procento \textbf{objemu} (1\%), musíme v každé dimenzi prohledat stále větší a větší podíl \textbf{rozsahu prostoru}. V 5D je to již skoro 40\%, v 50D dokonce 91\%, a v 999D se velmi blížíme ke 100\%!

{\bf Důsledek pro kNN:} Čím ve větší dimenzi hledáme nejbližší sousedy, tím větší prostor musíme prohledat, abychom našli dostatečný počet bodů. To znamená, že nejbližší sousedi jsou ve skutečnosti velmi daleko a jejich zprůměrováním nedostaneme dobrý odhad.

\end{itemize}

\subsection{Zajímavá videa na YouTube}

\begin{itemize}
\item \href{https://youtu.be/b6uHw7QW_n4?si=y__FINNjGKAyFuN5}{What is the K-Nearest Neighbor (KNN) Algorithm?
 (IBM Technology)} (vhodné pro prvotní myšlenku) \hfill \textbf{[8~min]}

\item \href{https://youtu.be/HVXime0nQeI?si=6jSZqkGOz5iFNzla}{StatQuest: K-nearest neighbors, Clearly Explained (StatQuest with Josh Starmer)} (prvotní myšlenka) \hfill \textbf{[5~min]}

\item \href{https://youtu.be/PGy1rATkViA?si=qjoQ_RjN5BxL3luG}{kNN.3 Voronoi cells and decision boundary (Victor Lavrenko)} \hfill \textbf{[3~min]}

\item \href{https://youtu.be/_yNLrPxG7PE?si=urvCuHDh7Q6C8bE_}{kNN.4 Sensitivity to outliers (Victor Lavrenko)} \hfill \textbf{[2~min]}

\item \href{https://youtu.be/maAeBlMP-3Y?si=rBw9VjRD9toYJD3P}{kNN.5 Nearest-neighbor classification algorithm (Victor Lavrenko)} \hfill \textbf{[2~min]}

\item \href{https://youtu.be/ZD_tfNpKzHY?si=kWvFnVM3kP-nnRZE}{
kNN.6 MNIST digit recognition (Victor Lavrenko)} \hfill \textbf{[4~min]}


\item \href{https://youtu.be/kbadomx9DIg?si=8O2tCtwopHLCGpur}{
kNN.7 Nearest-neighbor regression algorithm (Victor Lavrenko)} \hfill \textbf{[1~min]}

\item \href{https://youtu.be/3lp5CmSwrHI?si=iqebCEdyKAzD3dLO}{
kNN.8 Nearest-neighbor regression example (Victor Lavrenko)} \hfill \textbf{[4~min]}


\item \href{https://youtu.be/X_3Ke5zVqo4?si=N_KZp5yeCN_yI_sA}{
kNN.9 Number of nearest neighbors to use (Victor Lavrenko)} \hfill \textbf{[3~min]}

\item \href{https://youtu.be/nKOJK_9839U?si=8sCSH8I75YvxF5ky}{
kNN.10 Similarity / distance measures (Victor Lavrenko)} \hfill \textbf{[5~min]}

\item \href{https://youtu.be/dPg-35JQ7Ew?si=kh82kqrdZATXhtNp}{
kNN.11 Breaking ties between nearest neighbors (Victor Lavrenko)} \hfill \textbf{[3~min]}

\item \href{https://youtu.be/e_TKIjo62b8?si=CXjXDdrmHGxoB_u-}{
kNN.12 Parzen windows, kernels and SVM (Victor Lavrenko)} (nad rámec ML1, spadá do ML2) \hfill \textbf{[13~min]}

\item \href{https://youtu.be/aqou1ma8ZIs?si=PYywfIk6DUUMXHlj}{
kNN.13 Pros and cons of nearest-neighbor methods (Victor Lavrenko)} \hfill \textbf{[2~min]}

\item \href{https://youtu.be/UPAnUE_g5SQ?si=UE7LygUJL963YlTt}{
 kNN.14 Computational complexity of finding nearest-neighbors (Victor Lavrenko)} \hfill \textbf{[3~min]}

\item \href{https://youtu.be/LqcwaW2YE_c?si=d7dYhRchpCaDOuco}{kNN.16 Locality sensitive hashing (LSH) (Victor Lavrenko)} (rozšíření nad rámec ML1) \hfill \textbf{[7~min]}

\item \href{https://youtu.be/Mlp8hlKwETs?si=FLLWhzAAYAUrstq9}{kNN.17 Inverted index (Victor Lavrenko)} (rozšíření nad rámec ML1) \hfill \textbf{[4~min]}

\end{itemize}


\section{Lineární regrese (Linear Regression)}
\label{sec:linear_regression}

\begin{shaded}
\noindent\textbf{Otázka ke zkoušce 3 (Lineární regrese, metoda nejmenších čtverců):} 
Model lineární regrese, predikce, maticový zápis trénovací množiny. Metoda nejmenších čtverců: normální rovnice, řešení.
\end{shaded}

\subsection{Metoda nejmenších čtverců (Ordinary Least Squares)}
\label{sec:Met_NEJM_CTV}

\begin{itemize}

\item V modelu lineární regrese {\bf předpokládáme lineární závislost vysvětlované proměnné na hodnotách příznaků}, tedy: $$Y = w_0 + w_1 x_1 + w_2 x_2 + ... + w_p x_p + \varepsilon, \ p \in \N$$

kde $w_i, \ i \in \{0, 1,2,...p\}$ jsou nějaké neznámé koeficienty, které hledáme. $\varepsilon$ je náhodná veličina s $E(\varepsilon) = 0$. Koeficient $w_0$ se nazývá {\bf intercept} a odpovídá (očekávané) výchozí hodnotě $Y$ při nulových příznacích.

Pokud si označíme vektor příznaků jako $\mathbf{x} = (1, x_1, ... ,x_p)^T$ a vektor váh jako $\mathbf{w} = (w_0, w_1,...w_p)^T$, potom můžeme zapisovat kompaktněji:

$$Y = \mathbf{w}^T \mathbf{x} + \varepsilon$$

\item Měření chyby predikce pomocí ztrátové funkce (angl. loss function)

\begin{itemize}
\item Naším cílem je najít takovou hodnotu $\mathbf{w}$, aby chyba modelu byla co nejmenší.

\item Chybu modelu nejčastěji měříme pomocí nějaké nezáporné funkce $L : \mathbb{R}^2 \to \mathbb{R}$, nazývané ztrátová
funkce (angl. loss function), kterou aplikujeme na skutečnou hodnotu proměnné $Y$ a odpovídající
predikci $\hat{Y}$.

\item Obvyklou volbou v případě spojité vysvětlované veličiny bývá kvadratická ztrátová funkce,
$$L(Y, \hat{Y} ) = (Y - \hat{Y})^2$$
\end{itemize}

\item Při trénování minimalizujeme {\bf residuální součet čtverců} (angl. Residual Sum of Squares), který značíme $RSS(\mathbf{w})$ a definujeme předpisem:

$$RSS(\mathbf{w})= \sum_{i = 1}^N  L(Y_i, \mathbf{w}^T \mathbf{x}_i) = \sum_{i = 1}^N  (Y_i - \mathbf{w}^T \mathbf{x}_i)^2 = ||\mathbf{Y} - \mathbf{X}\mathbf{w}||^2$$

kde $N$ reprezentuje počet řádků v datasetu. Minimalizací tohoto výrazu získáme odhad $\hat{\mathbf{w}}$.


Toto chceme minimalizovat. Výraz si proto vhodně upravíme:

\begin{align*}
RSS(\mathbf{w}) &= \sum_{i = 1}^N (Y_i - \mathbf{w}^T \mathbf{x}_i)^2 \\
&= (\mathbf{Y} - \mathbf{X} \mathbf{w})^T(\mathbf{Y} - \mathbf{X} \mathbf{w}) \\
&= \mathbf{Y}^T\mathbf{Y} - 2\mathbf{Y}^T\mathbf{X}\mathbf{w} + \mathbf{w}^T\mathbf{X}^T\mathbf{X}\mathbf{w}
\end{align*}

Hledáme minimum, proto sestrojíme {\bf gradient} $RSS(\mathbf{w})$:

\begin{align*}
\nabla RSS(\mathbf{w}) = \frac{\partial RSS(\mathbf{w})}{\partial \mathbf{w}} &= \frac{\partial}{\partial \mathbf{w}} (\mathbf{Y}^T\mathbf{Y} - 2\mathbf{Y}^T\mathbf{X}\mathbf{w} + \mathbf{w}^T\mathbf{X}^T\mathbf{X}\mathbf{w}) \\
&= - 2\mathbf{X}^T\mathbf{Y} + 2\mathbf{X}^T\mathbf{X}\mathbf{w} 
\end{align*}

Potom {\bf Hessova matice} (angl. Hessian Matrix) $RSS(\mathbf{w})$ je:

$$
\begin{align*}
\nabla^2 RSS(\mathbf{w}) = \frac{\partial}{\partial \mathbf{w}} \left( \nabla RSS(\mathbf{w}) \right) 
&= \frac{\partial}{\partial \mathbf{w}} \left( - 2\mathbf{X}^T\mathbf{Y} + 2\mathbf{X}^T\mathbf{X}\mathbf{w} \right) \\
&= \frac{\partial}{\partial \mathbf{w}} \left( - 2\mathbf{X}^T\mathbf{Y} \right) + \frac{\partial}{\partial \mathbf{w}} \left( 2\mathbf{X}^T\mathbf{X}\mathbf{w} \right)
\\
&=2\mathbf{X}^T\mathbf{X}
\end{align*}
$$

\label{sec:proof_PD_lr}
Nyní musíme z definice dokázat, že Hessova matice je \textbf{pozitivně definitní}, abychom dokázali, že v řešení normální rovnice je lokální minimum. {\bf Předpokládejme, že $\mathbf{X}^T\mathbf{X}$ je regulární matice} (nebo ekvivalentně, že sloupce matice $\mathbf{X}$ jsou lineárně nezávislé). Pro libovolný vektor $\mathbf{v} \in \mathbb{R}^{p+1}$ platí:


$$\mathbf{v}^T(2\mathbf{X}^T\mathbf{X})\mathbf{v} = 2 \mathbf{v}^T(\mathbf{X}^T\mathbf{X})\mathbf{v}= 2 (\mathbf{X}\mathbf{v})^T(\mathbf{X}\mathbf{v})= 2||\mathbf{X}\mathbf{v}||^2 > 0$$

Z lineární nezávislosti sloupců $\mathbf{X}$ plyne, že pro $(\forall \mathbf{v} \neq \mathbf{0})(\mathbf{X}\mathbf{v} \neq \mathbf{0})$, a tedy:
$$2\|\mathbf{X} \mathbf{v}\|^2 > 0 \quad \text{pro} \ \forall \mathbf{v} \neq \mathbf{0}$$

Hessova matice je tedy pozitivně definitní, což znamená, že řešení normální rovnice $\hat{\mathbf{w}}$ je bodem ostrého lokálního minima.

Nyní, když jsme dokázali, že v řešení normální rovnice nastává lokální minimum, můžeme z podmínky $\nabla RSS(\mathbf{w}) = 0$ vyjádřit $\mathbf{w}$, čímž získáme odhad $\hat{\mathbf{w}}$:

$$
\nabla RSS(\mathbf{w}) = 0 \iff
- 2\mathbf{X}^T\mathbf{Y} + 2\mathbf{X}^T\mathbf{X}\mathbf{w} = 0 \iff \mathbf{X}^T\mathbf{X}\mathbf{w} = \mathbf{X}^T\mathbf{Y} \iff \mathbf{w} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}
$$

Odvozený výraz:
$$
\hat{\mathbf{w}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}
$$
nazýváme \textbf{řešením normální rovnice}.
\end{itemize}

\subsection{Řešení normální rovnice pomocí SVD rozkladu}
\label{sec:SVD}
\begin{itemize}
\item Pro standardní výpočet $\hat{\mathbf{w}}$ přes normální rovnici musíme vypočítat $(\mathbf{X}^T\mathbf{X})^{-1}$, což nelze pro singulární matice. Tato komplikace se řeší pomocí SVD rozkladu, který lze spočítat pro každou matici libovolných rozměrů.

\item {\bf SVD rozklad} (angl. Singular Value Decomposition) každé matice $\mathbf{X} \in \mathbb{R}^{N \times (p+1)}$ má tvar:
$$\mathbf{X} = \mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^T$$
kde $\mathbf{U} \in \mathbb{R}^{N \times N}$ a $\mathbf{V} \in \mathbb{R}^{(p+1) \times (p+1)}$ jsou ortogonální matice ($\mathbf{U}^T\mathbf{U} = I$, $\mathbf{V}^T\mathbf{V} = I$) a $\boldsymbol{\Sigma} \in \mathbb{R}^{N \times (p+1)}$ je diagonální matice se singulárními hodnotami $\sigma_1 \geq \sigma_2 \geq ... \geq \sigma_{p+1} \geq 0$ na diagonále.

\item $\hat{\mathbf{w}}$ lze proto stabilněji a univerzálněji spočítat pomocí SVD. Dosadíme-li $\mathbf{X} = \mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^T$ do řešení normální rovnice:
\begin{align*}
\hat{\mathbf{w}} &= (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y} \\
&= ((\mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^T)^T(\mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^T))^{-1}(\mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^T)^T\mathbf{Y} \\
&= (\mathbf{V}\boldsymbol{\Sigma}^T\mathbf{U}^T\mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^T)^{-1}\mathbf{V}\boldsymbol{\Sigma}^T\mathbf{U}^T\mathbf{Y} \\
&= (\mathbf{V}\boldsymbol{\Sigma}^T\boldsymbol{\Sigma}\mathbf{V}^T)^{-1}\mathbf{V}\boldsymbol{\Sigma}^T\mathbf{U}^T\mathbf{Y} \quad \text{(využili jsme $\mathbf{U}^T\mathbf{U} = I$)} \\
&= \mathbf{V}(\boldsymbol{\Sigma}^T\boldsymbol{\Sigma})^{-1}\mathbf{V}^T\mathbf{V}\boldsymbol{\Sigma}^T\mathbf{U}^T\mathbf{Y} \\
&= \mathbf{V}(\boldsymbol{\Sigma}^T\boldsymbol{\Sigma})^{-1}\boldsymbol{\Sigma}^T\mathbf{U}^T\mathbf{Y} \quad \text{(využili jsme $\mathbf{V}^T\mathbf{V} = I$)} \\
&= \mathbf{V}\boldsymbol{\Sigma}^+\mathbf{U}^T\mathbf{Y}
\end{align*}
kde $\boldsymbol{\Sigma}^+ = (\boldsymbol{\Sigma}^T\boldsymbol{\Sigma})^{-1}\boldsymbol{\Sigma}^T$ je {\bf Moorova-Penroseova pseudoinverze} (angl. Moore-Penrose pseudoinverse) matice $\boldsymbol{\Sigma}$.

\item {\bf Implementace v praxi:} Knihovna \texttt{scikit-learn} (\texttt{LinearRegression()}) používá pro výpočet $\hat{\mathbf{w}}$ numericky stabilní SVD rozklad prostřednictvím funkce \texttt{dgelsd} z knihovny LAPACK, což zajišťuje korektní řešení i pro singulární matice.

\end{itemize}
\subsection{Predikce lineární regrese}
\begin{itemize}


\begin{center}
\includegraphics[width=0.8\textwidth]{assets/lr.png}
\text{Obrázek 2: Vizualizace modelu lineární regrese}
\end{center}

\item Na obrázku 2: {\bf modré body} jsou trénovací data $(x_i, Y_i)$, {\bf červené body} jsou predikce $\hat{Y}_i$, {\bf modré křížky} jsou střední hodnoty $(x_i, E(Y_i))$, {\bf modrá čerchovaná čára} je skutečná přímka $y = \mathbf{w}^T \mathbf{x}$ (neznámá), a {\bf červená čára} je naše odhadnutá přímka $\hat{y} = \hat{\mathbf{w}}^T \mathbf{x}$. Nejlepší odhad by byl, kdyby se červená přímka dostala na modrou čerchovanou čáru.
\end{itemize}

\subsection{Závěrečné poznámky}

\begin{itemize}
\item Lineární regrese je ve vztahu ke problémům dimenzionality (narozdíl od kNN) poměrně {\bf rezistentní}. Důvodem je, že se jedná o parametrickou metodu -- v ideálním případě nám tedy pro $p$ příznaků $+1$ intercept může k určení přesného modelu stačit přesně $p + 1$ bodů trénovací množiny.

\item {\bf Problémy nastávají}, když jsou v důsledku malé trénovací množiny nebo špatných příznaků, které jsou např. silně korelované, sloupce matice $\mathbf{X}$ (skoro) lineárně závislé:

\begin{itemize}
\item {\bf Numerická nestabilita:} Přímý výpočet pomocí $(\mathbf{X}^T\mathbf{X})^{-1}$ je numericky nestabilní nebo matice není invertibilní. Tento problém řeší SVD rozklad, který poskytuje stabilní výpočet i pro singulární nebo špatně podmíněné matice.

\item {\bf Přeučení modelu:} I když SVD zvládne spočítat řešení, výsledný model může trpět přeučením (angl. overfitting), tj. příliš se přizpůsobí trénovací množině včetně náhodné chyby $\varepsilon$ a nebude schopen dobře predikovat nové body. Řešením je použití regularizace (např. hřebenová regrese).
\end{itemize}

\item {\bf Hyperparametry:} \texttt{LinearRegression()} v \texttt{scikit-learn} {\bf nemá žádné hyperparametry} k ladění. Jedinou volbou je, zda zahrnout intercept $w_0$ (parametr \texttt{fit\_intercept}, standardně \texttt{True}).

\end{itemize}

\subsection{Problém kolinearity}
\begin{itemize}
\item Kolinearita (angl. collinearity) nastává, když jsou příznaky (sloupce matice X) silně korelované nebo téměř lineárně závislé.

\vspace{0.3cm}


\begin{table}[h]
\centering
\small
\begin{tabular}{c|ccc}
\textbf{Problém} & \textbf{Přímá inverze} & \textbf{SVD} & \textbf{Ridge/Lasso} \\
\hline
Numerická nestabilita & Velký problém & Vyřešeno & Vyřešeno \\
Vysoký rozptyl modelu & Problém & Stále problém & Vyřešeno \\
\end{tabular}
\caption{Srovnání metod řešení lineární regrese při kolinearitě.}
\label{tab:colinearity_comparison}
\end{table}


Tabulka~\ref{tab:colinearity_comparison} ukazuje srovnání a problémy při kolinearitě. I když SVD rozklad zlepší numerickou stabilitu, stále zůstává problém s vysokým rozptylem — malá změna v trénovacích datech způsobí velkou změnu v predikcích modelu. V praxi se to řeší pomocí Ridge, Lasso či Elastic Net regrese.

\item Možné řešení kolinearity u lineární regrese:
\begin{enumerate}
    \item Přigenerovat další data či odebrat existující (nemusí pomoci).
    \item Snížit počet příznaků (feature selection, PCA) — chceme snížením počtu příznaků odebrat příznaky, které jsou (téměř) lineárně závislé.
    \item Použití Ridge, Lasso či Elastic Net regrese, které mají regularizační členy, jež problém kolinearity většinou odstraní nebo dostatečně zmírní.
\end{enumerate}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Příklad výpočtu metody nejmenších čtverců}

\begin{itemize}
    \item Uvažme, že máme data se 3 domy a chceme predikovat jejich cenu podle plochy ($m^2$). Potom vysvětlovaná proměnná $\mathbf{Y}$ je:
    $$\mathbf{Y} = (2.0, 3.5, 5.0)^T$$
    a vektor ploch je:
    $$\mathbf{x} = (50, 100, 170)^T$$

\item 3 domy reprezentované v grafu jsou vizualizované na \hyperref[fig:chart1.png]{obrázku 3}.
\end{itemize}

\begin{center}
\includegraphics[width=0.8\textwidth]{assets/chart1.png}
\end{center}
\begin{center}
\text{Obrázek 3: Každý bod $(x_i, Y_i)$ představuje jeden dům v datasetu.}
\label{fig:chart1.png}
\end{center}
\vspace{0.3cm}

\begin{itemize}
\item Chceme nyní najít přímku $\hat{Y} = w_0 + w_1 x$, která bude co nejblíže ke všem bodům. Možné příklady přímek včetně té nejlepší možné reprezentuje \hyperref[fig:chart2.png]{obrázek 4}. Naopak \hyperref[fig:chart5.png]{obrázek 5} reprezentuje residua, které se snažíme minimalizovat. Součet kvadrátů reziduí nazýváme residuální součet čtverců a značíme $RSS(\mathbf{w})$. Formálně tedy v našem ukázkovém příkladu platí: $RSS(\mathbf{w}) = e^2_1 + e^2_2 + e^2_3$.
\end{itemize}

\vspace{0.3cm}
\begin{center}
\includegraphics[width=0.8\textwidth]{assets/chart2.png}
\end{center}
\begin{center}
\text{Obrázek 4: Porovnání různých přímek proložených daty.}
\label{fig:chart2.png}
\end{center}
\vspace{0.3cm}

\vspace{0.3cm}
\begin{center}
\includegraphics[width=0.8\textwidth]{assets/chart5.png}
\end{center}
\begin{center}
\text{Obrázek 5: Vizualizace reziduí $e_i = Y_i - \hat{Y}_i$. Čtverce znázorňují hodnoty $e_i^{2}$, jejichž suma tvoří $RSS(\mathbf{w})$.}
\label{fig:chart5.png}
\end{center}
\vspace{0.3cm}

\begin{itemize}
\item Matematicky se snažíme najít minimum funkce $RSS(\mathbf{w})$. Funkci $RSS(\mathbf{w})$ jako 3D plochu vizualizuje \hyperref[fig:chart3.png]{obrázek 6} a její vrstevnice \hyperref[fig:chart4.png]{obrázek 7}.
\end{itemize}

\vspace{0.3cm}
\begin{center}
\includegraphics[width=0.8\textwidth]{assets/chart3.png}
\end{center}
\begin{center}
\text{Obrázek 6: Funkce $RSS(w_0, w_1)$ jako 3D plocha. Minimum této funkce odpovídá optimálním parametrům $\hat{w}_0, \hat{w}_1$.}
\label{fig:chart3.png}
\end{center}
\vspace{0.3cm}

\vspace{0.3cm}
\begin{center}
\includegraphics[width=0.8\textwidth]{assets/chart4.png}
\end{center}
\begin{center}
\text{Obrázek 7: Vrstevnice (contour plot) funkce $RSS(\mathbf{w})$. Pohled shora na 3D plochu. Červený bod označuje minimum.}
\label{fig:chart4.png}
\end{center}
\vspace{0.3cm}

\subsubsection{Krok 1: Sestavení matice $\mathbf{X}$ a vektoru $\mathbf{Y}$}

\begin{itemize}
\item Hledáme model ve tvaru $\hat{Y} = w_0 + w_1 x$, proto matici $\mathbf{X}$ sestavíme tak, aby první sloupec obsahoval samé jedničky (pro intercept $w_0$) a druhý sloupec obsahoval hodnoty příznaku (plochy):

$$\mathbf{X} = \begin{pmatrix}
1 & 50 \\
1 & 100 \\
1 & 170
\end{pmatrix}, \quad
\mathbf{Y} = \begin{pmatrix}
2.0 \\
3.5 \\
5.0
\end{pmatrix}, \quad
\mathbf{w} = \begin{pmatrix}
w_0 \\
w_1
\end{pmatrix}$$
\end{itemize}

\subsubsection{Krok 2: Výpočet $\mathbf{X}^T\mathbf{X}$}

\begin{itemize}
\item Transponovaná matice $\mathbf{X}^T$ má tvar:
$$\mathbf{X}^T = \begin{pmatrix}
1 & 1 & 1 \\
50 & 100 & 170
\end{pmatrix}$$

\item Součin $\mathbf{X}^T\mathbf{X}$:
\begin{align*}
\mathbf{X}^T\mathbf{X} &= \begin{pmatrix}
1 & 1 & 1 \\
50 & 100 & 170
\end{pmatrix}
\begin{pmatrix}
1 & 50 \\
1 & 100 \\
1 & 170
\end{pmatrix} \\[0.3cm]
&= \begin{pmatrix}
3 & 320 \\
320 & 2500 + 10000 + 28900
\end{pmatrix} = \begin{pmatrix}
3 & 320 \\
320 & 41400
\end{pmatrix}
\end{align*}
\end{itemize}

\subsubsection{Krok 3: Výpočet $\mathbf{X}^T\mathbf{Y}$}

\begin{itemize}
\item Součin $\mathbf{X}^T\mathbf{Y}$:
\begin{align*}
\mathbf{X}^T\mathbf{Y} &= \begin{pmatrix}
1 & 1 & 1 \\
50 & 100 & 170
\end{pmatrix}
\begin{pmatrix}
2.0 \\
3.5 \\
5.0
\end{pmatrix} \\[0.3cm]
&= \begin{pmatrix}
1 \cdot 2.0 + 1 \cdot 3.5 + 1 \cdot 5.0 \\
50 \cdot 2.0 + 100 \cdot 3.5 + 170 \cdot 5.0
\end{pmatrix} \\[0.3cm]
&= \begin{pmatrix}
10.5 \\
100 + 350 + 850
\end{pmatrix} = \begin{pmatrix}
10.5 \\
1300
\end{pmatrix}
\end{align*}
\end{itemize}

\subsubsection{Krok 4: Výpočet inverzní matice $(\mathbf{X}^T\mathbf{X})^{-1}$}

Pro výpočet inverze matice $\mathbf{X}^T\mathbf{X}$ použijeme Gauss-Jordanovu eliminaci na rozšířené matici $(\mathbf{X}^T\mathbf{X} \mid \mathbf{E})$:

$$
\renewcommand{\arraystretch}{1.4} % Zvětší mezery mezi řádky v matici pro lepší čitelnost zlomků
\begin{array}{c}
\left(\begin{array}{cc|cc}
3 & 320 & 1 & 0 \\
320 & 41400 & 0 & 1
\end{array}\right)
\sim
\left(\begin{array}{cc|cc}
1 & \frac{320}{3} & \frac{1}{3} & 0 \\
320 & 41400 & 0 & 1
\end{array}\right)
\sim
\left(\begin{array}{cc|cc}
1 & \frac{320}{3} & \frac{1}{3} & 0 \\
0 & \frac{21800}{3} & -\frac{320}{3} & 1
\end{array}\right)
\\[1cm] % Větší mezera mezi řádky postupu
\sim
\left(\begin{array}{cc|cc}
1 & \frac{320}{3} & \frac{1}{3} & 0 \\
0 & 1 & -\frac{320}{21800} & \frac{3}{21800}
\end{array}\right)
\sim
\left(\begin{array}{cc|cc}
1 & 0 & \frac{41400}{21800} & -\frac{320}{21800} \\
0 & 1 & -\frac{320}{21800} & \frac{3}{21800}
\end{array}\right)
\end{array}
$$

Výsledná inverzní matice:
$$(\mathbf{X}^T\mathbf{X})^{-1} = \frac{1}{21800} \begin{pmatrix}
41400 & -320 \\
-320 & 3
\end{pmatrix}$$

\subsubsection{Krok 5: Řešení normální rovnice $\hat{\mathbf{w}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}$}

Dosadíme do vzorce pro odhad vektoru vah:
\begin{align*}
\hat{\mathbf{w}} &= (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}
= \frac{1}{21800} \begin{pmatrix} 41400 & -320 \\ -320 & 3 \end{pmatrix} \begin{pmatrix} 10.5 \\ 1300 \end{pmatrix} \\[0.2cm]
&= \frac{1}{21800} \begin{pmatrix} 434700 - 416000 \\ -3360 + 3900 \end{pmatrix}
= \frac{1}{21800} \begin{pmatrix} 18700 \\ 540 \end{pmatrix} \\[0.2cm]
&\approx \begin{pmatrix} 0.858 \\ 0.0248 \end{pmatrix}
\end{align*}

Výsledný model lineární regrese:
$$\boxed{\hat{Y} = 0.858 + 0.0248 \cdot x}$$

\subsubsection{Krok 6: Výpočet predikcí a reziduí}

Výpočet predikcí $\hat{Y}_i$ a reziduí $e_i = Y_i - \hat{Y}_i$:
\begin{align*}
\hat{Y}_1 &= 0.858 + 0.0248(50) = 2.098 \quad \rightarrow \quad e_1 = 2.0 - 2.098 = -0.098 \\
\hat{Y}_2 &= 0.858 + 0.0248(100) = 3.338 \quad \rightarrow \quad e_2 = 3.5 - 3.338 = 0.162 \\
\hat{Y}_3 &= 0.858 + 0.0248(170) = 5.074 \quad \rightarrow \quad e_3 = 5.0 - 5.074 = -0.074
\end{align*}

Residuální součet čtverců ($RSS$):
$$RSS(\hat{\mathbf{w}}) = \sum_{i=1}^3 e_i^2 = (-0.098)^2 + (0.162)^2 + (-0.074)^2 \approx 0.041$$

\subsubsection{Shrnutí výsledků}

\begin{itemize}
    \item \textbf{Výsledný model:} $\hat{Y} = 0.858 + 0.0248 \cdot x$.
    \item \textbf{Interpretace:} Každý další $m^2$ plochy zvyšuje cenu domu v průměru o $24\,800$ Kč ($\hat{w}_1 \approx 0.0248$ mil. Kč). Intercept $\hat{w}_0$ odpovídá teoretické základní ceně.
\end{itemize}

\begin{table}[h]
\centering
\begin{tabular}{c|c|c|c|c}
\textbf{Dům} & $x_i$ $(m^2)$ & $Y_i$ (mil. Kč) & $\hat{Y}_i$ (predikce) & $e_i$ (chyba) \\ \hline
1 & 50 & 2.0 & 2.098 & $-0.098$ \\
2 & 100 & 3.5 & 3.338 & $+0.162$ \\
3 & 170 & 5.0 & 5.074 & $-0.074$ \\
\end{tabular}
\caption{Shrnutí výsledků OLS regrese.}
\label{tab:ols_compact_results}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Geometrická interpretace metody nejmenších čtverců}
\label{sec:geom_interp_met_nejm_ctvercu}

\begin{shaded}
\noindent\textbf{Otázka ke zkoušce 4 (Geometrická interpretace metody nejmenších čtverců):} 
Geometrická interpretace metody nejmenších čtverců, normální rovnice, řešení. Regularita versus lineární nezávislost sloupců matice X.
\end{shaded}


\begin{itemize}
\item Minimalizace $RSS(\mathbf{w}) = ||\mathbf{Y} - \mathbf{X}\mathbf{w}||^2$
je ekvivalentní minimalizaci $||\mathbf{Y} -\mathbf{X}\mathbf{w}||$.

\item To znamená, že pro optimální $\mathbf{w}$ je Euklidovská vzdálenost bodů $\mathbf{Y}$ a $\mathbf{Xw}$ v prostoru $\R^N$ nejmenší možná.

\item Označíme-li $i$-tý sloupec matice $\mathbf{X}$ jako $\mathbf{X}_{\bullet i}$, můžeme si všimnout, že:
$$ \mathbf{X}\mathbf{w} = w_0 \mathbf{X}_{\bullet 0} + w_1 \mathbf{X}_{\bullet 1} + \dots + w_p \mathbf{X}_{\bullet p} $$

\item Vektor $\mathbf{Xw}$ je lineární kombinací sloupců matice $\mathbf{X}$ s koeficienty $w_0, \dots, w_p$. Leží tedy v lineárním podprostoru prostoru $\R^N$, který je lineárním obalem $p+1$ sloupců $\mathbf{X}_{\bullet 0}, \dots, \mathbf{X}_{\bullet p}$. Pro různé hodnoty $\mathbf{w}$ pak vektor $\mathbf{Xw}$ celý tento podprostor pokrývá, tj.:
$$ \langle \mathbf{X}_{\bullet 0}, \dots, \mathbf{X}_{\bullet p} \rangle = \{ \mathbf{X}\mathbf{w} \mid \mathbf{w} \in \R^{p+1} \} $$


\item Chceme-li minimalizovat vzdálenost $\mathbf{Y}$ a $\mathbf{Xw}$, hledáme bod $\mathbf{Xw}$ v
podprostoru sloupců matice $\mathbf{X}$, který je k $\mathbf{Y}$ nejblíže. Bod $\mathbf{Xw}$ je k bodu $\mathbf{Y}$ nejblíže, jestliže je vektor $\mathbf{Y - Xw}$ na ten podprostor
kolmý, což reprezentuje modrý vektor na obrázku 8.

\vspace{0.3cm}
\begin{center}
\includegraphics[width=0.8\textwidth]{assets/gOLS.png}
\end{center}
\begin{center}
\text{Obrázek 8: Geometrická interpretace metody nejmenších čtverců}
\label{fig:gOLS.png}
\end{center}
\vspace{0.3cm}

\item To znamená, že vektor rozdílu musí být kolmý na všechny vektory $\mathbf{X}_{\bullet 0}, \dots, \mathbf{X}_{\bullet p}$, které podprostor generují:
$$ (\mathbf{X}_{\bullet i})^T (\mathbf{Y} - \mathbf{X}\mathbf{w}) = 0 \quad \text{pro } \forall i = 0, \dots, p $$

\item To lze maticově zapsat jako:
$$ \mathbf{X}^T (\mathbf{Y} - \mathbf{X}\mathbf{w}) = \mathbf{0} \quad \iff \quad \mathbf{X}^T\mathbf{Y} - \mathbf{X}^T\mathbf{X}\mathbf{w} = \mathbf{0} $$

Získali jsme tím starou známou \textbf{normální rovnici} a tedy stejné řešení. Z výše uvedených geometrických úvah navíc plyne, že pro jakékoliv řešení $\mathbf{w}$ normální rovnice je $||\mathbf{Y} - \mathbf{X}\mathbf{w}||$, a tedy i $RSS(\mathbf{w})$, nejmenší možné. Jakékoliv řešení normální rovnice tedy dává globální minimum.
\end{itemize}

\subsection{Zajímavá videa na YouTube}
\begin{itemize}
\item \href{https://youtu.be/7ArmBVF2dCs?si=4FaXsZSurjtSeNaC}{
Linear Regression, Clearly Explained!!! (StatQuest with Josh Starmer)} \hfill \textbf{[27~min]}
\item \href{https://youtu.be/73xKLFsXvvM?si=SJCneap-HV8NaMyP}{
Orthogonal Projection Formulas (Least Squares) - Projection, Part 2 (Sam Levey)} \hfill \textbf{[26~min]}
\end{itemize}

\section{Hřebenová regrese (Ridge Regression)}
\label{sec:ridge_regression}
\begin{shaded}

\noindent\textbf{Otázka ke zkoušce 5 (Hřebenová regrese):} 
Regularizovaný reziduální součet čtverců, řešení. Modely bázových funkcí.

\end{shaded}


\subsection{Hřebenová regrese: konstrukce}

\begin{itemize}

\item {\bf Hřebenová regrese} (angl. ridge regression) [Hoerl, Kennard (1970)] nebo taky $L_2$ regularizace se k problému kolinearity staví zavedením penalizačního členu úměrného kvadrátu normy vektoru koeficientů $\mathbf{w}$ s vynecháním interceptu.

\item Minimalizuje se tedy {\bf regularizovaný reziduální součet čtverců}
$$RSS_\lambda(\mathbf{w}) = \|\mathbf{Y} - \mathbf{X}\mathbf{w}\|^2 + \lambda \sum_{i=1}^{p} w_i^2$$
který závisí na parametru $\lambda \geq 0$.

\item Pro $\lambda = 0$ se dostává $RSS_0(\mathbf{w}) = RSS(\mathbf{w})$ a dostáváme tedy obyčejnou metodu nejmenších čtverců.

\item Pro $\lambda > 0$ je vidět, že v minimu se bude cílit na takové vektory $\mathbf{w}$, které mají co nejmenší složky.

\item Hodnota $w_0$ interceptu se nijak nepenalizuje. Jedná se pouze o vertikální posun, který zajišťuje předpoklad $\mathbb{E}\varepsilon = 0$ modelu a je tedy vhodné ho neomezovat.

\item Stále platí, že model pro $Y$ v bodě $\mathbf{x}$ je $Y = w_0 + w_1 x_1 + \dots + w_p x_p + \varepsilon$.

\item Zavedeme-li matici
$$\mathbf{I}' = \begin{pmatrix}
0 & 0 & \cdots & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1
\end{pmatrix} \in \mathbb{R}^{p+1,p+1},$$
lze psát
$$RSS_\lambda(\mathbf{w}) = 
\sum_{i = 1}^N  (Y_i - \mathbf{w}^T \mathbf{x}_i)^2 + \lambda \sum_{i=1}^{p} w_i^2 = \|\mathbf{Y} - \mathbf{X}\mathbf{w}\|^2 + \lambda \sum_{i=1}^{p} w_i^2 = \|\mathbf{Y} - \mathbf{X}\mathbf{w}\|^2 + \lambda \mathbf{w}^T \mathbf{I}' \mathbf{w}.$$

\item Hledá se minimum, proto sestrojí se {\bf gradient} $RSS_\lambda(\mathbf{w})$:
\begin{align*}
\nabla RSS_\lambda(\mathbf{w}) = \frac{\partial RSS_\lambda(\mathbf{w})}{\partial \mathbf{w}} &= \frac{\partial}{\partial \mathbf{w}} \left( \|\mathbf{Y} - \mathbf{X}\mathbf{w}\|^2 + \lambda \mathbf{w}^T \mathbf{I}' \mathbf{w} \right) \\
&= \frac{\partial}{\partial \mathbf{w}} \left( \mathbf{Y}^T\mathbf{Y} - 2\mathbf{Y}^T\mathbf{X}\mathbf{w} + \mathbf{w}^T\mathbf{X}^T\mathbf{X}\mathbf{w} + \lambda \mathbf{w}^T \mathbf{I}' \mathbf{w} \right) \\
&= -2\mathbf{X}^T\mathbf{Y} + 2\mathbf{X}^T\mathbf{X}\mathbf{w} + 2\lambda \mathbf{I}' \mathbf{w} \\
&= -2\mathbf{X}^T(\mathbf{Y} - \mathbf{X}\mathbf{w}) + 2\lambda \mathbf{I}' \mathbf{w}
\end{align*}

\item Ekvivalent normální rovnice je tedy
$$\mathbf{X}^T\mathbf{Y} - \mathbf{X}^T\mathbf{X}\mathbf{w} - \lambda \mathbf{I}' \mathbf{w} = \mathbf{0}.$$

\item {\bf Hessova matice} je dále
\begin{align*}
\nabla^2 RSS_\lambda(\mathbf{w}) = \frac{\partial}{\partial \mathbf{w}} \left( \nabla RSS_\lambda(\mathbf{w}) \right) 
&= \frac{\partial}{\partial \mathbf{w}} \left( -2\mathbf{X}^T\mathbf{Y} + 2\mathbf{X}^T\mathbf{X}\mathbf{w} + 2\lambda \mathbf{I}' \mathbf{w} \right) \\
&= 2\mathbf{X}^T\mathbf{X} + 2\lambda \mathbf{I}' \\
&= 2(\mathbf{X}^T\mathbf{X} + \lambda \mathbf{I}').
\end{align*}

\item $\forall \mathbf{v} \in \mathbb{R}^{p+1}$, $\mathbf{v} \neq \mathbf{0}$ a $\lambda > 0$ platí
$$\mathbf{v}^T(\mathbf{X}^T\mathbf{X} + \lambda \mathbf{I}')\mathbf{v} = (\mathbf{X}\mathbf{v})^T(\mathbf{X}\mathbf{v}) + \lambda \mathbf{v}^T \mathbf{I}' \mathbf{v} = \|\mathbf{X}\mathbf{v}\|^2 + \lambda \sum_{i=1}^{p} v_i^2 > 0,$$
protože pro $\mathbf{v} = (v_0, 0, \dots, 0)^T \neq \mathbf{0}$ platí $\mathbf{X}\mathbf{v} = (v_0, \dots, v_0)^T \neq \mathbf{0}$.

\item Hessova matice je tedy pozitivně definitní a matice $\mathbf{X}^T\mathbf{X} + \lambda \mathbf{I}'$ je vždy regulární pro $\lambda > 0$.

\item Pro $\lambda > 0$ tak vždy existuje jednoznačné řešení normální rovnice
$$\hat{\mathbf{w}}_\lambda = (\mathbf{X}^T\mathbf{X} + \lambda \mathbf{I}')^{-1}\mathbf{X}^T\mathbf{Y}$$
a odpovídá globálnímu minimu $RSS_\lambda$.

\item Predikce v bodě $\mathbf{x}$ je potom opět $\hat{Y} = \mathbf{x}^T \hat{\mathbf{w}}_\lambda$.
\end{itemize}

\subsection{Modely bázových funkcí}
\label{sec:bazove_funkce_model}

\begin{itemize}

\item Doposud se uvažoval pouze jednoduchý lineární model $Y = \mathbf{x}^T \mathbf{w} + \varepsilon$.

\item Jak rozšířit možnosti za obzor linearity?

\item Základní rozšíření spočívá v nahrazení původních příznaků jejich transformovanými variantami.

\item Pro $M \in \mathbb{N}$ vezměme $M$ funkcí $\varphi_1, \dots, \varphi_M$ z $\mathbb{R}^p$ do $\mathbb{R}$, které reprezentují transformace příznaků $X$, a nazvěme je {\bf bázové funkce} (angl. basis functions).

\item K těmto funkcím se přidá $\varphi_0(\mathbf{x}) = 1$ (intercept) a poskládají se do vektorové funkce $\varphi : \mathbb{R}^p \to \mathbb{R}^{M+1}$ vztahem $\varphi(\mathbf{x}) = (1, \varphi_1(\mathbf{x}), \dots, \varphi_M(\mathbf{x}))^T$.

\item Jako model vztahu $Y$ a $\mathbf{x}$ budeme uvažovat lineární model:
$$Y = \sum_{j=0}^{M} w_j \varphi_j(\mathbf{x}) + \varepsilon = \varphi(\mathbf{x})^T \mathbf{w} + \varepsilon.$$

\item Další postup je analogický jako předtím.

\item Predikce: $\hat{Y} = \varphi(\mathbf{x})^T \hat{\mathbf{w}}_\lambda$.

\item {\bf Příklady bázových funkcí:}
\begin{itemize}
\item $\varphi(\mathbf{x}) = x_i$ — přímo jednotlivé příznaky.
\item $\varphi(\mathbf{x}) = x_i^2$, $\varphi(\mathbf{x}) = x_k x_\ell$ — mocniny příznaků a jejich různé součiny, polynomiální regrese.
\item $\varphi(\mathbf{x}) = \log(x_i)$, $\sqrt{x_i}$, $\sin(x_i)$ — nelineární transformace jednotlivých příznaků.
\item $\varphi(\mathbf{x}) = \mathbf{1}_{(a,b)}(x_i)$, kde $\mathbf{1}_A(\mathbf{x}) = 1$ pokud $\mathbf{x} \in A$ a $0$ jinak — indikátory množin.
\item $\varphi(\mathbf{x}) = h(\|\mathbf{x} - \mathbf{x}_i\|)$, kde $\mathbf{x}_i$ je $i$-tý trénovací bod a $h$ je nějaká funkce — tzv. radiální bázové funkce centrované v bodech trénovací množiny.
\end{itemize}

\end{itemize}

\section{Statistické vlastnosti modelů}
\label{sec:stat_vlastnosti}

\begin{shaded}

\noindent\textbf{Otázka ke zkoušce 6 (Statistické vlastnosti modelů):} 
Rozklad očekávané chyby modelu, bias-variance tradeoff. Nestrannost odhadu v lineární regresi metodou nejmenších čtverců.

\end{shaded}

\subsection{Rozklad očekávané chyby modelu}

\begin{itemize}
\item Budeme předpokládat nezávislost trénovacích a testovacích dat, tj. nezávislost $\mathbf{Y}$
a $Y$ , a v důsledku tedy nezávislost $\hat{Y}$ a $Y$.
\item Pro očekávanou chybu tedy platí
\begin{align*}
\mathbb{E}L(Y, \hat{Y}) &= \mathbb{E}(Y - \hat{Y})^2 = \mathbb{E}(Y - \mathbb{E}Y + \mathbb{E}Y - \hat{Y})^2 \\
&= \mathbb{E}(Y - \mathbb{E}Y)^2 + 2\mathbb{E}\left[(Y - \mathbb{E}Y)(\mathbb{E}Y - \hat{Y})\right] + \mathbb{E}(\hat{Y} - \mathbb{E}Y)^2 \\
&= \mathbb{E}(Y - \mathbb{E}Y)^2 + \mathbb{E}(\hat{Y} - \mathbb{E}Y)^2.
\end{align*}
Označíme-li $\text{var}\,Y = \text{var}\,\varepsilon = \sigma^2$, dostáváme
\[
\mathbb{E}L(Y, \hat{Y}) = \sigma^2 + \mathbb{E}(\hat{Y} - \mathbb{E}Y)^2.
\]
\item První člen odpovídá \textbf{neodstranitelné chybě}, která je dána náhodností v modelu. Tato chyba se nazývá Bayesovská (angl. \textbf{Bayes error}). Druhý člen se značí $\text{MSE}(\hat{Y})$ a nazývá \textbf{střední kvadratická chyba odhadu} $\hat{Y}$ parametru $\mathbb{E}Y$ (angl. \textbf{mean squared error}).


\item Pro $MSE(\hat{Y})$ dále platí

\begin{align*}
{MSE}(\hat{Y}) &= \mathbb{E}(\hat{Y} - \mathbb{E}Y)^2 = \mathbb{E}(\mathbb{E}\hat{Y} - \mathbb{E}Y + \hat{Y} - \mathbb{E}\hat{Y})^2 \\
&= \mathbb{E}(\mathbb{E}\hat{Y} - \mathbb{E}Y)^2 + \mathbb{E}(\hat{Y} - \mathbb{E}\hat{Y})^2 + 2\mathbb{E}(\hat{Y} - \mathbb{E}\hat{Y})(\mathbb{E}\hat{Y} - \mathbb{E}Y) \\
&= (\mathbb{E}\hat{Y} - \mathbb{E}Y)^2 + \mathbb{E}(\hat{Y} - \mathbb{E}\hat{Y})^2 + 2 \cdot 0 \cdot (\mathbb{E}\hat{Y} - \mathbb{E}Y) \\
&= (\mathbb{E}\hat{Y} - \mathbb{E}Y)^2 + \text{var}\,\hat{Y} = (\text{bias}\,\hat{Y})^2 + \text{var}\,\hat{Y}
\end{align*}

kde bias $\hat{Y} = \mathbb{E}Yˆ − \mathbb{E}Y$ značí \textbf{vychýlení odhadu} (angl. \textbf{bias}).

\item Dohromady tedy máme finální dekompozici očekávané chyby jako
\[
\mathbb{E}L(Y,\hat{Y}) = \sigma^2 + (\text{bias}\,\hat{Y})^2 + \text{var}\,\hat{Y}.
\]

Očekávaná chyba modelu je tedy součtem \textbf{neodstranitelné chyby}, \textbf{kvadrátu vychýlení odhadu a rozptylu odhadu}.





\end{itemize}

\subsection{Bias-variance tradeoff (ilustrace na hřebenové regresi)}


U hřebenové regrese lze ukázat, že (hodně zjednodušeně) platí
\[
(\text{bias}\,\hat{Y})^2 \sim \left(1 - \frac{1}{1+\lambda}\right)^2 
\quad \text{a} \quad 
\text{var}\,\hat{Y} \sim \left(\frac{1}{1+\lambda}\right)^2.
\]

To znamená, že s rostoucím $\lambda$ vychýlení roste a rozptyl klesá. Takového chování v závislosti na hyperparametrech modelu je typické a nazývá se \textbf{bias-variance tradeoff}.

\begin{center}
\includegraphics[width=0.8\textwidth]{assets/bias-variance-tradeoff.png}
\text{Obrázek 9: Závislost $\text{MSE}(\hat{Y})$, $(\text{bias}\,\hat{Y})^2$ a $\text{var}\,\hat{Y}$ na parametru regularizace $\lambda$.}
\end{center}
\vspace{0.05cm}


\subsection{Nestrannost odhadu v lineární regresi metodou nejmenších čtverců}
\begin{itemize}
\item Odhad vektoru parametrů $\hat{\mathbf{w}}_\lambda = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}$ je náhodný vektor.
\item Je to tedy \textbf{bodový odhad} vektoru parametrů $\mathbf{w}$ a je příkladem tzv. statistiky.
\item \textbf{Věta:} Odhad $\hat{\mathbf{w}}_{OLS}$ získaný metodou nejmenších čtverců je za předpokladu $\mathbb{E}\boldsymbol{\varepsilon} = \mathbf{0}$ nestranný, tj. $\mathbb{E}\hat{\mathbf{w}}_{OLS} = \mathbf{w}$.
\item \textbf{Důkaz:} Z linearity střední hodnoty plyne:
\begin{align*}
\mathbb{E}\mathbf{Y} &= \mathbb{E}(\mathbf{X}\mathbf{w} + \boldsymbol{\varepsilon}) = \mathbf{X}\mathbf{w} + \mathbb{E}\boldsymbol{\varepsilon} = \mathbf{X}\mathbf{w}.
\end{align*}
Dále platí:
\begin{align*}
\mathbb{E}\hat{\mathbf{w}}_{OLS} &= \mathbb{E}\left[(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}\right] = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbb{E}\mathbf{Y} \\
&= (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{X}\mathbf{w} = \mathbf{w}.
\end{align*}
\item Z nestrannosti odhadu vektoru parametrů $\hat{\mathbf{w}}_{OLS}$ dále plyne nestrannost odhadu $\hat{Y}$ v bodě $\mathbf{x}$.
\item K tomu je třeba si uvědomit, že pomocí $\hat{Y} = \mathbf{x}^T\hat{\mathbf{w}}_{OLS}$ se snažíme predikovat skutečnou hodnotu $Y$, ale ze statistického pohledu se jedná o \textbf{bodový odhad střední hodnoty} $\mathbb{E}Y = \mathbf{x}^T\mathbf{w} + \mathbb{E}\varepsilon = \mathbf{x}^T\mathbf{w}$.
\item Z předchozí věty plyne:
\[
\mathbb{E}\hat{Y} = \mathbb{E}\mathbf{x}^T\hat{\mathbf{w}}_{OLS} = \mathbf{x}^T\mathbb{E}\hat{\mathbf{w}}_{OLS} = \mathbf{x}^T\mathbf{w} = \mathbb{E}Y
\]
a $\hat{Y}$ je tedy nestranným odhadem $\mathbb{E}Y$.
\item To samozřejmě znamená, že
\[
\text{bias}\,\hat{Y} = \mathbb{E}\hat{Y} - \mathbb{E}Y = 0,
\]
tj. vychýlení je 0.
\end{itemize}

\section{Logistická regrese}
\label{sec:logisticka_regrese}

\begin{shaded}
\noindent\textbf{Otázka ke zkoušce 7 (Logistická regrese):} 
Použití pro binární klasifikaci, jak funguje model, vlastnosti.
\end{shaded}

\subsection{Použití pro binární klasifikaci}
\begin{itemize}
\item Metoda určená pro \textbf{klasifikaci} (vysvětlovaná proměnná $Y \in \{0,1\}$).
\item Základní myšlenka: namísto hodnoty $Y \in \{0,1\}$ se snažíme predikovat pravděpodobnost $P(Y = 1)$ z intervalu $[0,1]$.
\item Hledáme funkční předpis, který pro dané hodnoty $x_i$ příznaků $X_i$ a dané hodnoty koeficientů $w_i$ vrátí číslo z intervalu $[0,1]$, které bude odhadem pravděpodobnosti rýmy $(Y = 1)$.
\end{itemize}

\subsection{Logistická funkce (Sigmoida)}
\begin{itemize}
\item $f(x) = \frac{e^x}{1+e^x} = \frac{1}{1+e^{-x}}$
\item $D_f = \mathbb{R}$, $H_f = (0,1)$
\item Ostře rostoucí funkce
\end{itemize}

\begin{center}
\includegraphics[width=0.8\textwidth]{assets/graf_sigmoidy.png}
\text{Obrázek 10: Graf sigmoidy}
\end{center}

\subsection{Způsob výpočtu}
\begin{itemize}
\item $\mathbf{x} = (x_0, x_1, \ldots, x_p)$ - vektor příznaků
\item $\mathbf{w} = (w_0, w_1, \ldots, w_p)$ - vektor koeficientů
\item $\mathbf{w}^T\mathbf{x}$ spočítáme a dosadíme do sigmoidy: $P(Y = 1 \mid \mathbf{x}, \mathbf{w}) = \frac{e^{\mathbf{w}^T\mathbf{x}}}{1 + e^{\mathbf{w}^T\mathbf{x}}}$
\item Predikce: $\hat{Y} = 1$ pokud $P(Y=1) > \frac{1}{2}$, jinak $\hat{Y} = 0$
\end{itemize}

\subsection{Logistická regrese: vlastnosti}
\begin{itemize}
\item Nevyžaduje lineární vztah $Y$ na příznacích
\item Náchylná k overfittingu (zejména pokud je příliš složitá a trénovací data jsou omezená)
\item Hodnoty $\mathbf{w}$ odhadujeme pomocí metody MLE (viz další kapitola)
\end{itemize}

\subsection{Maximálně věrohodný odhad (Maximum Likelihood Estimation)}


\begin{shaded}
\noindent\textbf{Otázka ke zkoušce 8 (Logistická regrese):} 
Logistická regrese jako MLE odhad: sestavení optimalizační úlohy pro trénování, myšlenka MLE odhadů.
\end{shaded}


\begin{itemize}
\item Základní myšlenka MLE je, že máme trénovací data a chceme najít parametry modelu. MLE říká: "Vyber takové parametry, aby byla trénovací data, která vidíš, co \textbf{nejpravděpodobnější}."

\item Odhad metodou MLE pak odpovídá hodnotě $\mathbf{w}$, pro kterou jsou trénovací data nejpravděpodobnější možná.

\item {\bf Pravděpodobnosti pro logistickou regresi:}

Označme pro úsporu místa:
$$p_1(\mathbf{x}, \mathbf{w}) = P(Y = 1 \mid \mathbf{x}, \mathbf{w}) = \frac{e^{\mathbf{w}^T\mathbf{x}}}{1 + e^{\mathbf{w}^T\mathbf{x}}}$$

$$p_0(\mathbf{x}, \mathbf{w}) = P(Y = 0 \mid \mathbf{x}, \mathbf{w}) = 1 - \frac{e^{\mathbf{w}^T\mathbf{x}}}{1 + e^{\mathbf{w}^T\mathbf{x}}} = \frac{1}{1 + e^{\mathbf{w}^T\mathbf{x}}}$$

\item {\bf Pravděpodobnost datového bodu:}

Máme-li $i$-tý datový bod s hodnotou vysvětlované proměnné $Y_i$ a s hodnotami příznaků $\mathbf{x}_i = (x_{i;0}, x_{i;1}, \dots, x_{i;p})$, lze pro zadané hodnoty parametrů $\mathbf{w}$ označit pravděpodobnost tohoto datového bodu jako $p_{Y_i}(\mathbf{x}_i, \mathbf{w})$.

\item {\bf Notace pro trénovací data:}

Předpokládejme, že máme $N$ bodů v trénovacích datech. Zapíšeme je do vektoru
$$\mathbf{Y} = (Y_1, Y_2, \dots, Y_N)^T \in \mathbb{R}^N$$
a do matice
$$\mathbf{X} = \begin{pmatrix}
\mathbf{x}_1^T \\
\vdots \\
\mathbf{x}_N^T
\end{pmatrix} = \begin{pmatrix}
1 & x_{1;1} & x_{1;2} & \cdots & x_{1;p} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{N;1} & x_{N;2} & \cdots & x_{N;p}
\end{pmatrix}$$
kde $x_{i;0} = 1$ je intercept.

\item {\bf Věrohodnostní funkce} (angl. likelihood function):

Vyjádříme pravděpodobnost konkrétních trénovacích dat při parametrech $\mathbf{w}$. Předpokládáme (většinou oprávněně), že jednotlivé datové body jsou navzájem nezávislé, pravděpodobnost lze tedy napsat jako součin pravděpodobností jednotlivých bodů:
$$L(\mathbf{w}) = \prod_{i=1}^{N} p_{Y_i}(\mathbf{x}_i, \mathbf{w})$$
což je reálná funkce $p+1$ proměnných $\mathbb{R}^{p+1} \to \mathbb{R}$ vyjadřující pravděpodobnost trénovacích dat pro danou hodnotu parametrů $\mathbf{w}$.

\item {\bf Logaritmus věrohodnostní funkce:}

Místo součinu pravděpodobností derivujeme jejich logaritmus (logaritmus je ostře rostoucí funkce, takže extrémy jsou ve stejných bodech):
\begin{align*}
\ell(\mathbf{w}) = \ln L(\mathbf{w}) &= \sum_{i=1}^{N} \ln p_{Y_i}(\mathbf{x}_i, \mathbf{w}) \\
&= \sum_{i=1}^{N} \left(Y_i \ln p_1(\mathbf{x}_i, \mathbf{w}) + (1-Y_i) \ln p_0(\mathbf{x}_i, \mathbf{w})\right) \\
&= \sum_{i=1}^{N} \left(Y_i \ln\left(\frac{e^{\mathbf{w}^T\mathbf{x}_i}}{1+e^{\mathbf{w}^T\mathbf{x}_i}}\right) + (1-Y_i) \ln\left(\frac{1}{1+e^{\mathbf{w}^T\mathbf{x}_i}}\right)\right) \\
&= \sum_{i=1}^{N} \left(Y_i\mathbf{w}^T\mathbf{x}_i - \ln(1 + e^{\mathbf{w}^T\mathbf{x}_i})\right)
\end{align*}

\item {\bf Gradient logaritmu věrohodnostní funkce:}

Stejně jako v případě lineární regrese se najde gradient, tj. vektor složený z parciálních derivací podle všech proměnných $w_0, w_1, \dots, w_p$:
$$\frac{\partial \ell}{\partial w_j}(\mathbf{w}) = \sum_{i=1}^{N} x_{i;j}\left(Y_i - p_1(\mathbf{x}_i, \mathbf{w})\right), \quad j = 0, 1, \dots, p$$

Pomocí maticového násobení lze pak gradient napsat ve tvaru:
$$\nabla \ell(\mathbf{w}) = \mathbf{X}^T(\mathbf{Y} - \mathbf{P})$$
kde $\mathbf{P} = (p_1(\mathbf{x}_1, \mathbf{w}), p_1(\mathbf{x}_2, \mathbf{w}), \dots, p_1(\mathbf{x}_N, \mathbf{w}))^T$.

\item {\bf Hledání maxima:}

Teorie říká, že maximum bychom měli nalézt mezi řešeními rovnice "gradient se rovná nule", tedy
$$\nabla \ell(\mathbf{w}) = \mathbf{X}^T(\mathbf{Y} - \mathbf{P}) = \mathbf{0}$$

\item {\bf Problém:} 

Na rozdíl od lineární regrese neumíme najít explicitní řešení, tedy neexistuje vzorec, do kterého bychom dosadili trénovací data a vypadly by nám z něho hodnoty koeficientů $\mathbf{w}$.

\item {\bf Numerické metody:}

Je nutné použít numerické aproximativní metody. Používají se buď vícerozměrná verze Newtonovy metody, nebo gradientní vzestup: v obou případech se konstruuje posloupnost $\mathbf{w}^{(0)}, \mathbf{w}^{(1)}, \mathbf{w}^{(2)}, \dots$ o které lze předpokládat, že konverguje k nějakému lokálnímu maximu. Pro logistickou regresi lze ukázat, že pokud existuje lokální maximum, je jediné a je to hledané globální maximum.


\end{itemize}

\subsection{Závěrečné poznámky}
\begin{itemize}
\item Celá metoda stojí na předpokladu, že se chování dat dá zachytit ve tvaru daném sigmoidou.

\textbf{Příklad 1} kruhová hranice:
\begin{itemize}
\item Třída 1: body \textbf{uvnitř} kruhu.
\item Třída 0: body \textbf{mimo} kruh.
\end{itemize}
Logistická regrese selže! hranice je nelineární.

\textbf{Příklad 2} XOR problém:
\begin{itemize}
\item  (0,0) → třída 0
\item (0,1) → třída 1
\item (1,0) → třída 1
\item (1,1) → třída 0
\end{itemize}


\begin{center}
\includegraphics[width=0.55\textwidth]{assets/vizualizace_XOR_problem.png}
\label{fig:xor_problem}
\end{center}
\begin{center}
\text{Obrázek 11: Vizualizace bodů (Problém 2)}
\end{center}

Logistická regrese opět selže! Protože nelze najít lineární hranici, která by třídy oddělila. Na toto potřebujeme nelineární model (např. neural network)

Problém 1 (XOR) nebo Problém 2 (kruh) \textbf{lze vyřešit i logistickou regresí, pokud provedeme transformaci příznaků}. Např. přidáním příznaku $x_1^2 + x_2^2$ (pro kruh) nebo $x_1 \cdot x_2$ (pro XOR)
\end{itemize}

\section{Ensemble metody (Ensemble Methods)}
\label{sec:ensemble}

\begin{shaded}
\noindent\textbf{Otázka ke zkoušce 9 (Ensemble metody: bagging, boosting a náhodné lesy):} 
Rozdíl mezi baggingem a boostingem. Náhodné lesy a jejich hyperparametry.
\end{shaded}

\subsection{Ensemble metody: základní myšlenka}

\begin{itemize}

\item Základní myšlenka spočívá v tom, že namísto jednoho modelu (např. rozhodovacího stromu) použijeme {\bf více modelů} a jejich predikce nějakým způsobem zkombinujeme do finálního rozhodnutí.

\item My si ukážeme dva nejobvyklejší způsoby: {\bf bagging} (bootstrap aggregating) a {\bf boosting}.

\item Každý z těchto dvou přístupů si ilustrujeme na nejznámějších reprezentantech, v obou případech spočívajících ve skládání rozhodovacích stromů, které už známe.

\item Tito dva reprezentanti jsou: {\bf náhodný les} (angl. Random Forest) pro bagging a {\bf AdaBoost} (Adaptive Boosting) pro boosting.

\end{itemize}

\subsection{Bagging vs. Boosting}

\begin{itemize}

\item {\bf Při baggingu} vybíráme do bagů (množin $\mathcal{D}_i$) data náhodně.

\item {\bf Při boostingu} tuto metodu upravíme tak, že data, co byla dříve špatně klasifikována mají větší pravděpodobnost být vybrána.

\item {\bf Shrnutí rozdílů:}

\begin{table}[h]
\centering
\begin{tabular}{l|l|l}
\textbf{Vlastnost} & \textbf{Bagging} & \textbf{Boosting} \\
\hline
Konstrukce modelů & Paralelní (nezávislé) & Sekvenční (závislé) \\
Datové sady & Bootstrapem, rovnoměrně & S váhami, preferují špatně klasifikovaná data \\
Redukuje & Variance (rozptyl) & Bias (vychýlení) \\
Podmodely & Plně vyvinuté (deep) & Slabé modely (weak learners, shallow) \\
\end{tabular}
\caption{Srovnání metod bagging a boosting.}
\label{tab:bagging_vs_boosting}
\end{table}

\end{itemize}

\subsection{Bagging: náhodné lesy}

\begin{itemize}

\item {\bf Náhodný les} (angl. Random Forest) pro klasifikaci je ensemble metoda založená na {\bf baggingu} (bootstrap aggregating).

\item Náhodný les pro klasifikaci je základní myšlenka.

\item Pro jednoduchost předpokládejme binární klasifikační problém ($Y$ je 0 nebo 1).

\item Ze vstupního trénovacího datasetu $\mathcal{D}$ vytvoříme $n$ datasetů $\mathcal{D}_1, \dots, \mathcal{D}_n$ obvykle stejně velkých pomocí metody {\bf bootstrap} neboli pomocí výběru s opakováním.

\item Na každém datasetu $\mathcal{D}_i$ naučíme rozhodovací strom tak, jak jsem si předvedli v minulé přednášce. Stromy se obvykle nechávají narůst hluboké (často bez omezení hloubky), aby měly nízké vychýlení (bias). Bagging pak redukuje jejich vysokou varianci průměrováním.
 Stromy označme $T_1, \dots, T_n$.

\item Každý datový bod (řádek z tabulky s daty z $\mathcal{D}$) proženeme všemi stromy $T_1, \dots, T_n$ a od každého z nich si uložíme rozhodnutí $Y_1, \dots, Y_n$.

\item Všechny tyto stromy tvoří {\bf náhodný les} a jeho finální rozhodnutí o hodnotě $Y$ je dané {\bf většinovým rozhodnutím stromů} — při klasifikaci nejčastější hodnota.

\end{itemize}

\subsection{Náhodný les: vizualizace}

\begin{center}
\includegraphics[width=0.8\textwidth]{assets/ensemble_methods.png}
\label{fig:ensemble_methods}
\end{center}
\begin{center}
\text{Obrázek 12: Náhodný les pro klasifikaci — základní myšlenka}
\end{center}
\hspace{0.3em}

Na \hyperref[fig:ensemble_methods]{obrázku 12} vidíme vstupní data (nahoře), z nichž bootstrapem vytvoříme několik datasetů (střední řada). Na každém z nich natrénujeme rozhodovací strom. Finální predikce náhodného lesa je dána většinovým hlasováním (majority vote) všech stromů.

\subsection{Bootstrap}

\begin{itemize}

\item Ukážeme si, jak funguje bootstrap na jednoduchém příkladu:

\begin{table}[h]
\centering
\begin{tabular}{c|cccc}
\textbf{id} & \textbf{rýmička} & \textbf{pohlaví} & $> \mathbf{39}^\circ$\textbf{C} & \textbf{vstal(a)?} \\
\hline
1 & ano & muž & ne & ne \\
2 & ne & žena & ano & ano \\
3 & ne & muž & ne & ano \\
4 & ano & žena & ano & ne \\
\end{tabular}
\caption{Původní trénovací data pro ilustraci metody bootstrap.}
\label{tab:bootstrap_original}
\end{table}

\item Chceme-li vytvořit "bootstrapem" dataset velikosti pět, pětkrát si náhodně vybereme řádek z tabulky s tím, že se řádky v našem výběru mohou opakovat.

\item Vybereme-li např. řádky s id 1, 4, 3, 3, 1, dostaneme dataset:

\begin{table}[h]
\centering
\begin{tabular}{c|cccc}
\textbf{id} & \textbf{rýmička} & \textbf{pohlaví} & $> \mathbf{39}^\circ$\textbf{C} & \textbf{vstal(a)?} \\
\hline
1 & ano & muž & ne & ne \\
4 & ano & žena & ano & ne \\
3 & ne & muž & ne & ano \\
3 & ne & muž & ne & ano \\
1 & ano & muž & ne & ne \\
\end{tabular}
\caption{Dataset vytvořený metodou bootstrap (výběr s opakováním).}
\label{tab:bootstrap_result}
\end{table}

\end{itemize}

\subsection{Predikce náhodného lesa}

\begin{itemize}

\item {\bf V případě regresního problému} (spojité $Y$) se postupuje analogicky jako u regresního rozhodovacího stromu: predikce náhodného lesa se bere jako {\bf průměr z predikcí jednotlivých stromů} lesa:
$$\hat{Y} = \frac{1}{n}\sum_{i=1}^{n} \hat{Y}_i$$

\item {\bf Pro klasifikaci:} Implementace \texttt{RandomForestClassifier} ve \texttt{scikit-learn} pracuje namísto predikcí tříd s predikcemi pravděpodobností tříd od jednotlivých podmodelů (relativní četnosti tříd v listech).

\item Tj. např. u binární klasifikace, pokud označíme jako $\hat{p}_i = P(\hat{Y} = 1 \mid T_i, X = \mathbf{x})$ predikci pravděpodobnosti příslušnosti bodu $\mathbf{x}$ ke třídě 1 podmodelu $T_i$, pak finální predikovaná pravděpodobnost třídy 1 pro náhodný strom je určena průměrem:
$$\hat{p} = \frac{1}{n}\sum_{i=1}^{n} \hat{p}_i$$

\item Finální predikce vysvětlované proměnné $Y$ je pak určena obvyklým porovnáním této pravděpodobnosti s číslem $1/2$ jako
$$\hat{Y} = \begin{cases}
1 & \text{pro } \hat{p} > 0.5 \\
0 & \text{jinak}
\end{cases}$$

\end{itemize}

\subsection{Hyperparametry náhodných lesů}

\begin{itemize}

\item Základními hyperparametry metod \texttt{RandomForestClassifier} a \texttt{RandomForestRegressor} v \texttt{scikit-learn} jsou:

\begin{itemize}
\item \texttt{n\_estimators}: Určuje počet stromů v náhodném lese.

\item \texttt{max\_depth}: Určuje maximální hloubku stromů v lese. {\bf Rozdíl oproti boostingu:} Na rozdíl od boostingu (kde se používají mělké stromy jako weak learners), náhodné lesy používají hluboké stromy s nízkým biasem a redukují jejich varianci průměrováním.

\item \texttt{max\_features}: Počet (náhodně vybraných) příznaků, ze kterých si hladový algoritmus vybírá ten, podle kterého bude v aktuálním kroku data "větvit". Defaultně se volí hodnota $\sqrt{p}$, tj. odmocnina počtu příznaků.

\item Je možné nastavovat i další obvyklé hyperparametry rozhodovacích stromů jakožto podmodelů:
\begin{itemize}
\item \texttt{min\_sample\_split}: Minimální počet dat v uzlu nutných k rozdělení uzlu.
\item \texttt{min\_sample\_leaf}: Minimální počet dat, která musí být v listu.
\item \texttt{criterion}: Funkce použitá při konstrukci stromu (entropie, gini index).
\end{itemize}

\end{itemize}

\end{itemize}

\subsection{Poznámky k náhodným lesům}

\begin{itemize}

\item U baggingu, který skládá rozhodnutí z více podmodelů, je vhodné, aby jednotlivé podmodely nebyly stejné, ale naopak co {\bf nejpestřejší}.

\item Toho se docílí nějakým druhem randomizace; v případě náhodných lesů je tento náhodný prvek daný bootstrap metodou pro generování jednotlivých trénovacích datasetů a také hodnotou parametru \texttt{max\_features}.

\item Jak víme, rozhodovací stromy jsou velice citlivé na změny v trénovacích datech (mají velký rozptyl), a tak často stačí odlišnost datasetů daná bootstrapem, abychom získávali velice odlišné rozhodovací stromy.

\item Tím, že při baggingu počítáme průměry predikcí těchto rozdílných podmodelů, snažíme se {\bf redukovat rozptyl} (a povětšinou se to úspěšně daří).

\item Přestože mohou být jednotlivé stromy samy o sobě slabé modely (podmodelům v ensemble metodách se proto často říká {\bf weak learners}), jejich kolektivní rozhodování dává až překvapivě dobré výsledky.

\item Náhodné lesy jsou na rozdíl od rozhodovacích stromů {\bf velice robustní a poměrně odolné vůči přeučení}. Bohužel ale částečně ztrácejí jejich jednoduchost a snadnou interpretovatelnost.

\item U ensemble metod chceme, aby jednotlivé modely (zde stromy) nebyly stejné, ale co nejrůznorodější — toho lze docílit pomocí randomizace — u náhodných lesů toho docílíme pomocí náhodné metody bootstrap a pomocí hodnoty \texttt{max\_features} (maximální počet features, se kterými každý strom pracuje).

\end{itemize}

\subsection{Boosting obecně}


\begin{shaded}
\noindent\textbf{Otázka ke zkoušce 10 (Ensemble metody: bagging, boosting a AdaBoost):} 
Rozdíl mezi baggingem a boostingem. Popis metody AdaBoost.
\end{shaded}


\begin{itemize}
\item Základní myšlenka boostingu je podobná jako u baggingu: budujeme více modelů (např. rozhodovacích stromů) a jejich finální rozhodnutí je (váženou) kompozicí rozhodnutí jednotlivých modelů.

\item {\bf Na rozdíl od baggingu} ale při boostingu nejsou tyto modely nezávislé, ale jsou {\bf seřazené a každý další je ovlivněn těmi předchozími}.

\item Tento vliv je při AdaBoostu realizován pomocí {\bf vah datových bodů}: Při konstrukci $m$-tého stromu je zvýšena váha těm bodům, které předchozí $(m-1)$-tý strom klasifikoval špatně.

\item Takovýmito změnami vah je zajištěno, že se další model soustředí více na ty datové body, se kterými si předchozí modely neporadily.

\item Přibližně takto funguje boosting obecně, my si dále ukážeme konkrétní implementaci této myšlenky známou jako algoritmus {\bf AdaBoost} [Freund, Schapire (1997)].

\end{itemize}

\subsection{AdaBoost (Adaptive Boosting): popis algoritmu}

\begin{itemize}

\item Na začátku máme dataset $\mathcal{D}$ s $N$ datovými body.

\item Pro jednoduchost opět uvažujeme binární klasifikaci. Existují ale i modifikace algoritmu pro více než dvě třídy a i pro regresi.

\item Počet zkonstruovaných stromů je zadán uživatelem v hyperparametru \texttt{n\_estimators}.

\item {\bf AdaBoost algoritmus:}

\begin{enumerate}

\item Nastavme váhy rovnoměrně, tedy $w_i = \frac{1}{N}$ a položme $m = 1$.

\item Pokud $m \leq$ \texttt{n\_estimators}, naučme strom $T^{(m)}$ na datech $\mathcal{D}$ s váhami $w_i$.

\item Do proměnné $e^{(m)}$ uložme součet vah těch bodů z $\mathcal{D}$, které jsou špatně klasifikované stromem $T^{(m)}$.

\item Pokud je $e^{(m)} = 0$ skončeme, všechna data jsou klasifikována správně.

\item Položme
$$\alpha^{(m)} = \texttt{learning\_rate} \cdot \log \frac{1 - e^{(m)}}{e^{(m)}},$$
kde \texttt{learning\_rate} je hyperparametr zadaný uživatelem; používá se ke zpomalení trénování a k zabránění přeučení (je-li menší než jedna).

\item Pro stromem $T^{(m)}$ špatně klasifikované body nastavme nové váhy
$$w_i \leftarrow w_i \exp(\alpha^{(m)}).$$

\item Znormalizujme váhy tak, aby jejich součet byl jedna.

\item Zvětšíme $m$ o jedna a vraťme se do bodu 2.

\end{enumerate}

\item Výsledkem algoritmu je tedy až \texttt{n\_estimators} rozhodovacích stromů $T^{(1)}, T^{(2)}, \ldots$

\end{itemize}

\subsection{AdaBoost: predikce}

\begin{itemize}

\item Abychom určili rozhodnutí tohoto modelu pro nějaký datový bod $\mathbf{x}$, postupujeme následovně:

\begin{enumerate}

\item Každému stromu $T^{(m)}$ přiřadíme váhu danou číslem $\alpha^{(m)}$ z kroku 5 algoritmu.

\item Sečti váhy $\alpha^{(m)}$ všech stromů, které pro $\mathbf{x}$ predikují $Y = 1$ a to samé udělej pro stromy predikující $Y = 0$.

\item Rozhodni se pro tu z možností, pro kterou je součet vah vyšší.

\end{enumerate}

\end{itemize}

\subsection{Poznámky k AdaBoostu}

\begin{itemize}

\item Verze algoritmu pro „více než binární" klasifikaci se nazývá {\bf AdaBoost-SAMME} [Zhu, Rosset, Zou, Hastie (2006)]. Tato verze je implementována v \texttt{sklearn}.

\item Varianta pro regresi se zase nazývá {\bf AdaBoost.R2} [Drucker (1997)].

\item AdaBoost nemusí nutně používat rozhodovací stromy; je možné použít jakýkoli model, který umí pracovat s parametrem \texttt{sample\_weight}.

\item V \texttt{sklearn} implementaci jsou rozhodovací stromy (hloubky 3) výchozí volba (parametr \texttt{base\_estimator}).

\item Parametr \texttt{learning\_rate} je obvykle zaváděn u většiny ensemble metod spadajících do kategorie Boostingu.

\item Jedná se o tzv. {\bf regularizaci}: čím je \texttt{learning\_rate} nižší, tím je model odolnější vůči přeučení. Nevýhodou je, že je pak obvykle nutné zvýšit počet stromů (parametr \texttt{n\_estimators}).

\item Při boostingu dochází k postupnému korigování chyb předchozích modelů, což vede na {\bf redukci vychýlení (bias)}. Podmodely se typicky konstruují jako {\bf slabé modely (weak learners)} — tj. např. jako poměrně mělké stromy.

\end{itemize}


\subsection{Zajímavá videa na YouTube}

\begin{itemize}
\item \href{https://youtu.be/LsK-xG1cLYA?si=gwOTzUiS3iIAhfdR}{
AdaBoost, Clearly Explained (StatQuest with Josh Starmer)} \hfill \textbf{[21~min]}
\end{itemize}

\section{Evaluace modelů: metriky}
\label{sec:evaluace_metriky}

\begin{shaded}
\noindent\textbf{Otázka ke zkoušce 11 (Evaluace modelů. Metriky):} 
Vyhodnocovací metriky regrese a klasifikace (odvozené z matice záměn, ROC, AUC).
\end{shaded}

\subsection{Úvod k evaluaci modelů}

\begin{itemize}

\item Obvykle existuje více kandidátů na finální model a je potřeba vybrat ten nejlepší. K tomu je nutné umět kvantifikovat kvalitu modelu, tedy jeho výkonnost.

\item Metodu výpočtu výkonnosti volíme podle cíle (někdy je prioritou nízká chybovost obecně, jindy spíše nižší konkrétní typ chyby, nebo žádné velké chyby ale s možností malých chybovostí, atd.).

\item {\bf Evaluace modelů:}
\begin{itemize}
\item Hlavní výzvou strojového učení je, aby natrénovaný model dobře fungoval i na nových vstupech, které nikdy neviděl — {\bf schopnost generalizace}.
\item Zaměřuje se na měření výkonnosti modelů supervizovaného učení.
\end{itemize}

\end{itemize}

\subsection{Ztrátová funkce}

\begin{itemize}

\item Model predikuje $\hat{Y} = \hat{Y}(X)$, které je funkcí $X$.

\item Chybu predikce $Y$ pomocí $\hat{Y}$ měříme pomocí {\bf ztrátové funkce} (angl. loss function) $L$.

\item {\bf U binární klasifikace} model často odhaduje pravděpodobnost:
$$\hat{p} = \hat{P}(Y = 1 \mid X = \mathbf{x})$$

\item Běžná ztrátová funkce je {\bf binary cross-entropy loss}:
$$L(Y, \hat{p}) = -Y \log \hat{p} - (1 - Y) \log(1 - \hat{p})$$

\end{itemize}

\subsection{Metriky pro regresi}

\begin{itemize}

\item {\bf MSE} (Mean Squared Error) — nejobvyklejší volba kvadratické chyby jako ztrátové funkce vede na evaluaci pomocí střední kvadratické chyby. Tato míra penalizuje především velké odchylky a je velmi citlivá na odlehlé hodnoty (angl. outliers):
$$MSE = \frac{1}{N} \sum_{i=1}^{N} (Y_i - \hat{Y}_i)^2$$

\item {\bf RMSE} (Root Mean Squared Error) — nelineárně přeškálované $MSE$ — má stejné vlastnosti jako $MSE$, ale je ve stejných jednotkách jako vysvětlovaná proměnná:
$$RMSE = \sqrt{MSE} = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (Y_i - \hat{Y}_i)^2}$$

\item {\bf RMSLE} (Root Mean Squared Logarithmic Error) — pro nezáporné hodnoty vysvětlované proměnné, soustředí se na relativní míru odchylek — pro malé hodnoty řeší i malé odchylky, pro velké hodnoty pouze ty velké, méně citlivá na odlehlé hodnoty:
$$RMSLE = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (\log Y_i - \log \hat{Y}_i)^2}$$

\item {\bf MAE} (Mean Absolute Error) — odchylky se skládají lineárně:
$$MAE = \frac{1}{N} \sum_{i=1}^{N} |Y_i - \hat{Y}_i|$$

\item $\mathbf{R^2}$ \textbf{(koeficient determinace)} — vyjadřuje, jaký podíl variability cílové proměnné model vysvětluje. Hodnota $R^2 = 1$ znamená perfektní predikci, $R^2 = 0$ znamená, že model nevysvětluje žádnou variabilitu (model je stejně dobrý jako průměr):
$$R^2 = 1 - \frac{\sum_{i=1}^{N}(Y_i - \hat{Y}_i)^2}{\sum_{i=1}^{N}(Y_i - \bar{Y})^2}$$

\end{itemize}

\subsection{Metriky pro klasifikaci}

\begin{itemize}

\item Při klasifikaci se chyba měřená pomocí ztrátové funkce příliš nehodí.

\item Nejčastěji se vyhodnocení klasifikačních modelů provádí od měr odvozených z {\bf matice záměn} (angl. Confusion Matrix) — matice četností různých predikovaných hodnot $\hat{Y}_i$ proti různým skutečným hodnotám $Y_i$.

\end{itemize}

\subsubsection{Matice záměn (Confusion Matrix)}

\begin{itemize}

\item {\bf Základní pojmy pro binární klasifikaci:}
\begin{itemize}
\item $TP$ (True Positive) — model správně predikoval $Y = 1$
\item $FP$ (False Positive) — model špatně predikoval $Y = 0$ jako $Y = 1$
\item $FN$ (False Negative) — model špatně predikoval $Y = 1$ jako $Y = 0$
\item $TN$ (True Negative) — model správně predikoval $Y = 0$
\end{itemize}

\begin{table}[h]
\centering
\begin{tabular}{cc|cc|c}
& & \multicolumn{2}{c|}{\textbf{Skutečnost}} & \\
& & $Y = 1$ & $Y = 0$ & $\sum$ \\
\hline
\multirow{2}{*}{\textbf{Predikce}} & $\hat{Y} = 1$ & TP & FP & $\hat{N}_+ = \text{TP} + \text{FP}$ \\
& $\hat{Y} = 0$ & FN & TN & $\hat{N}_- = \text{FN} + \text{TN}$ \\
\hline
& $\sum$ & $N_+ = \text{TP} + \text{FN}$ & $N_- = \text{FP} + \text{TN}$ & $N = \text{TP} + \text{FP} + \text{FN} + \text{TN}$ \\
\end{tabular}
\caption{Matice záměn (angl. confusion matrix) pro binární klasifikaci.}
\label{tab:confusion_matrix}
\end{table}

\end{itemize}

\subsubsection{Odvozené metriky z matice záměn}

\begin{itemize}

\item {\bf TPR} (True Positive Rate) — sensitivita neboli {\bf recall} nebo hit rate:
$$TPR = \frac{TP}{N_+} = \frac{TP}{TP + FN}$$
Udává, jakou část skutečně pozitivních případů model správně identifikoval.

\item {\bf FPR} (False Positive Rate) — false alarm rate nebo type I error rate:
$$FPR = \frac{FP}{N_-} = \frac{FP}{FP + TN}$$
Udává, jakou část skutečně negativních případů model chybně klasifikoval jako pozitivní.

\item {\bf FNR} (False Negative Rate) — miss rate nebo type II error rate:
$$FNR = \frac{FN}{N_+} = \frac{FN}{TP + FN}$$
Udává, jakou část skutečně pozitivních případů model chybně klasifikoval jako negativní.

\item {\bf TNR} (True Negative Rate) — specificita nebo selektivita:
$$TNR = \frac{TN}{N_-} = \frac{TN}{FP + TN}$$
Udává, jakou část skutečně negativních případů model správně identifikoval.

\item {\bf PPV} (Positive Predictive Value) — {\bf precision} nebo přesnost:
$$PPV = \frac{TP}{\hat{N}_+} = \frac{TP}{TP + FP}$$
Udává, jaká část predikovaných pozitivních případů je skutečně pozitivních.

\begin{table}[h]
\centering
\begin{tabular}{c|c|c}
& $\mathbf{Y = 1}$ & $\mathbf{Y = 0}$ \\
\hline
$\hat{Y} = 1$ & $TPR = \frac{TP}{N_+}$ & $FPR = \frac{FP}{N_-}$ \\
$\hat{Y} = 0$ & $FNR = \frac{FN}{N_+}$ & $TNR = \frac{TN}{N_-}$ \\
\end{tabular}
\caption{Míry odvozené z matice záměn vyjádřené jako podíly.}
\label{tab:confusion_matrix_rates}
\end{table}

\item {\bf ACC} (Accuracy) — přesnost:
$$ACC = \frac{TP + TN}{N} = \frac{TP + TN}{TP + FP + FN + TN}$$
Nejpoužívanější míra (pozor: pokud je dataset nevyvážený, např. 99.9\% zdravých a 0.1\% nemocných, model predikující vždy zdravý bude mít dobrou přesnost, i když je model k ničemu).

\item {\bf F1 score} — harmonický průměr precision a recall — řeší předchozí problém:
$$F_1 = 2 \frac{PPV \cdot TPR}{PPV + TPR} = \frac{2TP}{2TP + FP + FN}$$

\end{itemize}

\subsection{ROC křivka}

\begin{itemize}

\item Při predikci používáme jistou hranici $\tau$ (angl. threshold), podle které určujeme výsledek predikce. Pokud je pravděpodobnost, že vysvětlovaná proměnná záznamu je 1 větší než $\tau$, pak predikujeme 1.

\item Hodnotu $\tau$ jsme doted nejčastěji brali jako $1/2$.

\item {\bf Chování TPR a FPR v závislosti na $\tau$:}
\begin{itemize}
\item Pro různé hodnoty prahu $\tau$ dostáváme různé hodnoty TPR a FPR.
\item Snížením prahu $\tau$ zvýšíme TPR (zachytíme více pozitivních případů), ale současně zvýšíme i FPR (více falešných poplachů).
\item Zvýšením prahu $\tau$ snížíme FPR, ale současně snížíme i TPR.
\end{itemize}

\item {\bf ROC křivka} (Receiver Operating Characteristic curve) zobrazuje závislost TPR na FPR pro různé hodnoty prahu $\tau$.

\item Pro dobrý model bude graf strmě stoupat k levému hornímu rohu a poté již velmi pomalu do pravého horního rohu.

\begin{center}
\includegraphics[width=0.8\textwidth]{assets/roc_krivka.png}
\end{center}
\begin{center}
\text{Obrázek 13: ROC křivka — závislost TPR na FPR}
\end{center}

\item {\bf Interpretace:}
\begin{itemize}
\item Červená křivka = ROC křivka naučeného modelu
\item Modrá čerchovaná přímka = náhodné predikce (model bez predikční síly)
\item Čím více se ROC křivka blíží k levému hornímu rohu, tím lepší je model
\item Ideální model by měl TPR = 1 a FPR = 0 (levý horní roh)
\end{itemize}

\end{itemize}

\subsection{AUC (Area Under Curve)}

\begin{itemize}

\item {\bf AUC} je plocha pod ROC křivkou (area under curve).

\item Model s náhodnými predikcemi bude mít v průměru plochu pod křivkou 0.5.

\item AUC dokonalého modelu bude rovno 1.

\item Většina modelů je mezi 0.5 a 1.

\begin{center}
\includegraphics[width=0.8\textwidth]{assets/plocha_pod_ROC_krivkou.png}
\end{center}
\begin{center}
\text{Obrázek 14: Plocha pod ROC křivkou (AUC = 0.91)}
\end{center}

\item {\bf AUC jako metrika:}
\begin{itemize}
\item AUC = 0.5: Model je stejně dobrý jako náhodné hádání.
\item 0.5 $<$ AUC $<$ 1.0: Model je lepší nežli náhodné hádání, ale není bezchybný v predikcích.
\item AUC = 1.0: Perfektní model.
\end{itemize}

\item {\bf Výhody AUC:}
\begin{itemize}
\item Nezávislá na volbě prahu $\tau$
\item Vhodná pro nevyvážené datasety
\item Poskytuje celkový pohled na výkonnost modelu napříč všemi prahy
\end{itemize}
\end{itemize}

\section{Evaluace modelů: testovací chyba a její odhad}
\label{sec:evaluace_testovaci}

\begin{shaded}
\noindent\textbf{Otázka ke zkoušce 12 (Evaluace modelů):} 
Testovací chyba a její odhad. Evaluační scénáře - trénovací, validační a testovací množina versus křížová validace.
\end{shaded}

\subsection{Vyhodnocovací scénáře}

\begin{itemize}

\item Chceme-li natrénovat model a pak odhadnout jeho testovací chybu, musíme mít k dispozici trénovací data a nezávislá testovací data.

\item Typicky máme více kandidátů na finální model a snažíme se vybrat nejlepší. To znamená najít hodnoty hyperparametrů tak, aby testovací chyba finálně vybraného modelu byla co nejmenší.

\item {\bf Z pohledu evaluace máme dva úkoly:}
\begin{itemize}
\item {\bf Výběr modelu} — odhadnout výkonnost různých modelů a vybrat nejlepší.
\item {\bf Ohodnocení modelu} — odhadnout testovací chybu finálního modelu.
\end{itemize}

\item Protože výběr modelu vlastně spadá do procesu trénování (vybereme model, který se nejlépe přizpůsobí datům), nesmíme použít stejná data pro výběr finálního modelu i pro ohodnocení tohoto finálního modelu. Data rozdělíme.

\end{itemize}

\subsection{Trénovací, validační a testovací množina}

\begin{itemize}

\item Když máme dostatek dat, rozdělíme vstupní data na tři části: {\bf trénovací}, {\bf validační} a {\bf testovací}.

\begin{center}
\includegraphics[width=0.8\textwidth]{assets/trenovaci_validacni_testovaci.png}
\end{center}
\begin{center}
\text{Obrázek 15: Rozdělení dat na trénovací, validační a testovací množinu}
\end{center}
\hspace{0.1cm}

\item {\bf Trénovací část} — použijeme pro trénování konkrétních modelů se zafixovanými hyperparametry.

\item {\bf Validační část} — použijeme k ohodnocení daného modelu a porovnání s ostatními modely. Finální model vybereme na základě validační množiny.

\item {\bf Testovací část} — použijeme až v závěrečné fázi, když už máme vybraný a natrénovaný finální model. Tím získáme odhad testovací chyby, kterou můžeme očekávat na nových datech. Tento způsob práce s testovací množinou se nazývá {\bf hold-out}.

\item Trénovací, validační a testovací data by měla pocházet ze stejného rozdělení.

\item V případě chronologických dat (ceny na burze) není vhodné předem permutovat data — testovací (a případně i validační) data by měla správně reflektovat chronologické řazení.

\end{itemize}

\subsection{Křížová validace}

\begin{itemize}

\item Když nemáme dostatek dat, nebývá rozumné dělit je na trénovací, validační a testovací část.

\item Oddělíme si testovací data, ale trénovací a validační již dělit nebudeme.

\item {\bf $k$-násobná křížová validace} (angl. $k$-fold cross-validation):

\begin{itemize}
\item Trénovací data $\mathcal{D}$ náhodně rozdělíme na $k$ podobně velkých částí $\mathcal{D}_1, \ldots, \mathcal{D}_k$.

\item Pro každé $j = 1, \ldots, k$ model s danými hodnotami hyperparametrů natrénujeme na datech z množiny $\left(\bigcup_{i=1}^{k} \mathcal{D}_i\right) \setminus \mathcal{D}_j$.

\item Na množině $\mathcal{D}_j$ odhadneme jeho chybu jako $e_j$.

\item Nakonec vrátíme průměrnou {\bf cross-validační chybu}:
$$\hat{e} = \frac{1}{k} \sum_{i=1}^{k} e_i$$

\item Tento proces zopakujeme pro všechny zkoumané hodnoty hyperparametrů a na závěr vybereme jako nejlepší volbu ty hodnoty, které vedly k nejmenší cross-validační chybě.

\item Hodnota $k$ se typicky volí 5 až 10.
\end{itemize}

\begin{center}
\includegraphics[width=0.8\textwidth]{assets/trenovaci_D1_testovaci_D2_trenovaci_D3_trenovaci_D4_trenovaci_D5.png}
\end{center}
\begin{center}
\text{Obrázek 16: Příklad 5-násobné křížové validace}
\end{center}

\item {\bf Poznámky:}
\begin{itemize}
\item Může být neúnosně výpočetně náročná.
\item Pro fixní model je cross-validační chyba odhadem očekávané testovací chyby $Err$, nikoliv testovací chyby $Err_\mathcal{D}$.
\item V případě, kdy máme opravdu málo dat a nechceme ani oddělovat testovací množinu, můžeme namísto toho udělat dvoustupňovou validaci — vnitřní křížovou validaci používáme na výběr nejlepšího modelu a vnější na odhad očekávané chyby. Výsledná vnější cross-validační chyba ale odpovídá chybě celé procedury pro výběr nejlepšího modelu, nikoliv chybě konkrétního modelu.
\end{itemize}

\end{itemize}

\subsection{Trénovací chyba}

\begin{itemize}

\item Při učení se snažíme minimalizovat chybu predikce měřenou pomocí průměrné hodnoty ztrátové funkce $L$ na trénovacích datech, kde trénovací množina je $N$ dvojic $(Y_i, \mathbf{x}_i)$:
$$\mathcal{L} = \frac{1}{N} \sum_{i=1}^{N} L(Y_i, \hat{Y}(\mathbf{x}_i))$$

\item Tato průměrná hodnota se nazývá {\bf trénovací chyba} (angl. training error) a občas se značí jako $err_{train}$. Někdy se jí také říká trénovací ztráta (angl. training loss).

\end{itemize}

\subsection{Učení modelu}

\begin{itemize}

\item Během procesu učení modelu se zafixovanými hyperparametry se snažíme najít hodnoty parametrů, které minimalizují trénovací chybu.

\item Jakmile je model natrénován, zajímá nás jeho {\bf schopnost generalizace} — jak dobře funguje na nových datech.

\end{itemize}

\subsection{Testovací chyba}

\begin{itemize}

\item {\bf Testovací chyba} (angl. test error) neboli {\bf generalizační chyba} (angl. generalization error) je střední hodnota chyby na novém vstupu $X$ podmíněná danými trénovacími daty:
$$Err_\mathcal{D} = \mathbb{E}\left[L(Y, \hat{Y}(X)) \mid \mathcal{D}\right]$$
kde $\mathcal{D} = \{(Y_1, \mathbf{x}_1), \ldots, (Y_N, \mathbf{x}_N)\}$ značí trénovací data.

\item Testovací chybu můžeme odhadnout výběrovým průměrem:
$$err_{test} = \frac{1}{N_{test}} \sum_{i=1}^{N_{test}} L(Y_i, \hat{Y}(\mathbf{x}_i))$$
změřeným na testovacích datech $(Y_1, \mathbf{x}_1), \ldots, (Y_{N_{test}}, \mathbf{x}_{N_{test}})$, které byla získána nezávisle na trénovacích datech $\mathcal{D}$.

\item Poznamenejme, že $err_{test}$ podmíněno trénovacími daty je nestranný odhad $Err_\mathcal{D}$, tj.:
$$\mathbb{E}\left[err_{test} \mid \mathcal{D}\right] = Err_\mathcal{D}$$

\item {\bf Očekávaná testovací chyba} neboli {\bf očekávaná chyba predikce} (angl. expected test error, expected prediction error) je definována jako střední hodnota testovací chyby vzhledem k náhodnému výběru trénovací množiny:
$$Err = \mathbb{E}\left[Err_\mathcal{D}\right] = \mathbb{E}\left[L(Y, \hat{Y}(X))\right]$$

\end{itemize}

\section{Výběr příznaků (Feature Selection)}
\label{sec:lasso}

\begin{shaded}
\noindent\textbf{Otázka ke zkoušce 13 (Výběr příznaků):} 
Základní metody výběru příznaků (filtrační, obalové, vestavěné), Lasso.
\end{shaded}

\subsection{Úvod do výběru příznaků}

\begin{itemize}

\item {\bf Výběr příznaků} (angl. feature selection nebo také subset selection) je proces, při kterém se ze všech dostupných příznaků vybírá jistá podmnožina, kterou se použije pro trénování a predikci modelu.

\item Problematika výběru příznaků spadá do části předzpracování dat.

\item Z jistého pohledu se jedná o podoblast {\bf redukce dimenzionality} (angl. dimensionality reduction). Do té ale spadají především metody, které napočítávají (lineárně nebo nelineárně) kompletně nové příznaky. Proto se redukce dimenzionality často uvádí jako separátní oblast.

\item {\bf Účel výběru příznaků:}
\begin{itemize}
\item Zahozením nerelevantních a redundantních příznaků lze významně zlepšit schopnost generalizace modelu.
\item Pomáhá s prokletím dimenzionality — pro některé modely je velká dimenze prostoru příznaků problematická (např. kNN).
\item Zlepšuje vysvětlitelnost modelu — u modelů, které využívají menší počet příznaků, bývá jednodušší porozumět tomu, na základě čeho se rozhodují.
\item Snižuje výpočetní nároky pro trénování a použití výsledného modelu.
\end{itemize}

\item {\bf Poznámky k nerelevantním a redundantním příznakům:}
\begin{itemize}
\item Příznaky, které jsou nerelevantní a nesouvisí s vysvětlovanou proměnnou, škodí typicky i modelům, které by si s nimi teoreticky dokázaly poradit — jako např. lineární regrese (může tam použít koeficient 0) nebo rozhodovací strom (nepoužije tento příznak do žádného dělícího kritéria).
\item Redundantní příznaky jsou takové, které nesou stejnou informaci. Kromě toho, že zbytečně zvyšují dimenzi, mohou některým modelům vyloženě škodit, jako např. problém kolinearity u lineární regrese.
\end{itemize}

\end{itemize}

\subsection{Filtrační metody}

\begin{itemize}

\item {\bf Filtrační metody} (angl. filter methods) mohou být supervizované i nesupervizované. Typicky zkoumají, jak hodně informace může být v daném příznaku a případně jak ta informace souvisí s vysvětlovanou proměnnou.

\item {\bf Nesupervizované filtrační metody} (bez využití vysvětlované proměnné):
\begin{itemize}
\item Vyhodit příznaky, které mají příliš nízký rozptyl a jsou tedy téměř konstantní.
\item Vyhodit příznaky, které mají příliš chybějících hodnot.
\item Vyhodit některé z příznaků, které spolu hodně korelují a jsou tedy redundantní.
\end{itemize}

\item {\bf Supervizované filtrační metody} (s využitím vysvětlované proměnné):
\begin{itemize}
\item Vyhodit příznaky, které mají nízkou korelaci s vysvětlovanou proměnnou.
\item U binárních příznaků rozdělit vysvětlovanou na dvě populace a udělat test hypotézy o rovnosti středních hodnot (dvouvýběrový t-test). Pokud vyjde velká p-hodnota, lze tento příznak vyhodit.
\item U diskrétních příznaků i diskrétní vysvětlované proměnné udělat test hypotézy o nezávislosti těchto dvou diskrétních veličin (např. chí-kvadrát test nezávislosti). Pokud vyjde velká p-hodnota, je tento příznak kandidátem na vyhození.
\end{itemize}

\end{itemize}

\subsection{Obalové metody}

\begin{itemize}

\item {\bf Obalové metody} (angl. wrapper methods) využívají nějaký pomocný model pro ohodnocování podmnožin příznaků. Cílem je vybrat takovou podmnožinu, pro kterou je výkonnost modelu největší.

\item Pro každou kandidátní podmnožinu se model natrénuje a změří jeho výkonnost na validační množině (nebo křížovou validací). Jako finální podmnožina příznaků je použita taková, pro kterou je model nejvýkonnější.

\item {\bf Výhody:} Preferují podmnožiny příznaků, které jsou pro daný model výhodné.

\item {\bf Nevýhody:}
\begin{itemize}
\item Může snadno dojít k přeučení.
\item Takto vybrané podmnožiny nemusí být vhodné pro jiné modely (např. pro ten finální, který se má použít).
\item Vysoká výpočetní náročnost — i při použití jednoduchého pomocného modelu (jako např. lineární regrese) může být výpočetní čas značný.
\end{itemize}

\item {\bf Nejpoužívanější obalové metody:}

\begin{itemize}
\item {\bf Dopředný výběr} (angl. forward selection) — začíná se s prázdnou množinou příznaků a postupně se po jednom přidávají vždy tak, aby ten jeden přidaný nejvíce zlepšil výkonnost modelu v dané iteraci. Končí se, když je dosažen požadovaný počet příznaků, případně když už nedochází ke zlepšování výkonnosti.

\item {\bf Zpětný výběr} (angl. backward selection) — začíná se se všemi příznaky a postupně se po jednom odebírají tak, aby ten odebraný příznak vedl na nejvýkonnější model v dané iteraci. Končí se, když je dosažen požadovaný počet příznaků, případně když už nedochází ke zlepšování výkonnosti.

\item {\bf Rekurzivní odebírání příznaků} (angl. recursive feature elimination) — probíhá podobně jako zpětný výběr, ale model se využívá trochu jiným způsobem. Konkrétně se k vybírání příznaků používá vnitřní ohodnocení důležitosti jednotlivých příznaků modelem. U lineární (logistické) regrese to je například absolutní hodnota koeficientu u daného příznaku. U rozhodovacího stromu to může být informační zisk daného příznaku.
\end{itemize}

\item {\bf Implementace v \texttt{sklearn}:}
\begin{itemize}
\item Dopředný i zpětný výběr: \texttt{sklearn.feature\_selection.SequentialFeatureSelector}
\item Rekurzivní odebírání: \texttt{sklearn.feature\_selection.RFE}
\end{itemize}

\end{itemize}

\subsection{Vestavěné metody}

\begin{itemize}

\item {\bf Vestavěné metody} (angl. embedded methods) využívají model, který se trénuje pouze jednou na celých datech a při tom implicitně provede výběr příznaků.

\item Tento implicitní výběr se projeví tak, že se model naučí některé příznaky vůbec nevyužívat — např. u lineární regrese jsou příslušné koeficienty odhadnuty jako 0.

\item Modelem, který je nejpoužívanější, je $L_1$ regularizovaná lineární regrese ({\bf Lasso}). V případě klasifikace to je pak $L_1$ regularizovaná logistická regrese.

\item Jako vybraná podmnožina příznaků se vezmou příznaky, u kterých jsou příslušné koeficienty nenulové.

\end{itemize}

\subsection{Lasso regrese}

\begin{itemize}

\item {\bf Lasso} (zkr. angl. Least Absolute Shrinkage and Selection Operator) [Tibshirani (1996)] nebo také $L_1$ regularizace zavádí penalizační člen úměrný součtu absolutních hodnot složek vektoru koeficientů $\mathbf{w}$ s vynecháním interceptu.

\item Minimalizuje se {\bf regularizovaný reziduální součet čtverců}:
$$RSS^{Lasso}_\lambda(\mathbf{w}) = \|\mathbf{Y} - \mathbf{X}\mathbf{w}\|^2 + \lambda \sum_{i=1}^{p} |w_i|$$
který závisí na parametru $\lambda \geq 0$.

\item {\bf Vlastnosti Lasso:}
\begin{itemize}
\item Pro $\lambda = 0$ dostáváme $RSS^{Lasso}_0(\mathbf{w}) = RSS(\mathbf{w})$ a máme tedy obyčejnou metodu nejmenších čtverců.
\item Pro $\lambda > 0$ se v minimu cílí na takové vektory $\mathbf{w}$, které mají co nejmenší složky.
\item Hodnota $w_0$ interceptu se nijak nepenalizuje. Jedná se pouze o vertikální posun, který zajišťuje předpoklad $\mathbb{E}\varepsilon = 0$ modelu a je tedy vhodné ho neomezovat.
\end{itemize}

\item {\bf Rozdíl oproti hřebenové regresi:}
\begin{itemize}
\item Na rozdíl od modelu hřebenové regrese není regularizační člen $\sum_{i=1}^{p} |w_i|$ diferencovatelný v bodech, kde $w_j = 0$.
\item V tomto případě není možné nalézt explicitní řešení a existují pouze iterativní metody, které najdou řešení:
$$\hat{\mathbf{w}}^{Lasso}_\lambda = \arg\min_{\mathbf{w}} RSS^{Lasso}_\lambda(\mathbf{w})$$
\end{itemize}

\item {\bf Řídké řešení} (angl. sparse solution):
\begin{itemize}
\item Výhoda modelu Lasso je, že řešení je tzv. řídké.
\item To znamená, že odhady $\hat{w}^{Lasso}_{\lambda;j}$ některých složek $\mathbf{w}$ jsou rovny 0. Samozřejmě tím častěji, čím je $\lambda$ větší.
\item Tím se Lasso zásadně liší od hřebenové regrese, kde tento efekt v podstatě nikdy nenastane.
\end{itemize}

\item {\bf Geometrická interpretace:}

\begin{center}
\includegraphics[width=0.8\textwidth]{assets/lasso.png}
\label{fig:lasso_p}
\end{center}
\begin{center}
\text{Obrázek 17: Porovnání regularizace Lasso (vlevo) a Ridge (vpravo)}
\end{center}
\vspace{0.3cm}

Na \hyperref[fig:lasso_p]{obrázku 17} jsou znázorněny vrstevnice funkcí, které se minimalizují u Lasso (vlevo) a hřebenové regrese (vpravo). Na osách jsou jednotlivé složky vektoru vah $w_1$ a $w_2$. Červené elipsy jsou vrstevnice odpovídající neregularizované části $\|\mathbf{Y} - \mathbf{X}\mathbf{w}\|^2$, což je v zásadě parabolická jáma s minimem v bodě $\hat{\mathbf{w}}_{OLS}$. Tyrkysově je pak znázorněna oblast, která odpovídá regularizačnímu členu (omezení na normu vah).

U Lasso má tato oblast tvar {\bf kosočtverce} (resp. v obecné dimenzi polytop), protože omezení $\sum_{i=1}^{p} |w_i| \leq B$ definuje $L_1$ kouli. U hřebenové regrese má oblast tvar {\bf kruhu} (resp. v obecné dimenzi hyperkoule), protože omezení $\sum_{i=1}^{p} w_i^2 \leq B$ definuje $L_2$ kouli.

U Lasso existuje velká šance, že se vrstevnice RSS dotýká některého z rohů kosočtverce, což znamená, že je jedna ze složek $\mathbf{w}$ rovna 0. U hřebenové regrese se to samozřejmě také může stát, ale je to velmi nepravděpodobné, protože kruh nemá rohy.

\item {\bf Ekvivalentní formulace jako vázaná optimalizace:}

Úloha minimalizace $RSS^{Lasso}_\lambda(\mathbf{w})$ je ekvivalentní úloze vázané minimalizace:
$$\min_{\mathbf{w}} RSS(\mathbf{w}) = \|\mathbf{Y} - \mathbf{X}\mathbf{w}\|^2 \quad \text{za podmínky} \quad \sum_{i=1}^{p} |w_i| \leq B, \ \text{pro nějaké } B > 0.$$

Analogicky pro hřebenovou regresi platí, že minimalizace $RSS^{Ridge}_\lambda(\mathbf{w})$ je ekvivalentní úloze vázané minimalizace $RSS(\mathbf{w})$ za podmínky $\sum_{i=1}^{p} w_i^2 \leq B$.

\item {\bf Porovnání OLS, Ridge a Lasso pro ortonormální příznaky} ($\mathbf{X}^T\mathbf{X} = I$):

\begin{itemize}
\item Metoda nejmenších čtverců: $\hat{\mathbf{w}}_{OLS} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y} = \mathbf{X}^T\mathbf{Y}$

\item Hřebenová regrese: 
$$\hat{w}^{Ridge}_{\lambda;0} = \hat{w}^{OLS}_0 \quad \text{a} \quad \hat{w}^{Ridge}_{\lambda;j} = \frac{1}{1+\lambda}\hat{w}^{OLS}_j$$

\item Lasso (soft thresholding):
$$\hat{w}^{Lasso}_{\lambda;0} = \hat{w}^{OLS}_0 \quad \text{a} \quad \hat{w}^{Lasso}_{\lambda;j} = \text{sgn}(\hat{w}^{OLS}_j) \cdot \max\left(0, |\hat{w}^{OLS}_j| - \frac{\lambda}{2}\right)$$
\end{itemize}

\item {\bf Implementace a poznámky:}
\begin{itemize}
\item Lasso je v knihovně \texttt{sklearn} implementováno pomocí \texttt{sklearn.linear\_model.Lasso}.
\item Při využití pro výběr příznaků se dá použít implementace \texttt{sklearn.feature\_selection.SelectFromModel}.
\item U logistické regrese by se regularizační člen přidal k mínus logaritmu věrohodnostní funkce (k binární relativní entropii) a pak by se prováděla minimalizace.
\item Jednou z nevýhod Lasso proti hřebenové regresi je, že v případě kolineárních příznaků má tendenci vybrat pouze některé z nich. To se v některých případech při predikci na nových datech ukazuje jako jistá nevýhoda oproti hřebenové regresi, která typicky využije všechny příznaky.
\end{itemize}

\end{itemize}

\subsection{Elastic Net regrese}

\begin{itemize}

\item {\bf Elastic Net} je model, který kombinuje oba způsoby regularizace — $L_1$ (Lasso) i $L_2$ (Ridge) — a spojuje tak výhody obou přístupů.

\item Minimalizuje se:
$$RSS^{ElasticNet}_{\lambda_1, \lambda_2}(\mathbf{w}) = \|\mathbf{Y} - \mathbf{X}\mathbf{w}\|^2 + \lambda_1 \sum_{i=1}^{p} |w_i| + \lambda_2 \sum_{i=1}^{p} w_i^2$$

\end{itemize}


\section{Hierarchické shlukování (Hierarchical Clustering)}
\label{sec:hier_clustering}

\begin{shaded}
\noindent\textbf{Otázka ke zkoušce 14 (Hierarchické shlukování):} 
Cíle nesupervizovaného učení a shlukování. Vzdálenost (metrika): definice a příklady. Aglomerativní algoritmus, měření vzdáleností shluků, dendrogram.
\end{shaded}

\subsection{Nesupervizované učení (Unsupervised Learning)}

\begin{itemize}

\item {\bf Nesupervizované učení} (angl. unsupervised learning) nastává v situaci, kdy data nemáme nikterak označena. Tj. nemáme žádnou veličinu, kterou bychom u trénovacích dat znali a snažili se ji naučit predikovat.

\item Cílem nesupervizovaného učení je porozumět struktuře dat pouze na základě jich samotných. To znamená bez nějakého vnějšího vodítka. Proto se nesupervizovanému učení také říká {\bf učení bez učitele}.

\item Porozuměním zde typicky rozumíme nalezení co „nejmenších" oblastí v prostoru příznaků, kde se data vyskytují nejčastěji.

\item Obvykle totiž platí, že se naměřená skutečná data nevyskytují v celém prostoru stejně pravděpodobně, ale bývají více či méně lokalizována — tvoří nějaké shluky, vyskytují se v méně-dimenzionálních oblastech atd.

\item Porozumění této lokalizaci přináší důležitou informaci o vnitřní struktuře dat!

\begin{center}
\includegraphics[width=0.55\textwidth]{assets/rozmistene_body_v_R_na_3.png}
\label{fig:rozmistene_body_v_R_na_3}
\end{center}
\begin{center}
\text{Obrázek 18: Data rozložená v třírozměrném prostoru na kouli}
\end{center}
\vspace{0.3cm}

\item Na \hyperref[fig:rozmistene_body_v_R_na_3]{obrázku 18} jsou znázorněna data rozložená v třírozměrném prostoru. Podaří-li se zjistit, že jsou všechny tyto body na kouli, je možné jejich polohu popsat pomocí sférických souřadnic a získat tak ekvivalentní reprezentaci pomocí dvou spojitých příznaků (tzv. {\bf redukce dimenzionality}). Navíc je možné detekovat, že hustota bodů na pólech je vyšší než hustota bodů na rovníku.

\item {\bf Obecný problém nesupervizovaného učení:} Vůbec není jasné, jak bychom měli vyhodnocovat úspěšnost získaného porozumění. To je velký rozdíl oproti supervizovanému učení, kde je možné kvalitu naučeného modelu vyhodnocovat mnoha víceméně rovnocennými způsoby (např. přesností u klasifikace).

\end{itemize}

\subsection{Shluková analýza (Cluster Analysis)}

\begin{itemize}

\item Jednou ze základních metod nesupervizovaného učení je {\bf shlukování} nazývané také {\bf clusterování} (angl. clustering).

\item Cílem je roztřídit data do skupin, které se nazývají {\bf shluky}, tak, že platí dva přirozené požadavky:
\begin{itemize}
\item Blízké body budou ve stejném shluku.
\item Vzdálené body budou v různých shlucích.
\end{itemize}

\item Tento popis je ovšem hodně vágní a není vůbec jasné, jak ho zformalizovat. Jedním z problémů je fakt, že tyto intuitivní požadavky obecně nejsou kompatibilní.

\item Dalším problémem je již dříve zmiňovaná neexistence hodnotícího kritéria kvality nalezeného shlukování.

\item V důsledku absence jednoznačnosti tudíž existuje více způsobů formalizace a následně také mnoho různých shlukovacích algoritmů, které mohou na stejných datech dávat velmi rozdílné výsledky.

\end{itemize}

\subsubsection{Vzdálenost (metrika)}

\begin{itemize}

\item Klíčovým pojmem, na kterém stojí shlukování, je {\bf vzdálenost}.

\item {\bf Definice:} Vzdálenost nebo také {\bf metrika} na množině $\mathcal{X}$ je funkce $d : \mathcal{X} \times \mathcal{X} \to [0, +\infty)$ taková, že pro každé $\mathbf{x}, \mathbf{y}, \mathbf{z} \in \mathcal{X}$ platí:
\begin{enumerate}
\item $d(\mathbf{x}, \mathbf{y}) \geq 0$, a $d(\mathbf{x}, \mathbf{y}) = 0$ právě tehdy když $\mathbf{x} = \mathbf{y}$ — {\bf pozitivní definitnost},
\item $d(\mathbf{x}, \mathbf{y}) = d(\mathbf{y}, \mathbf{x})$ — {\bf symetrie},
\item $d(\mathbf{x}, \mathbf{y}) \leq d(\mathbf{x}, \mathbf{z}) + d(\mathbf{z}, \mathbf{y})$ — {\bf trojúhelníková nerovnost}.
\end{enumerate}

\item Dvojice $(\mathcal{X}, d)$ se potom nazývá {\bf metrický prostor}.

\item {\bf Poznámky k vlastnostem vzdálenosti:}
\begin{itemize}
\item Vzdálenost různých bodů je vždy kladná. Nulová je pouze pro stejné body.
\item Vzdálenost bodu $\mathbf{x}$ od bodu $\mathbf{y}$ je stejná jako vzdálenost $\mathbf{y}$ od $\mathbf{x}$.
\item Přímá vzdálenost mezi dvěma body je vždy menší nebo rovna vzdálenosti přes nějaký třetí bod.
\end{itemize}

\end{itemize}

\subsubsection{Příklady vzdáleností na $\mathbb{R}^p$}

\begin{itemize}

\item {\bf Euklidovská vzdálenost} (nebo také $L_2$ vzdálenost):
$$d_2(\mathbf{x}, \mathbf{y}) = \sqrt{\sum_{i=1}^{p} (x_i - y_i)^2}$$

\item {\bf Manhattanská vzdálenost} (nebo také $L_1$ vzdálenost):
$$d_1(\mathbf{x}, \mathbf{y}) = \sum_{i=1}^{p} |x_i - y_i|$$

Manhattanská vzdálenost je pojmenována podle ulice Manhattanu v New Yorku, kde je uliční síť pravidelná čtvercová mřížka.

\begin{center}
\includegraphics[width=0.55\textwidth]{assets/NY.png}
\end{center}
\begin{center}
\text{Obrázek 19: Mapa Manhattanu v New Yorku připomíná čtvercovou mřížku. Zdroj: Apple Maps}
\end{center}
\vspace{0.3cm}

\begin{center}
\includegraphics[width=0.55\textwidth]{assets/ctvercova_mrizka_norma.png}
\end{center}
\begin{center}
\text{Obrázek 20: Ve čtvercové mřížce má každá cesta z počátku do $\mathbf{x}$ délku alespoň $\|\mathbf{x}\|_1$. Zde $\|(5,4)\|_1 = 9$.}
\end{center}
\vspace{0.3cm}

\item {\bf Čebyševova vzdálenost} (nebo také $L_\infty$ vzdálenost):
$$d_\infty(\mathbf{x}, \mathbf{y}) = \max_{i} |x_i - y_i|$$

\item Příkladem vzdálenosti na množině řetězců je {\bf Levenštejnova vzdálenost} (angl. Levenshtein distance) definovaná jako minimální počet jednoznakových operací (vkládání, mazání, nahrazení), které převedou jeden řetězec na druhý.

\end{itemize}

\subsubsection{Vstupy a výstupy shlukování}

\begin{itemize}

\item {\bf Vstupy:}
\begin{itemize}
\item Metrický prostor $\mathcal{X}$ se vzdáleností $d$.
\item Množina dat $\mathcal{D} \subset \mathcal{X}$.
\item Obvykle také požadovaný počet shluků $k$.
\end{itemize}

\item {\bf Výstupy:}
\begin{itemize}
\item Rozklad množiny dat na jednotlivé shluky. To jest $\mathcal{C} = (C_1, \ldots, C_k)$, kde $C_i \subset \mathcal{D}$ pro každé $i$ a $C_i \cap C_j = \emptyset$ pro každé $i \neq j$, přičemž:
$$\mathcal{D} = \bigcup_{i=1}^{k} C_i$$
\item Bod $\mathbf{x} \in \mathcal{D}$ je tedy v $i$-tém shluku, jestliže $\mathbf{x} \in C_i$.
\item U hierarchického shlukování může být finálním výstupem {\bf dendrogram} jakožto grafické znázornění hierarchické struktury shlukování.
\end{itemize}

\end{itemize}

\subsection{Hierarchické shlukování}

\begin{itemize}

\item {\bf Aglomerativní hierarchické shlukování} je přirozený hladový přístup. Označme $N$ počet prvků množiny dat $\mathcal{D}$.

\item {\bf Algoritmus:}
\begin{enumerate}
\item Na začátku uvažujeme každý bod jako jeden shluk. Tj. máme právě $N$ shluků.
\item Nyní opakujeme následující kroky:
\begin{itemize}
\item Najdeme dva shluky, které jsou k sobě nejblíže.
\item Tyto dva shluky spojíme do nového shluku.
\end{itemize}
\item Po $N - 1$ opakováních skončíme s jediným velkým shlukem, ve kterém jsou všechny body.
\end{enumerate}

\item K tomu, aby bylo možné tento postup provést, je třeba stanovit způsob {\bf měření vzdálenosti dvou shluků}.

\item Má-li být navíc výstupem shlukování, je nutné určit nějaké {\bf zastavovací kritérium}. To bývá nejčastěji počet shluků $k$, případně limitní hodnota vzdálenosti shluků, nad kterou už nebudeme shluky spojovat.

\end{itemize}

\subsubsection{Měření vzdálenosti shluků}

\begin{itemize}

\item Vstupem do shlukování je vzdálenost dvojice bodů $d(\mathbf{x}, \mathbf{y})$. K měření vzdálenosti dvojic shluků $D(A, B)$ se obvykle používá jeden z následujících způsobů:

\item {\bf Metoda nejbližšího souseda} (angl. single linkage) — generuje dlouhé řetězce:
$$D(A, B) = \min_{\mathbf{x} \in A, \mathbf{y} \in B} d(\mathbf{x}, \mathbf{y})$$

\item {\bf Metoda nejvzdálenějšího souseda} (angl. complete linkage) — generuje kompaktní shluky:
$$D(A, B) = \max_{\mathbf{x} \in A, \mathbf{y} \in B} d(\mathbf{x}, \mathbf{y})$$

\item {\bf Párová vzdálenost} (angl. average linkage) — kompromis předchozích dvou:
$$D(A, B) = \frac{1}{|A||B|} \sum_{\mathbf{x} \in A, \mathbf{y} \in B} d(\mathbf{x}, \mathbf{y})$$

\item {\bf Wardova metoda} — v $\mathbb{R}^p$ velmi účinná, minimalizuje nárůst vnitřního rozptylu:
$$D(A, B) = \sum_{\mathbf{x} \in A \cup B} \|\mathbf{x} - \bar{\mathbf{x}}_{A \cup B}\|^2 - \sum_{\mathbf{x} \in A} \|\mathbf{x} - \bar{\mathbf{x}}_A\|^2 - \sum_{\mathbf{x} \in B} \|\mathbf{x} - \bar{\mathbf{x}}_B\|^2$$
kde $\bar{\mathbf{x}}_A = \frac{1}{|A|} \sum_{\mathbf{x} \in A} \mathbf{x}$ je geometrický střed množiny $A$ a $\bar{\mathbf{x}}_B$, $\bar{\mathbf{x}}_{A \cup B}$ analogicky.

\end{itemize}

\subsubsection{Vizualizace pomocí dendrogramu}

\begin{itemize}

\item Celý proces aglomerativního shlukování je možné reprezentovat a graficky znázornit pomocí {\bf dendrogramu}.

\item Jedná se o strom, jehož vrcholy představují shluky, které při běhu vznikly.

\item V listech jsou počáteční jednoprvkové shluky a kořen reprezentuje finální shluk všech bodů.

\item Strom je navíc nakreslený tak, že jsou všechny listy ve výšce 0 a výška ostatních vrcholů odpovídá vzdálenosti podřazených shluků, které se v tomto vrcholu spojily.

\begin{center}
\includegraphics[width=0.8\textwidth]{assets/rozlozeni_bodu_a_single_linkage_dendrogram.png}
\end{center}
\begin{center}
\text{Obrázek 21: Rozložení bodů a odpovídající dendrogram (single linkage)}
\end{center}
\vspace{0.3cm}

\end{itemize}

\subsubsection{Získání shlukování z dendrogramu}

\begin{itemize}

\item Známe-li požadovaný počet $k$ shluků, „rozřízneme" dendrogram mezi $k$-tým a $(k-1)$-tým nejvyšším vrcholem. Hrany procházející říznutím pak odpovídají výsledným shlukům.

\item Nebo je možné stanovit limitní hranici vzdálenosti shluků a rozříznout dendrogram v dané výšce.

\item Případně lze určovat místo rozříznutí a tím potažmo i počet shluků pomocí rozdílů výšek sousedních vrcholů.

\begin{center}
\includegraphics[width=0.8\textwidth]{assets/rez_dendrogramu_usprostred_nejvyssiho_rozdilu_vysek_a_vysledne_shlukovani.png}
\end{center}
\begin{center}
\text{Obrázek 22: Řez dendrogramu uprostřed nejvyššího rozdílu výšek a výsledné shlukování}
\end{center}
\vspace{0.3cm}

\end{itemize}

\subsubsection{Poznámky k hierarchickému shlukování}

\begin{itemize}

\item Ukázali jsme si {\bf aglomerativní} hierarchické shlukování.

\item Existuje také {\bf divizní} hierarchické shlukování, při kterém naopak vycházíme z jednoho shluku a opakovaně provádíme dělení.

\item {\bf Výhody hierarchického shlukování:}
\begin{itemize}
\item Možnost získat ucelený pohled pomocí dendrogramu a teprve na jeho základě vybírat vhodný počet shluků.
\item Hierarchická struktura — při zvýšení počtu shluků o jedničku dojde pouze k rozdělení některého ze stávajících, přičemž ostatní se nemění.
\item Kvalita vzniklého dendrogramu z pohledu zachování párových vzdáleností originálních dat může například být měřena pomocí {\bf cophenetic correlation}.
\end{itemize}

\item {\bf Nevýhody — výpočetní náročnost:}
\begin{itemize}
\item Hierarchické shlukování se příliš nehodí pro velké datové soubory, protože je výpočetně náročné.
\item V jednom kroku aglomerativního algoritmu je nutné provést $m(m-1)/2$ porovnání, kde $m$ je aktuální počet shluků. Složitost je tedy $\mathcal{O}(N^3)$. V případě single nebo complete linkage je možné se dostat na $\mathcal{O}(N^2)$.
\item V jednom kroku divizního algoritmu je nutné provést $2^{m-1}$ porovnání, kde $m$ je velikost děleného shluku, což implikuje složitost $\mathcal{O}(2^N)$.
\end{itemize}
\end{itemize}


\section{Shlukování pomocí algoritmu k-means}
\label{sec:kmeans}

\begin{shaded}
\noindent\textbf{Otázka ke zkoušce 15 (Shlukování pomocí algoritmu k-means):} 
Cíle nesupervizovaného učení a shlukování. Shlukování jako optimalizační úloha, algoritmus k-means a jeho účelová funkce. Vyhodnocování pomocí Silhouette skóre.
\end{shaded}

\subsection{Nesupervizované učení a shluková analýza}

\begin{itemize}

\item {\bf Nesupervizované učení} (angl. unsupervised learning) nastává v situaci, kdy data nemáme nikterak označena. Tj. nemáme žádnou veličinu, kterou bychom u trénovacích dat znali a snažili se ji naučit predikovat. Cílem je porozumět struktuře dat pouze na základě nich samotných.

\item {\bf Shlukování} (angl. clustering) je jednou ze základních metod nesupervizovaného učení. Cílem je roztřídit data do skupin nazývaných {\bf shluky} tak, že platí dva přirozené požadavky:
\begin{itemize}
\item Blízké body budou ve stejném shluku.
\item Vzdálené body budou v různých shlucích.
\end{itemize}

\item {\bf Problém:} Tyto intuitivní požadavky obecně nejsou kompatibilní a neexistuje univerzální hodnotící kritérium kvality shlukování.

\end{itemize}

\subsection{Shlukování jako optimalizační úloha}

\begin{itemize}

\item Oblíbeným přístupem ke shlukování je formulace ve tvaru {\bf optimalizační úlohy}.

\item Je třeba definovat {\bf účelovou funkci} (angl. objective function), která ohodnocuje daný rozklad množiny na shluky. Cílem je nalézt rozklad, který účelovou funkci minimalizuje.

\item Pro dané $k$ hledáme takový rozklad $\mathcal{C} = (C_1, \ldots, C_k)$ na prostoru $\mathcal{X} = \mathbb{R}^p$ vybaveném Euklidovskou vzdáleností, který minimalizuje účelovou funkci:
$$G(\mathcal{C}) = \sum_{i=1}^{k} \frac{1}{2|C_i|} \sum_{\mathbf{x}, \mathbf{y} \in C_i} \|\mathbf{x} - \mathbf{y}\|^2$$

\item V účelové funkci se pro každý shluk sečtou průměrné kvadráty vzdáleností všech bodů daného shluku od jeho ostatních bodů.

\end{itemize}

\subsection{Souvislost účelové funkce s geometrickým středem}

\begin{itemize}

\item {\bf Tvrzení:} Pro konečnou množinu bodů $A \subset \mathbb{R}^p$ platí:
$$\frac{1}{2|A|} \sum_{\mathbf{x}, \mathbf{y} \in A} \|\mathbf{x} - \mathbf{y}\|^2 = \sum_{\mathbf{x} \in A} \|\mathbf{x} - \bar{\mathbf{x}}\|^2 = \min_{\boldsymbol{\mu} \in \mathbb{R}^p} \sum_{\mathbf{x} \in A} \|\mathbf{x} - \boldsymbol{\mu}\|^2,$$
kde $\bar{\mathbf{x}} = \frac{1}{|A|} \sum_{\mathbf{x} \in A} \mathbf{x}$ je {\bf geometrický střed} (angl. centroid) množiny $A$.

\item {\bf Důkaz:} Pro každé $\mathbf{a}, \mathbf{b} \in \mathbb{R}^p$ platí $\|\mathbf{a} - \mathbf{b}\|^2 = (\mathbf{a} - \mathbf{b})^T(\mathbf{a} - \mathbf{b}) = \|\mathbf{a}\|^2 - 2\mathbf{a}^T\mathbf{b} + \|\mathbf{b}\|^2$, protože $\mathbf{b}^T\mathbf{a} = \mathbf{a}^T\mathbf{b}$. Pro $\mathbf{a} = \mathbf{x} - \boldsymbol{\mu}$ a $\mathbf{b} = \mathbf{y} - \boldsymbol{\mu}$ z toho plyne:
$$\frac{1}{2|A|} \sum_{\mathbf{x}, \mathbf{y} \in A} \|\mathbf{x} - \mathbf{y}\|^2 = \frac{1}{2|A|} \sum_{\mathbf{x}, \mathbf{y} \in A} \|\mathbf{x} - \boldsymbol{\mu}\|^2 + \frac{1}{2|A|} \sum_{\mathbf{x}, \mathbf{y} \in A} \|\mathbf{y} - \boldsymbol{\mu}\|^2 - \frac{1}{|A|} \sum_{\mathbf{x}, \mathbf{y} \in A} (\mathbf{x} - \boldsymbol{\mu})^T(\mathbf{y} - \boldsymbol{\mu}).$$

Poslední člen upravíme:
$$\frac{1}{|A|} \sum_{\mathbf{x}, \mathbf{y} \in A} (\mathbf{x} - \boldsymbol{\mu})^T(\mathbf{y} - \boldsymbol{\mu}) = \frac{1}{|A|} \sum_{\mathbf{x} \in A} (\mathbf{x} - \boldsymbol{\mu})^T \sum_{\mathbf{y} \in A} (\mathbf{y} - \boldsymbol{\mu}) = \frac{1}{|A|} \left\| \sum_{\mathbf{x} \in A} (\mathbf{x} - \boldsymbol{\mu}) \right\|^2.$$

Poslední člen je tedy vždy nezáporný. Navíc je rovný nule právě, když $\boldsymbol{\mu} = \bar{\mathbf{x}}$, což plyne z definice $\bar{\mathbf{x}}$. Prohodíme-li ve druhém členu $\mathbf{x}$ a $\mathbf{y}$, dostaneme první člen, a po vysčítání přes $\mathbf{y}$ dostaneme:
$$\frac{1}{2|A|} \sum_{\mathbf{x}, \mathbf{y} \in A} \|\mathbf{x} - \mathbf{y}\|^2 \leq \sum_{\mathbf{x} \in A} \|\mathbf{x} - \boldsymbol{\mu}\|^2, \quad \text{s rovností pouze pokud } \boldsymbol{\mu} = \bar{\mathbf{x}}. \quad \square$$

\item Na základě tohoto tvrzení můžeme účelovou funkci vyjádřit jako:
$$G(\mathcal{C}) = \sum_{i=1}^{k} \sum_{\mathbf{x} \in C_i} \|\mathbf{x} - \bar{\mathbf{x}}_i\|^2,$$
kde $\bar{\mathbf{x}}_i$ je geometrický střed $i$-tého shluku.

\end{itemize}

\subsection{Algoritmus k-means}

\begin{itemize}

\item {\bf Poznámka (NP-těžkost):} Problém nalezení globálního minima účelové funkce je výpočetně obtížný — je {\bf NP-těžký} [Aloise et al. (2009)]. NP-těžký problém je takový, pro který neexistuje známý polynomiální algoritmus a předpokládá se, že ani existovat nemůže. V praxi to znamená, že pro velké instance nelze nalézt optimální řešení v rozumném čase.

\item {\bf Algoritmus k-means} je heuristický iterativní algoritmus, který v každém kroku zmenšuje hodnotu účelové funkce a konverguje tak k jejímu {\bf lokálnímu minimu}.

\item {\bf Slovní popis algoritmu:}
\begin{enumerate}
\item Nejprve zvolíme $k$ středových bodů.
\item Iterativně opakujeme:
\begin{itemize}
\item[(i)] Vytvoříme shluky odpovídající středovým bodům tak, že pro každý bod $\mathbf{x}$ najdeme k němu nejbližší středový bod a podle něj $\mathbf{x}$ zařadíme do shluku.
\item[(ii)] Spočítáme nové středové body jako geometrické středy těchto shluků.
\end{itemize}
\end{enumerate}

\item {\bf Formalizace algoritmu k-means:}

\begin{itemize}
\item {\bf Inicializace:} Počáteční rozmístění $k$ středových bodů $\boldsymbol{\mu}_1, \ldots, \boldsymbol{\mu}_k$.

\item {\bf Iterativní část:}
\begin{itemize}
\item[(i)] Roztřídíme body do shluků: $C_i = \{\mathbf{x} \in \mathcal{D} \mid i = \arg\min_j \|\mathbf{x} - \boldsymbol{\mu}_j\|\}$.
\item[(ii)] Přepočítáme body $\boldsymbol{\mu}_1, \ldots, \boldsymbol{\mu}_k$ jako geometrické středy těchto shluků: $\boldsymbol{\mu}_i \leftarrow \frac{1}{|C_i|} \sum_{\mathbf{x} \in C_i} \mathbf{x}$.
\end{itemize}
\end{itemize}

\item Běh algoritmu zastavíme, jakmile je změna hodnoty účelové funkce mezi jednotlivými iteracemi dostatečně malá.

\item Výsledek algoritmu významně závisí na {\bf inicializační části}. Počáteční středové body jsou obvykle generovány náhodně (např. náhodným výběrem z dat). Existují i ``chytřejší metody'' jako např. {\bf k-means++}.

\item Algoritmus je následně opakovaně spouštěn. Jako finální pak bereme výsledek běhu s nejnižší hodnotou účelové funkce.

\end{itemize}

\subsection{Důkaz lokální optimalizace}

\begin{itemize}

\item Zafixujme $\boldsymbol{\mu}_i = \bar{\mathbf{x}}_i$. Vytvořme nové shluky $\tilde{\mathcal{C}} = \{\tilde{C}_1, \ldots, \tilde{C}_k\}$ tak, že bod $\mathbf{x}$ přesuneme do takového shluku $\tilde{C}_i$, ve kterém je vzdálenost $\|\mathbf{x} - \boldsymbol{\mu}_i\|$ nejmenší.

\item Tím zcela jistě dojde ke zmenšení součtů kvadrátů vzdáleností:
$$\sum_{i=1}^{k} \sum_{\mathbf{x} \in \tilde{C}_i} \|\mathbf{x} - \boldsymbol{\mu}_i\|^2 \leq \sum_{i=1}^{k} \sum_{\mathbf{x} \in C_i} \|\mathbf{x} - \boldsymbol{\mu}_i\|^2.$$

\item Z druhé rovnosti předchozího tvrzení pak plyne:
$$G(\tilde{\mathcal{C}}) = \sum_{i=1}^{k} \sum_{\mathbf{x} \in \tilde{C}_i} \|\mathbf{x} - \bar{\tilde{\mathbf{x}}}_i\|^2 \leq \sum_{i=1}^{k} \sum_{\mathbf{x} \in \tilde{C}_i} \|\mathbf{x} - \boldsymbol{\mu}_i\|^2,$$
kde $\bar{\tilde{\mathbf{x}}}_i$ je geometrický střed shluku $\tilde{C}_i$.

\item Dohromady tedy máme $G(\tilde{\mathcal{C}}) \leq G(\mathcal{C})$. Tento postup můžeme opakovat a postupně tak klesat k nějakému lokálnímu minimu účelové funkce.

\end{itemize}

\subsection{Ukázka výsledku algoritmu k-means}

\begin{center}
\includegraphics[width=0.8\textwidth]{assets/vysledne_shlukovani_v_vcetne_geometrickych_stred_shluku.png}
\end{center}
\begin{center}
\text{Obrázek 23: Výsledné shlukování včetně geometrických středů shluků}
\end{center}

\subsection{Problematika volby $k$}

\begin{itemize}

\item Na rozdíl od hierarchického shlukování, kde lze počet shluků určovat až na základě dendrogramu, je u algoritmu k-means nezbytné stanovit $k$ {\bf dopředu}.

\item Bohužel neexistuje žádný univerzální způsob, jak počet shluků určit automaticky.

\item {\bf Metoda lokte} (angl. elbow method):
\begin{itemize}
\item Je-li $k^*$ optimální počet shluků, lze očekávat, že pro $k < k^*$ bude účelová funkce s měnícím se $k$ klesat hodně. Pro $k \geq k^*$ lze naopak očekávat zmenšení poklesu účelové funkce.
\item Optimální $k$ tedy můžeme detekovat jako hodnotu, pro kterou se mění pokles účelové funkce z hodně prudkého na méně prudký — hledat tzv. {\bf loket}.
\item Je to ale hodně subjektivní a pro některá data v podstatě nepoužitelné.
\end{itemize}

\begin{center}
\includegraphics[width=0.8\textwidth]{assets/ucelova_funkce.png}
\label{fig:ucelova_funkce}
\end{center}
\begin{center}
\text{Obrázek 24: Metoda lokte — závislost účelové funkce na $k$}
\end{center}
\vspace{0.3cm}

Na \hyperref[fig:ucelova_funkce]{obrázku 24} bychom nejspíš určili jako optimální $k = 2$. Přitom ve skutečnosti byla data vygenerována jako směs tří různých dvourozměrných normálních rozdělení.

\end{itemize}

\subsection{Vyhodnocování pomocí Silhouette skóre}

\begin{itemize}

\item {\bf Silhouette} [Rousseeuw (1987)] je metoda pro evaluaci shlukování, kterou lze využít i k určení optimálního počtu shluků.

\item Uvažujme shlukování $\mathcal{D} = C_1 \cup \ldots \cup C_k$ na metrickém prostoru $\mathcal{X}$ s metrikou $d(\mathbf{x}, \mathbf{y})$ a pro libovolný bod $\mathbf{x} \in \mathcal{D}$ označme $j(\mathbf{x})$ index shluku, do kterého $\mathbf{x}$ patří.

\item {\bf Výpočet Silhouette skóre pro bod $\mathbf{x}$:}

\begin{itemize}
\item {\bf Vnitřní rozdílnost} (angl. within dissimilarity) — průměrná vzdálenost bodu $\mathbf{x}$ od všech ostatních bodů ve stejném shluku:
$$a(\mathbf{x}) = \frac{1}{|C_{j(\mathbf{x})}| - 1} \sum_{\mathbf{y} \in C_{j(\mathbf{x})}, \mathbf{y} \neq \mathbf{x}} d(\mathbf{x}, \mathbf{y})$$

\item Pro každý další shluk $C_i$, $i \neq j(\mathbf{x})$ spočteme průměrnou vzdálenost:
$$d(\mathbf{x}, C_i) = \frac{1}{|C_i|} \sum_{\mathbf{y} \in C_i} d(\mathbf{x}, \mathbf{y})$$

\item {\bf Sousední rozdílnost} (angl. between dissimilarity) — minimum z průměrných vzdáleností od ostatních shluků:
$$b(\mathbf{x}) = \min_{i \neq j(\mathbf{x})} d(\mathbf{x}, C_i)$$
\end{itemize}

\item {\bf Silhouette skóre bodu $\mathbf{x}$:}
$$s(\mathbf{x}) = \frac{b(\mathbf{x}) - a(\mathbf{x})}{\max\{a(\mathbf{x}), b(\mathbf{x})\}}$$

Jestliže máme pouze jeden shluk, položíme $s(\mathbf{x}) = 0$.

\item {\bf Interpretace:} Vždy platí $-1 \leq s(\mathbf{x}) \leq 1$.
\begin{itemize}
\item $s(\mathbf{x}) \approx 1$: Vnitřní rozdílnost $a(\mathbf{x})$ je mnohem menší než sousední rozdílnost $b(\mathbf{x})$ — bod je dobře zatříděn.
\item $s(\mathbf{x}) \approx 0$: $a(\mathbf{x})$ je podobně velké jako $b(\mathbf{x})$ — bod je na okraji svého shluku.
\item $s(\mathbf{x}) \approx -1$: $a(\mathbf{x})$ je mnohem větší než $b(\mathbf{x})$ — bod je špatně přiřazen a měl by patřit do sousedního shluku.
\end{itemize}

\item {\bf Průměrné skóre pro shluk $C_i$:}
$$s_i = \frac{1}{|C_i|} \sum_{\mathbf{x} \in C_i} s(\mathbf{x})$$

\item {\bf Průměrné skóre pro celé shlukování:}
$$s = \frac{1}{|\mathcal{D}|} \sum_{\mathbf{x} \in \mathcal{D}} s(\mathbf{x})$$

\item Čím vyšší hodnota $s$, tím je celé shlukování lepší. Porovnání skóre pro různé počty shluků lze použít k nalezení optimálního $k$ jako hodnoty, při které je skóre $s$ {\bf maximální}.

\end{itemize}
%
%
%
\section{Shlukování pomocí algoritmu DBSCAN}
\label{sec:dbscan}

\begin{shaded}
\noindent\textbf{Otázka ke zkoušce 16 (Shlukování pomocí algoritmu DBSCAN):} 
Cíle nesupervizovaného učení a shlukování. Algoritmus DBSCAN.
\end{shaded}

\subsection{Úvod do shlukování pomocí hustoty}

\begin{itemize}

\item V této sekci se zaměříme na shlukování, které využívá odhad hustoty rozdělení dat na prostoru $\mathcal{X}$ možných hodnot příznaků.

\item Z pravděpodobnostního pohledu chápeme pozorovaná data jako realizace náhodného vektoru $\mathbf{X} = (X_1, \ldots, X_p)^T$.

\item Pokud budeme mít nějakým způsobem odhadnutou hustotu pravděpodobnosti $f_X(\mathbf{x}) \equiv f_X(x_1, \ldots, x_p)$ (resp. něco, co jí je úměrné), můžeme ji využít k získání shlukování včetně identifikace bodů, které do žádných shluků nepatří.

\item Shluky můžeme získat jako souvislé oblasti, ve kterých odhad hustoty překročí nějakou zvolenou hranici.

\item Body mimo tyto oblasti pak označíme jako {\bf šum} (angl. noise).

\begin{center}
\includegraphics[width=0.8\textwidth]{assets/02.png}
\end{center}
\begin{center}
\text{Obrázek 25: Shlukování pomocí hustoty — shluky jako oblasti s vysokou hustotou}
\end{center}

\end{itemize}

\subsection{DBSCAN: úvodní pojmy}

\begin{itemize}

\item Jedním z aktuálně nejpoužívanějších algoritmů shlukování, který je implicitně založen na principu odhadu hustoty, je algoritmus {\bf DBSCAN}, což je zkratka angl. {\bf density-based spatial clustering of applications with noise}, [Ester et al. (1996)].

\item Připravme si nyní základní pojmy, na kterých DBSCAN stojí.

\item Uvažujme metrický prostor $\mathcal{X}$ s metrikou $d(\mathbf{x}, \mathbf{y})$ ze kterého pochází dataset $\mathcal{D}$ a parametry $\varepsilon > 0$ a $\text{MinPts} \in \mathbb{N}^+$.

\item Definujme {\bf $\varepsilon$ okolí} bodu $\mathbf{x}$ v $\mathcal{D}$ (angl. $\varepsilon$-neighborhood) jako množinu:
$$N_\varepsilon(\mathbf{x}) = \{\mathbf{y} \in \mathcal{D} \mid d(\mathbf{x}, \mathbf{y}) \leq \varepsilon\}$$

\vspace{0.05cm}
\begin{center}
\includegraphics[width=0.45\textwidth]{assets/03.png}
\end{center}
\begin{center}
\text{Obrázek 26: Znázornění $\varepsilon$ okolí}
\end{center}

\end{itemize}

\subsection{DBSCAN: klíčové body, přímá dosažitelnost}

\begin{itemize}

\item Bod $\mathbf{x} \in \mathcal{D}$ je {\bf klíčový bod} (angl. core point), jestliže v jeho $\varepsilon$ okolí v $\mathcal{D}$ je alespoň MinPts bodů:
$$|N_\varepsilon(\mathbf{x})| \geq \text{MinPts}$$

\item Bod $\mathbf{y} \in \mathcal{D}$ je {\bf přímo dosažitelný} (angl. directly density-reachable) z bodu $\mathbf{x} \in \mathcal{D}$, jestliže $\mathbf{x}$ je klíčový bod a $\mathbf{y} \in N_\varepsilon(\mathbf{x})$.

\item Relace přímé dosažitelnosti je symetrická pro dvojici klíčových bodů, je ale nesymetrická pro tzv. {\bf okrajový bod} (angl. border point), což je bod, který není klíčový, ale je přímo dosažitelný z klíčového bodu.

\begin{center}
\includegraphics[width=0.55\textwidth]{assets/04.png}
\end{center}
\begin{center}
    \parbox{0.8\textwidth}{
        \centering 
        Obrázek 27: Pro MinPts $= 3$ je bod $\mathbf{y}$ přímo dosažitelný z $\mathbf{x}$. Všechny klíčové body jsou modré, všechny okrajové body jsou černé.
    }
\end{center}

\end{itemize}

\subsection{DBSCAN: dosažitelnost}

\begin{itemize}

\item Bod $\mathbf{y} \in \mathcal{D}$ je {\bf dosažitelný} (angl. density-reachable) z bodu $\mathbf{x} \in \mathcal{D}$, pokud existuje posloupnost $\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n \in \mathcal{D}$ bodů tak, že $\mathbf{x}_1 = \mathbf{x}$, $\mathbf{x}_n = \mathbf{y}$ a pro každé $i = 1, \ldots, n-1$ je $\mathbf{x}_{i+1}$ přímo dosažitelný z bodu $\mathbf{x}_i$.

\item Z toho plyne, že všechny body po cestě kromě posledního musí být klíčové body.

\begin{center}
\includegraphics[width=0.55\textwidth]{assets/05.png}
\end{center}
\begin{center}
\text{Obrázek 28: Pro MinPts $= 3$ je bod $\mathbf{y}$ dosažitelný z bodu $\mathbf{x}$}
\end{center}

\end{itemize}

\subsection{DBSCAN: spojenost}

\begin{itemize}

\item Bod $\mathbf{y} \in \mathcal{D}$ je {\bf spojený} (angl. density-connected) s bodem $\mathbf{x} \in \mathcal{D}$, jestliže existuje (klíčový bod) $\mathbf{p} \in \mathcal{D}$ tak, že $\mathbf{x}$ i $\mathbf{y}$ jsou dosažitelné z bodu $\mathbf{p}$.

\item Relace spojenosti je zjevně symetrická. Pro klíčové body je také tranzitivní.

\item Jestliže jsou dva body spojené a jeden z nich je klíčový bod, tak ten druhý je z toho prvního dosažitelný.

\begin{center}
\includegraphics[width=0.55\textwidth]{assets/06.png}
\end{center}
\begin{center}
\text{Obrázek 29: Pro MinPts $= 3$ je bod $\mathbf{y}$ spojený s bodem $\mathbf{x}$}
\end{center}

\end{itemize}

\subsection{DBSCAN: shluky a šum}

\begin{itemize}

\item Shluk nyní definujeme jako maximální množinu spojených bodů.

\item {\bf Definice:} {\bf Shluk} (angl. cluster) $C$ je podmnožina $\mathcal{D}$ obsahující alespoň jeden klíčový bod tak, že:
\begin{itemize}
\item Pro každé $\mathbf{x}, \mathbf{y} \in \mathcal{D}$ platí, že když $\mathbf{x} \in C$ a $\mathbf{y}$ je dosažitelný z $\mathbf{x}$, pak $\mathbf{y} \in C$ ({\bf maximalita}).
\item Pro každé $\mathbf{x}, \mathbf{y} \in C$ je $\mathbf{x}$ spojený s $\mathbf{y}$ ({\bf souvislost}).
\end{itemize}

\item Označme $C_1, \ldots, C_k$ množinu všech shluků v $\mathcal{D}$ (vzhledem k $\varepsilon$ a MinPts). Množinu $N$ bodů z $\mathcal{D}$, které nejsou v žádném ze shluků nazýváme {\bf šumem} (angl. noise):
$$N = \mathcal{D} \setminus \bigcup_{i=1}^{k} C_i$$


\hspace{0.3cm}
\begin{center}
\includegraphics[width=0.55\textwidth]{assets/07.png}
\end{center}
\begin{center}
\text{Obrázek 30: Body ve shluku jsou modré, bod šumu je šedivý}
\end{center}

\end{itemize}

\subsection{Poznámky k definici shluků}

\begin{itemize}

\item Shluk $C$ vždy obsahuje nějaký klíčový bod $\mathbf{p} \in C$, ze kterého jsou dosažitelné všechny body v jeho $\varepsilon$ okolí, kterých je alespoň MinPts. Právě jsme tedy ukázali, že {\bf každý shluk obsahuje alespoň MinPts bodů}.

\item Každý bod ve shluku $C$ je spojený se všemi klíčovými body v $C$ a tedy je z libovolného klíčového bodu dosažitelný. Shluk je tedy tvořen všemi body, které jsou dosažitelné z libovolného klíčového bodu v $C$.

\item To nám dává návod, jak může algoritmus tvorby shluků fungovat. Najdeme klíčový bod a vytvoříme k němu shluk jako množinu všech z něho dosažitelných bodů (které ještě nejsou v jiném shluku).

\end{itemize}

\subsection{Abstraktní popis algoritmu DBSCAN}

\begin{itemize}

\item {\bf Algoritmus DBSCAN:}

\begin{enumerate}
\item {\bf Nalezení klíčových bodů:} Spočítáme $\varepsilon$ okolí každého bodu a identifikujeme klíčové body.

\item {\bf Vytvoření zárodků shluků:} Spojíme sousední (přímo dosažitelné) klíčové body do shluků.

\item {\bf Pro každý bod, který není klíčový:}
\begin{itemize}
\item Přidáme do shluku podle klíčového bodu v jeho okolí.
\item Pokud takový neexistuje, přidáme mezi šum.
\end{itemize}
\end{enumerate}

\item {\bf Poznámky k algoritmu:}
\begin{itemize}
\item Okrajový bod, který má ve svém $\varepsilon$ okolí klíčové body z různých (zárodků) shluků spadne do prvního z těchto shluků, ke kterému se algoritmus dostane.
\item Finálním výsledkem v takovém případě budou shluky, které nesplňují podmínku maximality v předchozí definici, protože okrajový bod bude pouze v jednom shluku a nikoliv ve všech, kde by dle maximality měl být.
\item Lze ukázat ([Schubert et al. (2017)]), že složitost v nejhorším případě je $\mathcal{O}(n^2)$. V mnoha reálných situacích se ale lze dostat na $\mathcal{O}(n \log n)$.
\end{itemize}

\end{itemize}

\subsection{DBSCAN: volba parametrů}

\begin{itemize}

\item Zaměřme se nyní krátce na volbu parametrů algoritmu DBSCAN.

\item Ukazuje se, že parametr MinPts je mnohem méně důležitý než parametr $\varepsilon$. Obvykle dobrou volbou jsou hodnoty okolo $4-6$ (někdy bývá doporučováno $2 \cdot p$, kde $p$ je počet příznaků).

\item Pro parametr $\varepsilon$ bývá doporučováno volit co nejmenší hodnotu s tím, že lze například brát průměrnou vzdálenost bodů k jejich nejbližšímu $(2 \cdot p - 1)$-tému sousedovi.

\item Také lze sledovat velikost šumu. Uvádí se, že obvykle by poměr šumu měl být mezi 1\% a 30\%.

\item Dále je dobré sledovat velikost největšího shluku. Pokud jeho velikost překračuje 50\% velikosti datasetu, bývá vhodné zmenšit $\varepsilon$, případně použít nějaký pokročilejší algoritmus (např. HDBSCAN).

\item Více detailů k volbě parametrů najdete v [Schubert et al. (2017)] nebo v [Sander et al. (1998)].

\end{itemize}

\subsection{DBSCAN: ukázka a poznámky}

\begin{itemize}

\item Mezi hlavní {\bf výhody} DBSCAN (a obecně metod založených na hustotě) patří možnost nalezení {\bf nekonvexních shluků}.

\item Další výhodou je snížená citlivost na odlehlé hodnoty — které jsou označeny jako šum.

\begin{center}
\includegraphics[width=0.8\textwidth]{assets/08.png}
\end{center}
\begin{center}
\text{Obrázek 31: Znázornění shluků a šumu — DBSCAN dokáže nalézt nekonvexní shluky}
\end{center}

\end{itemize}

\subsection{Evaluace pomocí Silhouette skóre}

\begin{itemize}

\item {\bf Silhouette} [Rousseeuw, P. (1987)] je metoda pro evaluaci shlukování, kterou lze využít i k určení optimálního počtu shluků.

\item Uvažujme shlukování $\mathcal{D} = C_1 \cup \ldots \cup C_k$ na metrickém prostoru $\mathcal{X}$ s metrikou $d(\mathbf{x}, \mathbf{y})$ a pro libovolný bod $\mathbf{x} \in \mathcal{D}$ označme $j(\mathbf{x})$ index shluku, do kterého $\mathbf{x}$ patří, tj. $\mathbf{x} \in C_{j(\mathbf{x})}$.

\item {\bf Výpočet Silhouette skóre pro bod $\mathbf{x}$:}
\begin{itemize}
\item {\bf Vnitřní rozdílnost} (angl. within dissimilarity) — průměrná vzdálenost bodu $\mathbf{x}$ od všech ostatních bodů ve stejném shluku:
$$a(\mathbf{x}) = \frac{1}{|C_{j(\mathbf{x})}| - 1} \sum_{\mathbf{y} \in C_{j(\mathbf{x})}, \mathbf{y} \neq \mathbf{x}} d(\mathbf{x}, \mathbf{y})$$

\item Pro každý další shluk $C_i$, $i \neq j(\mathbf{x})$ spočteme průměrnou vzdálenost:
$$d(\mathbf{x}, C_i) = \frac{1}{|C_i|} \sum_{\mathbf{y} \in C_i} d(\mathbf{x}, \mathbf{y})$$

\item {\bf Sousední rozdílnost} (angl. between dissimilarity) — minimum z průměrných vzdáleností od ostatních shluků:
$$b(\mathbf{x}) = \min_{i \neq j(\mathbf{x})} d(\mathbf{x}, C_i)$$
\end{itemize}

\item {\bf Silhouette skóre bodu $\mathbf{x}$:}
$$s(\mathbf{x}) = \frac{b(\mathbf{x}) - a(\mathbf{x})}{\max\{a(\mathbf{x}), b(\mathbf{x})\}}$$
Jestliže máme pouze jeden shluk, položíme $s(\mathbf{x}) = 0$.

\item {\bf Interpretace:} Vždy platí $-1 \leq s(\mathbf{x}) \leq 1$.
\begin{itemize}
\item $s(\mathbf{x}) \approx 1$: Vnitřní rozdílnost $a(\mathbf{x})$ je mnohem menší než sousední rozdílnost $b(\mathbf{x})$ — bod je {\bf dobře zatříděn}.
\item $s(\mathbf{x}) \approx 0$: $a(\mathbf{x})$ je podobně velké jako $b(\mathbf{x})$ — bod je {\bf na okraji svého shluku} a sousední shluk je blízko.
\item $s(\mathbf{x}) \approx -1$: $a(\mathbf{x})$ je mnohem větší než $b(\mathbf{x})$ — bod je {\bf špatně přiřazen} a měl by spíše patřit do sousedního shluku.
\end{itemize}

\item {\bf Průměrné skóre pro shluk $C_i$:}
$$s_i = \frac{1}{|C_i|} \sum_{\mathbf{x} \in C_i} s(\mathbf{x})$$

\item {\bf Průměrné skóre pro celé shlukování:}
$$s = \frac{1}{|\mathcal{D}|} \sum_{\mathbf{x} \in \mathcal{D}} s(\mathbf{x})$$

\item Čím vyšší hodnotu $s$ dostaneme, tím jsou body ve shlucích správněji umístěné a tedy je celé shlukování lepší.

\item Porovnání skóre pro různé počty shluků lze použít k nalezení optimálního $k$ jako hodnoty, při které je skóre $s$ {\bf maximální}.

\begin{center}
\includegraphics[width=0.8\textwidth]{assets/09.png}
\end{center}
\begin{center}
\text{Obrázek 32: Závislost průměrného silhouette skóre na počtu shluků $k$}
\end{center}

\begin{center}
\includegraphics[width=0.8\textwidth]{assets/01.png}
\end{center}
\begin{center}
\text{Obrázek 33: Silhouette analýza pro algoritmus k-means s $k = 2, 3, 4$}
\end{center}

\end{itemize}


\section{Nesupervizované učení, asociační pravidla}
\label{sec:asociacni_pravidla}

\begin{shaded}
\noindent\textbf{Otázka ke zkoušce 17 (Nesupervizované učení, asociační pravidla):} 
Cíle nesupervizovaného učení. Asociační pravidla.
\end{shaded}

\subsection{Nesupervizované učení (Unsupervised Learning)}

\begin{itemize}

\item {\bf Nesupervizované učení} (angl. unsupervised learning) nastává v situaci, kdy data nemáme nikterak označena. Tj. nemáme žádnou veličinu, kterou bychom u trénovacích dat znali a snažili se ji naučit predikovat.

\item Cílem nesupervizovaného učení je porozumět struktuře dat pouze na základě nich samotných. To znamená bez nějakého vnějšího vodítka. Proto se nesupervizovanému učení také říká {\bf učení bez učitele}.

\item Porozuměním zde typicky rozumíme nalezení co „nejmenších" oblastí v prostoru příznaků, kde se data vyskytují nejčastěji.

\item Obvykle totiž platí, že se naměřená skutečná data nevyskytují v celém prostoru stejně pravděpodobně, ale bývají více či méně lokalizována — tvoří nějaké shluky, vyskytují se v méně-dimenzionálních oblastech atd.

\item Porozumění této lokalizaci přináší důležitou informaci o vnitřní struktuře dat!

\item {\bf Obecný problém nesupervizovaného učení:} Vůbec není jasné, jak bychom měli vyhodnocovat úspěšnost získaného porozumění. To je velký rozdíl oproti supervizovanému učení, kde je možné kvalitu naučeného modelu vyhodnocovat mnoha víceméně rovnocennými způsoby (např. přesností u klasifikace).

\end{itemize}

\subsection{Analýza asociačních pravidel}

\begin{itemize}

\item {\bf Analýza asociačních pravidel} je jedna ze standardních a oblíbených metod pro dobývání znalostí z komerčních databází.

\item Obecným cílem je nalézt společné hodnoty příznaků $\mathbf{X} = (X_1, \ldots, X_p)^T$, které se v databázi nejčastěji vyskytují.

\item V zásadě jde přesně o jeden z hlavních cílů nesupervizovaného učení, kdy chceme nalézt oblasti prostoru, kde se data vyskytují s velkou pravděpodobností.

\item V nejčastěji používaném zjednodušení se zabýváme pouze oblastmi, které jsou ve tvaru kartézského součinu pro jednotlivé příznaky, tj. chceme, aby pravděpodobnost:
$$P\left(\bigcap_{j=1}^{p} (X_j \in s_j)\right),$$
kde $s_j$ je podmnožina hodnot příznaku $X_j$, byla relativně velká.

\item Průnik $\bigcap_{j=1}^{p} (X_j \in s_j)$ se v takovém případě nazývá {\bf konjunktivní pravidlo} (angl. conjunctive rule).

\end{itemize}

\subsection{Analýza nákupního košíku}

\begin{itemize}

\item {\bf Analýza nákupního košíku} (angl. market basket analysis) je nejčastěji aplikována v případě binárních příznaků, $X_j \in \{0, 1\}$.

\item Pro oblast, kterou hledáme ve tvaru kartézského součinu se v tomto případě používá ještě další omezení, a to, že $s_j$ je buď jednoprvková množina $\{1\}$ nebo všechny možnosti daného příznaku $X_j$ (v tu chvíli příznak vypadne).

\item Ekvivalentně tedy hledáme množinu indexů $\mathcal{K} \subset \{1, \ldots, p\}$ tak, že:
$$P\left(\bigcap_{j \in \mathcal{K}} (X_j = 1)\right) = P\left(\prod_{j \in \mathcal{K}} X_j = 1\right)$$
je relativně velká.

\item Množina $\mathcal{K}$ se pak nazývá {\bf množina položek} (angl. item set).

\item Relativní velikost položek v datasetu, které danou množinu položek obsahují se značí $T(\mathcal{K})$ a nazývá {\bf podpora} (angl. support) množiny položek $\mathcal{K}$ a odpovídá odhadu výše uvedené pravděpodobnosti:
$$T(\mathcal{K}) = \hat{P}\left(\bigcap_{j \in \mathcal{K}} (X_j = 1)\right) = \frac{1}{N} \sum_{i=1}^{N} \prod_{j \in \mathcal{K}} x_{i;j}$$

\end{itemize}

\subsection{Asociační pravidla: definice}

\begin{itemize}

\item Při provádění analýzy nákupního košíku hledáme všechny množiny položek, pro které je podpora větší než nějaká zvolená mez $t$:
$$\{\mathcal{K}_\ell \mid T(\mathcal{K}_\ell) > t\}$$

\item K nalezení řešení se používá efektivní algoritmus (případně jeho novější vylepšení), který se nazývá {\bf Apriori algoritmus} [Agrawal, Srikant (1994)].

\item Pro každou množinu položek $\mathcal{K}$, kterou takto získáme, dále hledáme vhodné rozložení na dvě disjunktní podmnožiny $A$ a $B$, $A \cup B = \mathcal{K}$, které budeme nazývat {\bf asociační pravidlo} (angl. association rule) a značit:
$$A \Rightarrow B$$

\item První položka $A$ asociačního pravidla se nazývá {\bf předpoklad} (angl. antecedent) a druhá položka $B$ se nazývá {\bf závěr} (angl. consequent).

\item {\bf Podpora} $T(A \Rightarrow B)$ pravidla $A \Rightarrow B$ je definována jako podpora sjednocení $\mathcal{K} = A \cup B$.

\item {\bf Spolehlivost} (angl. confidence) $C(A \Rightarrow B)$ pravidla $A \Rightarrow B$ je definována jako podpora pravidla podělená podporou předpokladu $A$:
$$C(A \Rightarrow B) = \frac{T(A \Rightarrow B)}{T(A)},$$
což odpovídá odhadu podmíněné pravděpodobnosti $P(B \mid A)$.

\end{itemize}

\subsection{Asociační pravidla: další míry}

\begin{itemize}

\item Asociační pravidla jsou volena tak, aby spolehlivost byla větší než nějaká zvolená mez $c$.

\item Finálním výstupem asociační analýzy pravidel je množina asociačních pravidel, které splňují:
$$T(A \Rightarrow B) > t \quad \text{a} \quad C(A \Rightarrow B) > c$$

\item U nalezených asociačních pravidel se dále často měří {\bf zdvih} (angl. lift) definovaný jako:
$$L(A \Rightarrow B) = \frac{C(A \Rightarrow B)}{T(B)},$$
který odpovídá odhadu podílu $\frac{P(B \mid A)}{P(B)}$, což znamená, kolikanásobně je větší $P(B \mid A)$ oproti $P(B)$.

\item Někdy se také měří {\bf pokrytí} (angl. coverage) definované jako:
$$\text{Coverage}(A \Rightarrow B) = \frac{T(A \Rightarrow B)}{T(B)},$$
což odpovídá odhadu podmíněné pravděpodobnosti $P(A \mid B)$.

\item Pokrytí tedy indikuje, jak často lze závěr vyložit coby důsledek předpokladu.

\end{itemize}

\subsection{Asociační pravidla: příklady}

\begin{itemize}

\item Klasickým příkladem asociačního pravidla je:
$$\{\texttt{párky}\} \Rightarrow \{\texttt{hořčice}, \texttt{chléb}\}$$

\item {\bf Podpora} 0.06 znamená, že v celém datasetu se trojice položek $\{\texttt{párky}, \texttt{hořčice}, \texttt{chléb}\}$ vyskytuje v 6\% případů.

\item Pokud se párky vyskytují v datasetu v 8\% případů, bude {\bf spolehlivost} $0.06/0.08 = 0.75$, což znamená, že když si zákazník koupil párky, v 75\% případů si koupil také hořčici a chléb.

\item Pokud je dvojice hořčice a chléb v datasetu v 15\% případů, {\bf zdvih} bude $0.75/0.15 = 5$.

\item {\bf Pokrytí} v takovém případě bude $0.06/0.15 = 0.4$. Čili ve 40\% případů lze hořčici a chléb uvažovat jako důsledek koupě párků.

\item {\bf Nevýhoda omezení na podporu:} Pravidla s velkou hodnotou spolehlivosti i zdvihu ale s nízkou podporou, jako např.:
$$\{\texttt{doutník}\} \Rightarrow \{\texttt{rum}\},$$
nebudou nalezeny.

\end{itemize}

\subsection{Shrnutí asociačních pravidel}

\begin{itemize}

\item {\bf Podpora} (angl. support): Jak často se množina položek vyskytuje v datech.
$$T(\mathcal{K}) = \frac{\text{počet transakcí obsahujících } \mathcal{K}}{\text{celkový počet transakcí}}$$

\item {\bf Spolehlivost} (angl. confidence): Jak často se závěr vyskytuje, pokud se vyskytuje předpoklad.
$$C(A \Rightarrow B) = \frac{T(A \cup B)}{T(A)} = \hat{P}(B \mid A)$$

\item {\bf Zdvih} (angl. lift): Kolikanásobně je pravděpodobnější závěr při splnění předpokladu oproti samostatnému výskytu závěru.
$$L(A \Rightarrow B) = \frac{C(A \Rightarrow B)}{T(B)} = \frac{P(B \mid A)}{P(B)}$$

\item {\bf Pokrytí} (angl. coverage): Jak často lze závěr vyložit jako důsledek předpokladu.
$$\text{Coverage}(A \Rightarrow B) = \frac{T(A \cup B)}{T(B)} = \hat{P}(A \mid B)$$

\item {\bf Interpretace zdvihu:}
\begin{itemize}
\item $L > 1$: Předpoklad a závěr jsou pozitivně korelované.
\item $L = 1$: Předpoklad a závěr jsou nezávislé.
\item $L < 1$: Předpoklad a závěr jsou negativně korelované.
\end{itemize}

\end{itemize}
%
%
%

\section{Zajímavá videa na YouTube přesahující rámec kurzu}

\subsection{Vybraná videa z pravděpodobnosti a statistiky}
\begin{itemize}

\item \href{https://youtu.be/HZGCoVF3YvM?si=rd2jAOUqzx6wwy6e}{But what is the Central Limit Theorem? (3Blue1Brown)} \hfill\textbf{[31~min]}

\item \href{https://youtu.be/zeJD6dqJ5lo?si=7y1gcf-pBlBT7G4P}{Bayes theorem, the geometry of changing beliefs
 (3Blue1Brown)} (část~I) \hfill\textbf{[15~min]}

\item \href{https://youtu.be/lG4VkPoG3ko?si=h7YEb7bAs9GE1l3W}{The medical test paradox, and redesigning Bayes' rule
 (3Blue1Brown)} (část~II) \hfill\textbf{[21~min]}

 \item \href{https://youtu.be/cy8r7WSuT1I?si=l3B86oTKAPq2853W}{Why $\pi$ is in the normal distribution (beyond integral tricks) (3Blue1Brown)} \hfill\textbf{[24~min]}

  \item \href{https://youtu.be/KuXjwB4LzSA?si=H4IxvjF6uflpSYzX}{ But what is a convolution? (3Blue1Brown)} \hfill\textbf{[23~min]}

  \item \href{https://youtu.be/KuXjwB4LzSA?si=WRv8qian4WwAombF}{Convolutions $\vert$ Why X+Y in probability is a beautiful mess
(3Blue1Brown)} \hfill\textbf{[27~min]}

 \item \href{https://youtu.be/d_qvLDhkg00?si=al14EJ-hNa4BdZu5}{A pretty reason why Gaussian + Gaussian = Gaussian (3Blue1Brown)} \hfill\textbf{[13~min]}

 \item \href{https://youtu.be/8idr1WZ1A7Q?si=PIdTiUFiZKLc846j}{
Binomial distributions $\vert$ Probabilities of probabilities, part 1 (3Blue1Brown)} (část~I) \hfill\textbf{[12~min]}

 \item \href{https://youtu.be/ZA4JkHKZM50?si=zZlkQYbCz359Ixr7}{
Why “probability of 0” does not mean “impossible” $\vert$ Probabilities of probabilities, part 2 (3Blue1Brown)} (část~II) \hfill \textbf{[10~min]}
\end{itemize}

\subsection{Vybraná videa z linearní algebry}
\begin{itemize}
\item \href{https://youtu.be/PFDu9oVAE-g?si=t5v9X7CPGAyjPMWT}{Eigenvectors and eigenvalues $\vert$ Chapter 14, Essence of linear algebra (3Blue1Brown)} \hfill \textbf{[17~min]}

\item \href{https://youtu.be/e50Bj7jn9IQ?si=EjTaFI7WCRUFVWIU}{A quick trick for computing eigenvalues $\vert$ Chapter 15, Essence of linear algebra (3Blue1Brown)} \hfill \textbf{[13~min]}

\item \href{https://youtu.be/rYz83XPxiZo?si=7AoDQKQFT9tmcL7O}{6. Singular Value Decomposition (SVD) (MIT OpenCourseWare, Gilbert Strang)} \hfill \textbf{[53~min]}

\item \href{https://youtu.be/YPe5OP7Clv4?si=Zl6oCBcFLMdBGpbm}{Gilbert Strang: Singular Value Decomposition (Lex Fridman)} \hfill \textbf{[5~min]}

\item \href{https://youtu.be/TgKwz5Ikpc8?si=mT7WnBAylYZanK8G}{Abstract vector spaces $\vert$ Chapter 16, Essence of linear algebra (3Blue1Brown)} \hfill \textbf{[16~min]}

\item \href{https://youtu.be/P2LTAUO1TdA?si=JA_Db8pkSSzs3Sr3}{Change of basis $\vert$ Chapter 13, Essence of linear algebra (3Blue1Brown)} \hfill \textbf{[12~min]}

\item \href{https://youtu.be/Ip3X9LOh2dk?si=iYn-8O6SsmlZ1xGO}{The determinant $\vert$ Chapter 6, Essence of linear algebra
 (3Blue1Brown)} \hfill \textbf{[10~min]}

\item \href{https://youtu.be/AmgkSdhK4K8?si=VzBc1TggI7PQjGjG}{Who cares about topology? (Old version) (3Blue1Brown)} \hfill \textbf{[16~min]}
\end{itemize}

\subsection{Markov Chains}
\begin{itemize}
\item \href{https://youtu.be/KZeIEiBrT_w?si=VNDmdCU37pB4QF--}{The Strange Math That Predicts (Almost) Anything
(Veritasium)} (vhodné pro prvotní myšlenku) \hfill \textbf{[32~min]}

\item \href{https://youtu.be/rHdX3ANxofs?si=DpbEo8VfhyWfQbls}{Intro to Markov Chains \& Transition Diagrams (Dr. Trefor Bazett)} \hfill \textbf{[11~min]}

\item \href{https://youtu.be/1GKtfgwf3ig?si=D_lMmw0QYaRrjhX8}{Markov Chains \& Transition Matrices (Dr. Trefor Bazett)} \hfill \textbf{[7~min]}
\end{itemize}

\subsection{Hidden Markov Models}
\begin{itemize}
\item \href{https://youtu.be/kqSzLo9fenk?si=01ewS6fNSK-zYiNu}{A friendly introduction to Bayes Theorem and Hidden Markov Models
 (Serrano.Academy)} (jednodušší, vhodné pro prvotní myšlenku) \hfill \textbf{[31~min]}

 \item \href{https://youtu.be/IcSou8bdKIA?si=39zYvpK6PBlmlHXB}{6.047/6.878 Lecture 4 - HMMs 1 Fall 2020 (Manolis Kellis)} \hfill \textbf{[81~min]}
\end{itemize}

\subsection{Deep Learning}
\begin{itemize}

\item Poznámka: Problematika neuronových sítí spadá do navazujícího kurzu Strojové učení 2.
\item \href{https://youtu.be/aircAruvnKk?si=Nl9qRp0B8Ewv6mzW}{But what is a neural network? $\vert$ Deep Learning chapter 1 (3Blue1Brown)} \hfill \textbf{[18~min]}

\item \href{https://youtu.be/IHZwWFHWa-w?si=QEgJk7ZLYR0otZE2}{Gradient descent, how neural networks learn $\vert$ Deep Learning Chapter 2 (3Blue1Brown)} \hfill \textbf{[20~min]}

\item \href{https://youtu.be/Ilg3gGewQ5U?si=Ox73-QheHCAm00iz}{Backpropagation, intuitively $\vert$ Deep Learning Chapter 3 (3Blue1Brown)} \hfill \textbf{[12~min]}

\item \href{https://youtu.be/tIeHLnjs5U8?si=Bbsx2T4vEjvTQwn5}{Backpropagation calculus $\vert$ Deep Learning Chapter 4 (3Blue1Brown)} \hfill \textbf{[10~min]}
\end{itemize}

\subsection{Large Language Models}
\begin{itemize}
\item Doporučení: Pro hlubší pochopení problematiky je vhodné se nejprve seznámit s obsahem předchozí sekce o hlubokém učení.
\item \href{https://youtu.be/LPZh9BOjkQs?si=J4z0VuBw64R6X3tL}{Large Language Models explained briefly (3Blue1Brown)} \hfill \textbf{[8~min]}

\item \href{https://youtu.be/wjZofJX0v4M?si=jsV5Va9NmxVQVb_m}{Transformers, the tech behind LLMs $\vert$ Deep Learning Chapter 5
 (3Blue1Brown)}

 \item \href{https://youtu.be/eMlx5fFNoYc?si=JrCksJ0LxjPA8Vub}{Attention in transformers, step-by-step $\vert$ Deep Learning Chapter 6 (3Blue1Brown)} \hfill \textbf{[27~min]}

 \item \href{https://youtu.be/9-Jl0dxWQs8?si=b9DifRBXTYm6CFe0}{How might LLMs store facts $\vert$ Deep Learning Chapter 7
 (3Blue1Brown)} \hfill \textbf{[22~min]}

  \item \href{https://youtu.be/iv-5mZ_9CPY?si=OpFMb-ZOguugeGnc}{But how do AI images and videos actually work? (3Blue1Brown \& Welch Labs)} \hfill \textbf{[37~min]}
\end{itemize}

\subsection{The Black-Scholes-Merton Model}
\begin{itemize}
\item Byl publikován v roce 1973 (Fischer Black, Myron Scholes a Robert Merton) a způsobil revoluci na finančních trzích. Myron Scholes a Robert Merton za tuto rovnici později získali Nobelovu cenu. Fischer Black by byl nepochybně třetím oceněným, avšak udělení ceny se nedožil (statut ceny neumožňuje udělení in memoriam).
\item \href{https://youtu.be/A5w-dEgIU1M?si=VzJjerMwGxFM8eTo}{
The Trillion Dollar Equation (Veritasium)} (vhodné pro prvotní myšlenku) \hfill \textbf{[31~min]}

\item \href{https://youtu.be/2UCHztlWuZg?si=JuKbUuv1tutDTsu_}{Lecture 21: Black-Scholes Formula, Risk Neutral Valuation (MIT OpenCourseWare)} \hfill \textbf{[79~min]}

\item \href{https://youtu.be/H4X-4e7fPgI?si=dt8_DmyHjESrQ6_P}{
Warren Buffett: Black-Scholes Formula Is Total Nonsense} (zajímavý názorový protipól) \hfill \textbf{[15~min]}

\end{itemize}

\subsection{Principal Component Analysis (PCA)}
\begin{itemize}
\item Poznámka: Problematika PCA spadá do navazujícího kurzu Strojové učení 2.
\item \href{https://youtu.be/HMOI_lkzW08?si=dhEswcA_shLLCkvm}{
StatQuest: PCA main ideas in only 5 minutes!!! (StatQuest with Josh Starmer)} (prvotní myšlenka) \hfill \textbf{[6~min]}

\item \href{https://youtu.be/FgakZw6K1QQ?si=vR2uBTxGtZr2mIpo}{
StatQuest: Principal Component Analysis (PCA), Step-by-Step
 (StatQuest with Josh Starmer)} \hfill \textbf{[22~min]}

\item \href{https://youtu.be/WW3ZJHPwvyg?si=8JMt5-8UecQW6woT}{
19. Principal Component Analysis (MIT OpenCourseWare)} (část I) \hfill \textbf{[77~min]}

\item \href{https://youtu.be/a1ZCeFpeW0o?si=utwwCjkiGtMgk5XL}{
20. Principal Component Analysis (cont.) (MIT OpenCourseWare)} (část II) \hfill \textbf{[76~min]}

\item \href{https://youtu.be/CechARGinR4?si=qRFkm4pF55qwGjOr}{
Lecture 9: Principal Component Analysis in Finance
 (MIT OpenCourseWare)} \hfill \textbf{[83~min]}

\end{itemize}

\subsection{Support Vector Machines (SVM)}
\begin{itemize}
\item Poznámka: Problematika SVM spadá do navazujícího kurzu Strojové učení 2.

\item \href{https://youtu.be/efR1C6CvhmE?si=AyQT-9fmMxp77rgl}{
Support Vector Machines Part 1: Main Ideas!!! (StatQuest with Josh Starmer)} (část~I) \hfill \textbf{[20~min]}

\item \href{https://youtu.be/Toet3EiSFcM?si=c5PHXXJEYlo6SvLs}{
Support Vector Machines Part 2: The Polynomial Kernel (StatQuest with Josh Starmer)} (část~II) \hfill \textbf{[7~min]}

\item \href{https://youtu.be/Qc5IyLW_hns?si=KVY3Sl5grcmenmrb}{
Support Vector Machines Part 3: The Radial (RBF) Kernel (StatQuest with Josh Starmer)} (část~III) \hfill \textbf{[15~min]}
 

\item \href{https://youtu.be/_PwhiWxHK8o?si=sDg4scI7O_xQKzd8}{
16. Learning: Support Vector Machines (MIT OpenCourseWare)} (komplexní úvod) \hfill \textbf{[49~min]}
\end{itemize}

\section{Souhrn důležitých vzorců} 

\subsection{Rozhodovací stromy (Decision Trees)}
\subsubsection{Entropie (Entropy)}
\begin{itemize}
\item Funkce měřící neuspořádanost existuje a říká se jí entropie:
\hyperref[sec:entropie]{$$H(\mathcal{D}) = -p_0 \log p_0 - p_1 \log p_1 = -p_0 \log p_0 - (1 - p_0) \log(1 - p_0)$$}
Ve vzorci se nejčastěji používá dvojkový logaritmus. V takovém
případě se jednotce entropie říká {\bf bit}. (např. $H(\mathcal{D})$ se bude rovnat $0.81$ a jednotce se říká $0.81$ bitu, tj. $H(\mathcal{D}) = 0.81 \ bitu$).
\end{itemize}
\subsubsection{Informační zisk (Information Gain)}
\begin{itemize}
\item Formálně:
\hyperref[sec:information_gain]{$$IG(\mathcal{D}, X_i) = H(\mathcal{D}) - t_0H(\mathcal{D}_0) - t_1H(\mathcal{D}_1), \ i \in \{1,...p\}$$}

kde pro $v \in \{0, 1\}$ je $\mathcal{D}_v = \{ \mathbf{x} \in \mathcal{D} \mid x_i = v \}$ a $t_v$ je podíl počtu prvků v $\mathcal{D}_v$ a $\mathcal{D}$, neboli $t_v = \frac{\left|\mathcal{D}_v\right|}{\left|\mathcal{D}\right|}$. V tomto vzorci $X_i$ označuje $i$-tý sloupec (resp. příznak) matice $\mathbf{X}$, což je matice dat.
\end{itemize}

\subsubsection{Gini Index (Gini impurity)}
\begin{itemize}
\item Pro množinu $\mathcal{D}$ s $k$ různými hodnotami Gini Index formálně definujeme jako:
\hyperref[sec:gini_index]{$$GI(\mathcal{D}) = 1 - \sum_{i=0}^{k-1} p_i^2 = \sum_{i=0}^{k-1} p_i(1 - p_i)$$}

kde $p_i$ je relativní četnost tj. $$p_i = \frac{\text{počet vzorků třídy } i}{\text{celkový počet vzorků}}$$
\end{itemize}

\subsubsection{Analogie informačního zisku pro regresi}
\begin{itemize}
\item Funguje stejně jako ID3 — rozdělí $\mathcal{D}$ na $\mathcal{D}_L$ a $\mathcal{D}_R$, ale místo entropie:
$$IG(\mathcal{D}, X_i) = H(\mathcal{D}) - t_L H(\mathcal{D}_L) - t_R H(\mathcal{D}_R), \ i \in \{1,...,p\}$$
používá:
\hyperref[sec:cart]{$$IG(\mathcal{D}, X_i) = MSE(\mathcal{D}) - t_L MSE(\mathcal{D}_L) - t_R MSE(\mathcal{D}_R), \ i \in \{1,...,p\}$$}
kde $t_L = \frac{|\mathcal{D}_L|}{|\mathcal{D}|}$ a $t_R = \frac{|\mathcal{D}_R|}{|\mathcal{D}|}$. V tomto vzorci $X_i$ označuje $i$-tý sloupec (resp. příznak) matice $\mathbf{X}$, což je matice dat.
\end{itemize}

\subsection{kNN (k-Nearest Neighbor)}
\subsubsection{$\mathbf{k}$-normy}
\begin{itemize}
\item Pro $k \in \mathbb{N}$ definujeme $k$-normy předpisem:
\hyperref[sec:knn_hyperparametry]{$$\|\mathbf{x} - \mathbf{y}\|_k = d_k(\mathbf{x}, \mathbf{y}) = \sqrt[k]{\sum_{i=1}^{p} |x_i - y_i|^k}$$}
kde $k \in \N$.
\end{itemize}

\subsubsection{Euklidovská vzdálenost}
\begin{itemize}

\item Pro $k = 2$ dostáváme Euklidovskou vzdálenost:
\hyperref[sec:knn_hyperparametry]{$$\|\mathbf{x} - \mathbf{y}\|_2 = d_2(\mathbf{x}, \mathbf{y}) = \sqrt{\sum_{i=1}^{p} (x_i - y_i)^2}$$}

\end{itemize}

\subsubsection{Manhattanská vzdálenost}
\begin{itemize}

\item Pro $k = 1$ dostáváme Manhattanskou vzdálenost:
\hyperref[sec:knn_hyperparametry]{$$\|\mathbf{x} - \mathbf{y}\|_1 = d_1(\mathbf{x}, \mathbf{y}) = \sum_{i=1}^{p} |x_i - y_i|$$}
\end{itemize}

\subsubsection{Min-max normalizace}
\begin{itemize}
\item Pro daný příznak najdeme jeho minimální a maximální hodnotu v trénovacích datech: $\min_x$, $\max_x$.
\item Pak hodnotu $x_i$ tohoto příznaku pro $i$-tý datový bod nahradíme:
\hyperref[sec:minmax_normalizace]{$$x_i \leftarrow \frac{x_i - \min_x}{\max_x - \min_x} \in [0, 1]$$}

\end{itemize}

\subsubsection{Standardizace}
\begin{itemize}
\item Pro každý příznak nalezneme jeho výběrový průměr $\bar{x}$ a výběrový rozptyl $s_x^2$ a každý $i$-tý bod nahradíme:
\hyperref[sec:minmax_normalizace]{$$x_i \leftarrow \frac{x_i - \bar{x}}{\sqrt{s_x^2}}$$}
kde $\bar{x}$ je výběrový průměr a $s_x^2$ je výběrový rozptyl.
\item Převádí na škálu s $\bar{x}=0$ a $\sqrt{s_x^2} = 1$.
\end{itemize}




\subsection{Lineární regrese (Linear Regression)}
\subsubsection{Residuální součet čtverců (Residual Sum of Squares)}
\begin{itemize}
\item Při trénování minimalizujeme {\bf residuální součet čtverců} (angl. Residual Sum of Squares), který značíme $RSS(\mathbf{w})$ a definujeme předpisem:

\hyperref[sec:linear_regression]{$$RSS(\mathbf{w})= \sum_{i = 1}^N  L(Y_i, \mathbf{w}^T \mathbf{x}_i) = \sum_{i = 1}^N  (Y_i - \mathbf{w}^T \mathbf{x}_i)^2 = ||\mathbf{Y} - \mathbf{X}\mathbf{w}||^2$$}

kde $N$ reprezentuje počet řádků v datasetu. Minimalizací tohoto výrazu získáme odhad $\hat{\mathbf{w}}$.

Toto chceme minimalizovat. Výraz si proto vhodně upravíme:

\hyperref[sec:linear_regression]{\begin{align*}
RSS(\mathbf{w}) &= \sum_{i = 1}^N (Y_i - \mathbf{w}^T \mathbf{x}_i)^2 \\
&= (\mathbf{Y} - \mathbf{X} \mathbf{w})^T(\mathbf{Y} - \mathbf{X} \mathbf{w}) \\
&= \mathbf{Y}^T\mathbf{Y} - 2\mathbf{Y}^T\mathbf{X}\mathbf{w} + \mathbf{w}^T\mathbf{X}^T\mathbf{X}\mathbf{w}
\end{align*}}
\end{itemize}


\subsubsection{Gradient $RSS(\mathbf{w})$}
\begin{itemize}
\item Hledáme minimum, proto sestrojíme {\bf gradient} $RSS(\mathbf{w})$:

\hyperref[sec:linear_regression]{\begin{align*}
\nabla RSS(\mathbf{w}) = \frac{\partial RSS(\mathbf{w})}{\partial \mathbf{w}} &= \frac{\partial}{\partial \mathbf{w}} (\mathbf{Y}^T\mathbf{Y} - 2\mathbf{Y}^T\mathbf{X}\mathbf{w} + \mathbf{w}^T\mathbf{X}^T\mathbf{X}\mathbf{w}) \\
&= - 2\mathbf{X}^T\mathbf{Y} + 2\mathbf{X}^T\mathbf{X}\mathbf{w} 
\end{align*}}
\end{itemize}


\subsubsection{Hessova matice $RSS(\mathbf{w})$}
\begin{itemize}
\item Potom {\bf Hessova matice} (angl. Hessian Matrix) $RSS(\mathbf{w})$ je:

$$
\hyperref[sec:linear_regression]{\begin{align*}
\nabla^2 RSS(\mathbf{w}) = \frac{\partial}{\partial \mathbf{w}} \left( \nabla RSS(\mathbf{w}) \right) 
&= \frac{\partial}{\partial \mathbf{w}} \left( - 2\mathbf{X}^T\mathbf{Y} + 2\mathbf{X}^T\mathbf{X}\mathbf{w} \right) \\
&= \frac{\partial}{\partial \mathbf{w}} \left( - 2\mathbf{X}^T\mathbf{Y} \right) + \frac{\partial}{\partial \mathbf{w}} \left( 2\mathbf{X}^T\mathbf{X}\mathbf{w} \right)
\\
&=2\mathbf{X}^T\mathbf{X}
\end{align*}$$}
\end{itemize}


\subsubsection{Důkaz pozitivní definitnosti Hessovy matice $RSS(\mathbf{w})$}
\begin{itemize}
\item Nyní musíme z definice dokázat, že Hessova matice je \textbf{pozitivně definitní}, abychom dokázali, že v řešení normální rovnice je lokální minimum. {\bf Předpokládejme, že $\mathbf{X}^T\mathbf{X}$ je regulární matice} (nebo ekvivalentně, že sloupce matice $\mathbf{X}$ jsou lineárně nezávislé). Pro libovolný vektor $\mathbf{v} \in \mathbb{R}^{p+1}$ platí:

\hyperref[sec:proof_PD_lr]{$$\mathbf{v}^T(2\mathbf{X}^T\mathbf{X})\mathbf{v} = 2 \mathbf{v}^T(\mathbf{X}^T\mathbf{X})\mathbf{v}= 2 (\mathbf{X}\mathbf{v})^T(\mathbf{X}\mathbf{v})= 2||\mathbf{X}\mathbf{v}||^2 > 0$$}

Z lineární nezávislosti sloupců $\mathbf{X}$ plyne, že pro $(\forall \mathbf{v} \neq \mathbf{0})(\mathbf{X}\mathbf{v} \neq \mathbf{0})$, a tedy:
$$2\|\mathbf{X} \mathbf{v}\|^2 > 0 \quad \text{pro} \ \forall \mathbf{v} \neq \mathbf{0}$$

Hessova matice je tedy pozitivně definitní, což znamená, že řešení normální rovnice $\hat{\mathbf{w}}$ je bodem ostrého lokálního minima.
\end{itemize}


\subsubsection{Řešení normální rovnice $\hat{\mathbf{w}}$}
\begin{itemize}
\item Nyní, když jsme dokázali, že v řešení normální rovnice nastává lokální minimum, můžeme z podmínky $\nabla RSS(\mathbf{w}) = 0$ vyjádřit $\mathbf{w}$, čímž získáme odhad $\hat{\mathbf{w}}$:

\hyperref[sec:proof_PD_lr]{$$\nabla RSS(\mathbf{w}) = 0 \iff
- 2\mathbf{X}^T\mathbf{Y} + 2\mathbf{X}^T\mathbf{X}\mathbf{w} = 0 \iff \mathbf{X}^T\mathbf{X}\mathbf{w} = \mathbf{X}^T\mathbf{Y} \iff \mathbf{w} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}$$}

Odvozený výraz:
$$
\hat{\mathbf{w}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}
$$
nazýváme \textbf{řešením normální rovnice}.
\end{itemize}


\subsubsection{Řešení normální rovnice pomocí SVD rozkladu}
\begin{itemize}
\item $\hat{\mathbf{w}}$ lze proto stabilněji a univerzálněji spočítat pomocí SVD. Dosadíme-li $\mathbf{X} = \mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^T$ do řešení normální rovnice:
\hyperref[sec:SVD]{\begin{align*}
\hat{\mathbf{w}} &= (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y} \\
&= ((\mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^T)^T(\mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^T))^{-1}(\mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^T)^T\mathbf{Y} \\
&= (\mathbf{V}\boldsymbol{\Sigma}^T\mathbf{U}^T\mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^T)^{-1}\mathbf{V}\boldsymbol{\Sigma}^T\mathbf{U}^T\mathbf{Y} \\
&= (\mathbf{V}\boldsymbol{\Sigma}^T\boldsymbol{\Sigma}\mathbf{V}^T)^{-1}\mathbf{V}\boldsymbol{\Sigma}^T\mathbf{U}^T\mathbf{Y} \quad \text{(využili jsme $\mathbf{U}^T\mathbf{U} = I$)} \\
&= \mathbf{V}(\boldsymbol{\Sigma}^T\boldsymbol{\Sigma})^{-1}\mathbf{V}^T\mathbf{V}\boldsymbol{\Sigma}^T\mathbf{U}^T\mathbf{Y} \\
&= \mathbf{V}(\boldsymbol{\Sigma}^T\boldsymbol{\Sigma})^{-1}\boldsymbol{\Sigma}^T\mathbf{U}^T\mathbf{Y} \quad \text{(využili jsme $\mathbf{V}^T\mathbf{V} = I$)} \\
&= \mathbf{V}\boldsymbol{\Sigma}^+\mathbf{U}^T\mathbf{Y}
\end{align*}}
kde $\boldsymbol{\Sigma}^+ = (\boldsymbol{\Sigma}^T\boldsymbol{\Sigma})^{-1}\boldsymbol{\Sigma}^T$ je {\bf Moorova-Penroseova pseudoinverze} (angl. Moore-Penrose pseudoinverse) matice $\boldsymbol{\Sigma}$.
\end{itemize}


\subsubsection{Geometrická interpretace metody nejmenších čtverců}
\begin{itemize}
\item Minimalizace $RSS(\mathbf{w}) = ||\mathbf{Y} - \mathbf{X}\mathbf{w}||^2$
je ekvivalentní minimalizaci $||\mathbf{Y} -\mathbf{X}\mathbf{w}||$.

\vspace{0.3cm}
\begin{center}
\includegraphics[width=0.8\textwidth]{assets/gOLS.png}
\end{center}
\begin{center}
\text{Obrázek 3: Geometrická interpretace metody nejmenších čtverců}
\label{fig:gOLS.png}
\end{center}
\vspace{0.3cm}

\textbf{Hlavní myšlenka:} Hledáme $\mathbf{w}$ tak, aby $\mathbf{Xw}$ byla ortogonální projekce $\mathbf{Y}$ na sloupcový prostor matice $\mathbf{X}$.

\textbf{Podmínka kolmosti residuí:}
\hyperref[sec:geom_interp_met_nejm_ctvercu]{$$(\mathbf{X}_{\bullet i})^T (\mathbf{Y} - \mathbf{X}\mathbf{w}) = 0 \quad \forall i = 0, \dots, p$$}

\textbf{Maticový zápis} (normální rovnice):
\hyperref[sec:geom_interp_met_nejm_ctvercu]{$$\mathbf{X}^T (\mathbf{Y} - \mathbf{X}\mathbf{w}) = \mathbf{0} \quad \iff \quad \mathbf{X}^T\mathbf{X}\mathbf{w} = \mathbf{X}^T\mathbf{Y}$$}
\end{itemize}


\subsection{Hřebenová regrese (Ridge Regression)}
\subsubsection{Regularizovaný reziduální součet čtverců}
\begin{itemize}
\item Zavedeme-li matici
\hyperref[sec:ridge_regression]{$$\mathbf{I}' = \begin{pmatrix}
0 & 0 & \cdots & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1
\end{pmatrix} \in \mathbb{R}^{p+1,p+1},$$}

potom se minimalizuje tedy {\bf regularizovaný reziduální součet čtverců}
\hyperref[sec:ridge_regression]{$$RSS_\lambda(\mathbf{w}) = 
\sum_{i = 1}^N  (Y_i - \mathbf{w}^T \mathbf{x}_i)^2 + \lambda \sum_{i=1}^{p} w_i^2 = \|\mathbf{Y} - \mathbf{X}\mathbf{w}\|^2 + \lambda \sum_{i=1}^{p} w_i^2 = \|\mathbf{Y} - \mathbf{X}\mathbf{w}\|^2 + \lambda \mathbf{w}^T \mathbf{I}' \mathbf{w}$$}
\end{itemize}


\subsubsection{Gradient $RSS_\lambda(\mathbf{w})$}
\begin{itemize}
\item Hledá se minimum, proto sestrojí se {\bf gradient} $RSS_\lambda(\mathbf{w})$:
\hyperref[sec:ridge_regression]{\begin{align*}
\nabla RSS_\lambda(\mathbf{w}) = \frac{\partial RSS_\lambda(\mathbf{w})}{\partial \mathbf{w}} &= \frac{\partial}{\partial \mathbf{w}} \left( \|\mathbf{Y} - \mathbf{X}\mathbf{w}\|^2 + \lambda \mathbf{w}^T \mathbf{I}' \mathbf{w} \right) \\
&= \frac{\partial}{\partial \mathbf{w}} \left( \mathbf{Y}^T\mathbf{Y} - 2\mathbf{Y}^T\mathbf{X}\mathbf{w} + \mathbf{w}^T\mathbf{X}^T\mathbf{X}\mathbf{w} + \lambda \mathbf{w}^T \mathbf{I}' \mathbf{w} \right) \\
&= -2\mathbf{X}^T\mathbf{Y} + 2\mathbf{X}^T\mathbf{X}\mathbf{w} + 2\lambda \mathbf{I}' \mathbf{w} \\
&= -2\mathbf{X}^T(\mathbf{Y} - \mathbf{X}\mathbf{w}) + 2\lambda \mathbf{I}' \mathbf{w}
\end{align*}}
\end{itemize}

\subsubsection{Normální rovnice}
\hyperref[sec:ridge_regression]{$$\mathbf{X}^T\mathbf{Y} - \mathbf{X}^T\mathbf{X}\mathbf{w} - \lambda \mathbf{I}' \mathbf{w} = \mathbf{0}.$$}

\subsubsection{Hessova matice $RSS_\lambda(\mathbf{w})$}
\begin{itemize}
\hyperref[sec:ridge_regression]{\begin{align*}
\nabla^2 RSS_\lambda(\mathbf{w}) = \frac{\partial}{\partial \mathbf{w}} \left( \nabla RSS_\lambda(\mathbf{w}) \right) 
&= \frac{\partial}{\partial \mathbf{w}} \left( -2\mathbf{X}^T\mathbf{Y} + 2\mathbf{X}^T\mathbf{X}\mathbf{w} + 2\lambda \mathbf{I}' \mathbf{w} \right) \\
&= 2\mathbf{X}^T\mathbf{X} + 2\lambda \mathbf{I}' \\
&= 2(\mathbf{X}^T\mathbf{X} + \lambda \mathbf{I}').
\end{align*}}
\end{itemize}


\subsubsection{Důkaz pozitivní definitnosti Hessovy matice}
\begin{itemize}
\item $\forall \mathbf{v} \in \mathbb{R}^{p+1}$, $\mathbf{v} \neq \mathbf{0}$ a $\lambda > 0$ platí
\hyperref[sec:ridge_regression]{$$\mathbf{v}^T(\mathbf{X}^T\mathbf{X} + \lambda \mathbf{I}')\mathbf{v} = (\mathbf{X}\mathbf{v})^T(\mathbf{X}\mathbf{v}) + \lambda \mathbf{v}^T \mathbf{I}' \mathbf{v} = \|\mathbf{X}\mathbf{v}\|^2 + \lambda \sum_{i=1}^{p} v_i^2 > 0,$$}
protože pro $\mathbf{v} = (v_0, 0, \dots, 0)^T \neq \mathbf{0}$ platí $\mathbf{X}\mathbf{v} = (v_0, \dots, v_0)^T \neq \mathbf{0}$.

\item Hessova matice je tedy pozitivně definitní a matice $\mathbf{X}^T\mathbf{X} + \lambda \mathbf{I}'$ je vždy regulární pro $\lambda > 0$.
\end{itemize}


\subsubsection{Řešení normální rovnice $\hat{\mathbf{w}}_\lambda$}
\begin{itemize}
\item Pro $\lambda > 0$ tak vždy existuje jednoznačné řešení normální rovnice
\hyperref[sec:ridge_regression]{$$\hat{\mathbf{w}}_\lambda = (\mathbf{X}^T\mathbf{X} + \lambda \mathbf{I}')^{-1}\mathbf{X}^T\mathbf{Y}$$}
a odpovídá globálnímu minimu $RSS_\lambda$.

\item Predikce v bodě $\mathbf{x}$ je potom opět $\hat{Y} = \mathbf{x}^T \hat{\mathbf{w}}_\lambda$.
\end{itemize}

\subsubsection{Model bázových funkcí}
\begin{itemize}
\hyperref[sec:bazove_funkce_model]{$$Y = \sum_{j=0}^{M} w_j \varphi_j(\mathbf{x}) + \varepsilon = \boldsymbol{\varphi}(\mathbf{x})^T \mathbf{w} + \varepsilon$$}

Predikce: \hyperref[sec:bazove_funkce_model]{$\hat{Y} = \boldsymbol{\varphi}(\mathbf{x})^T \hat{\mathbf{w}}_\lambda$}
\end{itemize}



\subsection{Statistické vlastnosti modelů}
\subsubsection{Rozklad očekávané chyby}
\begin{itemize}
\item Očekávaná chyba modelu je součtem neodstranitelné chyby, kvadrátu vychýlení odhadu a rozptylu odhadu:
\hyperref[sec:stat_vlastnosti]{$$\mathbb{E}L(Y,\hat{Y}) = \sigma^2 + (\text{bias}\,\hat{Y})^2 + \text{var}\,\hat{Y}$$}
\end{itemize}

\subsubsection{Definice biasu a variance}
\begin{itemize}
\item \textbf{Vychýlení odhadu} (angl. bias):
\hyperref[sec:stat_vlastnosti]{$$\text{bias}\,\hat{Y} = \mathbb{E}\hat{Y} - \mathbb{E}Y$$}

\item \textbf{Rozptyl odhadu} (angl. variance):
\hyperref[sec:stat_vlastnosti]{$$\text{var}\,\hat{Y} = \mathbb{E}(\hat{Y} - \mathbb{E}\hat{Y})^2$$}

\item \textbf{Střední kvadratická chyba odhadu} (angl. MSE):
\hyperref[sec:stat_vlastnosti]{$$\text{MSE}(\hat{Y}) = (\text{bias}\,\hat{Y})^2 + \text{var}\,\hat{Y}$$}
\end{itemize}

\subsubsection{Bias-variance tradeoff}
\begin{itemize}
\item Pro hřebenovou regresi platí (zjednodušeně):
\hyperref[sec:stat_vlastnosti]{$$(\text{bias}\,\hat{Y})^2 \sim \left(1 - \frac{1}{1+\lambda}\right)^2 \quad \text{a} \quad \text{var}\,\hat{Y} \sim \left(\frac{1}{1+\lambda}\right)^2$$}
S rostoucím $\lambda$ vychýlení roste a rozptyl klesá.
\end{itemize}

\subsubsection{Nestrannost OLS}
\begin{itemize}
\item \textbf{Věta:} Odhad $\hat{\mathbf{w}}_{OLS}$ získaný metodou nejmenších čtverců je za předpokladu $\mathbb{E}\boldsymbol{\varepsilon} = \mathbf{0}$ nestranný:
\hyperref[sec:stat_vlastnosti]{$$\mathbb{E}\hat{\mathbf{w}}_{OLS} = \mathbf{w}$$}
\end{itemize}

\subsubsection{Důkaz: nestrannost OLS}
\begin{itemize}
\item Z linearity střední hodnoty:
\hyperref[sec:stat_vlastnosti]{\begin{align*}
\mathbb{E}\hat{\mathbf{w}}_{OLS} &= \mathbb{E}\left[(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}\right] = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbb{E}\mathbf{Y} \\
&= (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{X}\mathbf{w} = \mathbf{w}
\end{align*}}
\item Z nestrannosti $\hat{\mathbf{w}}_{OLS}$ plyne nestrannost predikce:
\hyperref[sec:stat_vlastnosti]{$$\mathbb{E}\hat{Y} = \mathbf{x}^T\mathbb{E}\hat{\mathbf{w}}_{OLS} = \mathbf{x}^T\mathbf{w} = \mathbb{E}Y \quad \Rightarrow \quad \text{bias}\,\hat{Y} = 0$$}
\end{itemize}

\subsection{Logistická regrese}
\subsubsection{Logistická funkce (sigmoida)}
\begin{itemize}
\item Logistická funkce $f : \mathbb{R} \to (0,1)$:
\hyperref[sec:logisticka_regrese]{$$f(x) = \frac{e^x}{1+e^x} = \frac{1}{1+e^{-x}}$$}
\end{itemize}

\subsubsection{Výpočet $P(Y=1 \mid \mathbf{x}, \mathbf{w})$}
\begin{itemize}
\item Pravděpodobnost třídy 1:
\hyperref[sec:logisticka_regrese]{$$P(Y = 1 \mid \mathbf{x}, \mathbf{w}) = \frac{e^{\mathbf{w}^T\mathbf{x}}}{1 + e^{\mathbf{w}^T\mathbf{x}}}$$}

\item Pravděpodobnost třídy 0:
\hyperref[sec:logisticka_regrese]{$$P(Y = 0 \mid \mathbf{x}, \mathbf{w}) = \frac{1}{1 + e^{\mathbf{w}^T\mathbf{x}}}$$}

\item Predikce: $\hat{Y} = 1$ pokud $P(Y=1) > \frac{1}{2}$, jinak $\hat{Y} = 0$.
\end{itemize}

\subsubsection{Maximálně věrohodný odhad (Maximum Likelihood Estimation)}
\begin{itemize}
\item \textbf{Věrohodnostní funkce}:
\hyperref[sec:logisticka_regrese]{$$L(\mathbf{w}) = \prod_{i=1}^{N} p_{Y_i}(\mathbf{x}_i, \mathbf{w})$$}

\item \textbf{Logaritmus věrohodnostní funkce}:
\hyperref[sec:logisticka_regrese]{$$\ell(\mathbf{w}) = \sum_{i=1}^{N} \left(Y_i\mathbf{w}^T\mathbf{x}_i - \ln(1 + e^{\mathbf{w}^T\mathbf{x}_i})\right)$$}

\item \textbf{Gradient}:
\hyperref[sec:logisticka_regrese]{$$\nabla \ell(\mathbf{w}) = \mathbf{X}^T(\mathbf{Y} - \mathbf{P})$$}
kde $\mathbf{P} = (p_1(\mathbf{x}_1, \mathbf{w}), \dots, p_1(\mathbf{x}_N, \mathbf{w}))^T$.
\end{itemize}

\subsection{Ensemble metody (Ensemble Methods)}
\subsubsection{Predikce náhodného lesa (regrese)}
\begin{itemize}
\item Predikce je průměr predikcí jednotlivých stromů:
\hyperref[sec:ensemble]{$$\hat{Y} = \frac{1}{n}\sum_{i=1}^{n} \hat{Y}_i$$}
\end{itemize}

\subsubsection{Predikce náhodného lesa (binární klasifikace)}
\begin{itemize}
\item Predikovaná pravděpodobnost třídy 1:
\hyperref[sec:ensemble]{$$\hat{p} = \frac{1}{n}\sum_{i=1}^{n} \hat{p}_i$$}

\item Finální predikce:
\hyperref[sec:ensemble]{$$\hat{Y} = \begin{cases} 1 & \text{pro } \hat{p} > 0.5 \\ 0 & \text{jinak} \end{cases}$$}
\end{itemize}

\subsubsection{AdaBoost: výpočet $\alpha^{(m)}$}
\begin{itemize}
\item Váha $m$-tého stromu:
\hyperref[sec:ensemble]{$$\alpha^{(m)} = \texttt{learning\_rate} \cdot \log \frac{1 - e^{(m)}}{e^{(m)}}$$}
kde $e^{(m)}$ je součet vah špatně klasifikovaných bodů stromem $T^{(m)}$.
\end{itemize}

\subsubsection{AdaBoost: aktualizace vah}
\begin{itemize}
\item Pro špatně klasifikované body:
\hyperref[sec:ensemble]{$$w_i \leftarrow w_i \exp(\alpha^{(m)})$$}
Poté se váhy znormalizují tak, aby jejich součet byl jedna.
\end{itemize}

\subsection{Evaluace modelů: metriky}
\subsubsection{Odhad pravděpodobnosti $\hat{p}$}
\begin{itemize}
\item U binární klasifikace model odhaduje pravděpodobnost:
\hyperref[sec:evaluace_metriky]{$$\hat{p} = \hat{P}(Y = 1 \mid X = \mathbf{x})$$}
\end{itemize}

\subsubsection{Ztrátová funkce $L(Y,\hat{p})$ (binary cross-entropy)}
\begin{itemize}
\hyperref[sec:evaluace_metriky]{$$L(Y, \hat{p}) = -Y \log \hat{p} - (1 - Y) \log(1 - \hat{p})$$}
\end{itemize}

\subsubsection{MSE (Mean Squared Error)}
\begin{itemize}
\hyperref[sec:evaluace_metriky]{$$MSE = \frac{1}{N} \sum_{i=1}^{N} (Y_i - \hat{Y}_i)^2$$}
\end{itemize}

\subsubsection{RMSE (Root Mean Squared Error)}
\begin{itemize}
\hyperref[sec:evaluace_metriky]{$$RMSE = \sqrt{MSE} = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (Y_i - \hat{Y}_i)^2}$$}
\end{itemize}

\subsubsection{RMSLE (Root Mean Squared Logarithmic Error)}
\begin{itemize}
\hyperref[sec:evaluace_metriky]{$$RMSLE = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (\log Y_i - \log \hat{Y}_i)^2}$$}
\end{itemize}

\subsubsection{MAE (Mean Absolute Error)}
\begin{itemize}
\hyperref[sec:evaluace_metriky]{$$MAE = \frac{1}{N} \sum_{i=1}^{N} |Y_i - \hat{Y}_i|$$}
\end{itemize}

\subsubsection{$R^2$ (koeficient determinace)}
\begin{itemize}
\hyperref[sec:evaluace_metriky]{$$R^2 = 1 - \frac{\sum_{i=1}^{N}(Y_i - \hat{Y}_i)^2}{\sum_{i=1}^{N}(Y_i - \bar{Y})^2}$$}
\end{itemize}

\subsubsection{Matice záměn (Confusion Matrix)}
\begin{itemize}
\item $TP$ (True Positive), $FP$ (False Positive), $FN$ (False Negative), $TN$ (True Negative)
\item $N_+ = TP + FN$ (skutečně pozitivní), $N_- = FP + TN$ (skutečně negativní)
\item $N = TP + FP + FN + TN$ (celkový počet)
\end{itemize}

\subsubsection{Odvozené metriky z matice záměn}
\begin{itemize}
\item \textbf{TPR} (sensitivita, recall):
\hyperref[sec:evaluace_metriky]{$$TPR = \frac{TP}{TP + FN}$$}

\item \textbf{FPR} (false positive rate):
\hyperref[sec:evaluace_metriky]{$$FPR = \frac{FP}{FP + TN}$$}

\item \textbf{TNR} (specificita):
\hyperref[sec:evaluace_metriky]{$$TNR = \frac{TN}{FP + TN}$$}

\item \textbf{PPV} (precision):
\hyperref[sec:evaluace_metriky]{$$PPV = \frac{TP}{TP + FP}$$}
\end{itemize}

\subsubsection{ACC (Accuracy)}
\begin{itemize}
\hyperref[sec:evaluace_metriky]{$$ACC = \frac{TP + TN}{TP + FP + FN + TN}$$}
\end{itemize}

\subsubsection{F1 score}
\begin{itemize}
\item Harmonický průměr precision a recall:
\hyperref[sec:evaluace_metriky]{$$F_1 = 2 \frac{PPV \cdot TPR}{PPV + TPR} = \frac{2TP}{2TP + FP + FN}$$}
\end{itemize}

\subsection{Evaluace modelů: testovací chyba a její odhad}
\subsubsection{Trénovací chyba}
\begin{itemize}
\hyperref[sec:evaluace_testovaci]{$$err_{train} = \frac{1}{N} \sum_{i=1}^{N} L(Y_i, \hat{Y}(\mathbf{x}_i))$$}
\end{itemize}

\subsubsection{Testovací chyba $err_{test}$}
\begin{itemize}
\hyperref[sec:evaluace_testovaci]{$$err_{test} = \frac{1}{N_{test}} \sum_{i=1}^{N_{test}} L(Y_i, \hat{Y}(\mathbf{x}_i))$$}
\end{itemize}

\subsubsection{Cross-validační chyba $\hat{e}$}
\begin{itemize}
\item Při $k$-násobné křížové validaci:
\hyperref[sec:evaluace_testovaci]{$$\hat{e} = \frac{1}{k} \sum_{i=1}^{k} e_i$$}
kde $e_i$ je chyba na $i$-té validační části.
\end{itemize}

\subsection{Výběr příznaků (Feature Selection)}
\subsubsection{Regularizovaný reziduální součet čtverců $RSS_\lambda^{Lasso}(\mathbf{w})$}
\begin{itemize}
\hyperref[sec:lasso]{$$RSS^{Lasso}_\lambda(\mathbf{w}) = \|\mathbf{Y} - \mathbf{X}\mathbf{w}\|^2 + \lambda \sum_{i=1}^{p} |w_i|$$}
\end{itemize}

\subsubsection{Definice řešení Lasso: $\hat{\mathbf{w}}_\lambda^{Lasso}$}
\begin{itemize}
\hyperref[sec:lasso]{$$\hat{\mathbf{w}}^{Lasso}_\lambda = \arg\min_{\mathbf{w}} RSS^{Lasso}_\lambda(\mathbf{w})$$}

\item Pro ortonormální příznaky (soft thresholding):
\hyperref[sec:lasso]{$$\hat{w}^{Lasso}_{\lambda;j} = \text{sgn}(\hat{w}^{OLS}_j) \cdot \max\left(0, |\hat{w}^{OLS}_j| - \frac{\lambda}{2}\right)$$}
\end{itemize}

\subsubsection{Elastic Net}
\begin{itemize}
\hyperref[sec:lasso]{$$RSS^{ElasticNet}_{\lambda_1, \lambda_2}(\mathbf{w}) = \|\mathbf{Y} - \mathbf{X}\mathbf{w}\|^2 + \lambda_1 \sum_{i=1}^{p} |w_i| + \lambda_2 \sum_{i=1}^{p} w_i^2$$}
\end{itemize}

\subsection{Hierarchické shlukování (Hierarchical Clustering)}
\subsubsection{Euklidovská vzdálenost $L_2$}
\begin{itemize}
\hyperref[sec:hier_clustering]{$$d_2(\mathbf{x}, \mathbf{y}) = \sqrt{\sum_{i=1}^{p} (x_i - y_i)^2}$$}
\end{itemize}

\subsubsection{Manhattanská vzdálenost $L_1$}
\begin{itemize}
\hyperref[sec:hier_clustering]{$$d_1(\mathbf{x}, \mathbf{y}) = \sum_{i=1}^{p} |x_i - y_i|$$}
\end{itemize}

\subsubsection{Čebyševova vzdálenost $L_\infty$}
\begin{itemize}
\hyperref[sec:hier_clustering]{$$d_\infty(\mathbf{x}, \mathbf{y}) = \max_{i} |x_i - y_i|$$}
\end{itemize}

\subsubsection{Měření vzdálenosti shluků: Metoda nejbližšího souseda (single linkage)}
\begin{itemize}
\hyperref[sec:hier_clustering]{$$D(A, B) = \min_{\mathbf{x} \in A, \mathbf{y} \in B} d(\mathbf{x}, \mathbf{y})$$}
\end{itemize}

\subsubsection{Měření vzdálenosti shluků: Metoda nejvzdálenějšího souseda (complete linkage)}
\begin{itemize}
\hyperref[sec:hier_clustering]{$$D(A, B) = \max_{\mathbf{x} \in A, \mathbf{y} \in B} d(\mathbf{x}, \mathbf{y})$$}
\end{itemize}

\subsubsection{Měření vzdálenosti shluků: Párová vzdálenost (average linkage)}
\begin{itemize}
\hyperref[sec:hier_clustering]{$$D(A, B) = \frac{1}{|A||B|} \sum_{\mathbf{x} \in A, \mathbf{y} \in B} d(\mathbf{x}, \mathbf{y})$$}
\end{itemize}

\subsubsection{Měření vzdálenosti shluků: Wardova metoda}
\begin{itemize}
\hyperref[sec:hier_clustering]{$$D(A, B) = \sum_{\mathbf{x} \in A \cup B} \|\mathbf{x} - \bar{\mathbf{x}}_{A \cup B}\|^2 - \sum_{\mathbf{x} \in A} \|\mathbf{x} - \bar{\mathbf{x}}_A\|^2 - \sum_{\mathbf{x} \in B} \|\mathbf{x} - \bar{\mathbf{x}}_B\|^2$$}
kde $\bar{\mathbf{x}}_A = \frac{1}{|A|} \sum_{\mathbf{x} \in A} \mathbf{x}$ je geometrický střed množiny $A$.
\end{itemize}

\subsection{Shlukování pomocí algoritmu k-means}
\subsubsection{Účelová funkce pro k-means $G(\mathcal{C})$}
\begin{itemize}
\hyperref[sec:kmeans]{$$G(\mathcal{C}) = \sum_{i=1}^{k} \frac{1}{2|C_i|} \sum_{\mathbf{x}, \mathbf{y} \in C_i} \|\mathbf{x} - \mathbf{y}\|^2$$}
\end{itemize}

\subsubsection{Souvislost účelové funkce s geometrickým středem}
\begin{itemize}
\item \textbf{Tvrzení:} Pro konečnou množinu $A \subset \mathbb{R}^p$ platí:
\hyperref[sec:kmeans]{$$\frac{1}{2|A|} \sum_{\mathbf{x}, \mathbf{y} \in A} \|\mathbf{x} - \mathbf{y}\|^2 = \sum_{\mathbf{x} \in A} \|\mathbf{x} - \bar{\mathbf{x}}\|^2 = \min_{\boldsymbol{\mu} \in \mathbb{R}^p} \sum_{\mathbf{x} \in A} \|\mathbf{x} - \boldsymbol{\mu}\|^2$$}

\item Ekvivalentní zápis účelové funkce:
\hyperref[sec:kmeans]{$$G(\mathcal{C}) = \sum_{i=1}^{k} \sum_{\mathbf{x} \in C_i} \|\mathbf{x} - \bar{\mathbf{x}}_i\|^2$$}
\end{itemize}

\subsubsection{Algoritmus k-means}
\begin{itemize}
\item \textbf{Inicializace:} Zvolíme $k$ středových bodů $\boldsymbol{\mu}_1, \ldots, \boldsymbol{\mu}_k$.
\item \textbf{Iterativní část:}
\begin{itemize}
\item[(i)] Roztřídění: \hyperref[sec:kmeans]{$C_i = \{\mathbf{x} \in \mathcal{D} \mid i = \arg\min_j \|\mathbf{x} - \boldsymbol{\mu}_j\|\}$}
\item[(ii)] Přepočet středů: \hyperref[sec:kmeans]{$\boldsymbol{\mu}_i \leftarrow \frac{1}{|C_i|} \sum_{\mathbf{x} \in C_i} \mathbf{x}$}
\end{itemize}
\end{itemize}

\subsubsection{Silhouette skóre: vnitřní rozdílnost $a(\mathbf{x})$}
\begin{itemize}
\hyperref[sec:kmeans]{$$a(\mathbf{x}) = \frac{1}{|C_{j(\mathbf{x})}| - 1} \sum_{\mathbf{y} \in C_{j(\mathbf{x})}, \mathbf{y} \neq \mathbf{x}} d(\mathbf{x}, \mathbf{y})$$}
\end{itemize}

\subsubsection{Silhouette skóre: sousední rozdílnost $b(\mathbf{x})$}
\begin{itemize}
\hyperref[sec:kmeans]{$$b(\mathbf{x}) = \min_{i \neq j(\mathbf{x})} d(\mathbf{x}, C_i), \quad \text{kde} \quad d(\mathbf{x}, C_i) = \frac{1}{|C_i|} \sum_{\mathbf{y} \in C_i} d(\mathbf{x}, \mathbf{y})$$}
\end{itemize}

\subsubsection{Silhouette skóre bodu $\mathbf{x}$: $s(\mathbf{x})$}
\begin{itemize}
\hyperref[sec:kmeans]{$$s(\mathbf{x}) = \frac{b(\mathbf{x}) - a(\mathbf{x})}{\max\{a(\mathbf{x}), b(\mathbf{x})\}} \in [-1, 1]$$}
\end{itemize}

\subsubsection{Průměrné skóre pro shluk $C_i$: $s_i$}
\begin{itemize}
\hyperref[sec:kmeans]{$$s_i = \frac{1}{|C_i|} \sum_{\mathbf{x} \in C_i} s(\mathbf{x})$$}
\end{itemize}

\subsubsection{Průměrné skóre pro celé shlukování: $s$}
\begin{itemize}
\hyperref[sec:kmeans]{$$s = \frac{1}{|\mathcal{D}|} \sum_{\mathbf{x} \in \mathcal{D}} s(\mathbf{x})$$}
\end{itemize}

\subsection{Shlukování pomocí algoritmu DBSCAN}
\subsubsection{$\varepsilon$ okolí bodu ($\varepsilon$-neighborhood)}
\begin{itemize}
\hyperref[sec:dbscan]{$$N_\varepsilon(\mathbf{x}) = \{\mathbf{y} \in \mathcal{D} \mid d(\mathbf{x}, \mathbf{y}) \leq \varepsilon\}$$}
\end{itemize}

\subsubsection{Podmínka pro klíčový bod (core point)}
\begin{itemize}
\item Bod $\mathbf{x} \in \mathcal{D}$ je klíčový bod, jestliže:
\hyperref[sec:dbscan]{$$|N_\varepsilon(\mathbf{x})| \geq \text{MinPts}$$}
\end{itemize}

\subsection{Asociační pravidla}
\subsubsection{Podpora množiny položek $\mathcal{K}$}
\begin{itemize}
\hyperref[sec:asociacni_pravidla]{$$T(\mathcal{K}) = \frac{1}{N} \sum_{i=1}^{N} \prod_{j \in \mathcal{K}} x_{i;j}$$}
\end{itemize}

\subsubsection{Spolehlivost (confidence)}
\begin{itemize}
\hyperref[sec:asociacni_pravidla]{$$C(A \Rightarrow B) = \frac{T(A \cup B)}{T(A)} = \hat{P}(B \mid A)$$}
\end{itemize}

\subsubsection{Zdvih (lift)}
\begin{itemize}
\hyperref[sec:asociacni_pravidla]{$$L(A \Rightarrow B) = \frac{C(A \Rightarrow B)}{T(B)} = \frac{\hat{P}(B \mid A)}{\hat{P}(B)}$$}
\end{itemize}

\subsubsection{Pokrytí (coverage)}
\begin{itemize}
\hyperref[sec:asociacni_pravidla]{$$\text{Coverage}(A \Rightarrow B) = \frac{T(A \cup B)}{T(B)} = \hat{P}(A \mid B)$$}
\end{itemize}

\subsubsection{Podmínky pro výběr pravidel}
\begin{itemize}
\item Asociační pravidlo $A \Rightarrow B$ je vybráno, pokud splňuje obě podmínky:
\hyperref[sec:asociacni_pravidla]{$$T(A \Rightarrow B) > t \quad \text{a} \quad C(A \Rightarrow B) > c$$}
kde $t$ je minimální podpora a $c$ je minimální spolehlivost.
\end{itemize}


\end{document}